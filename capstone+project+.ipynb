{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Udacity Machine Learning Nanodegree Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recurrent Neural Network based language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naren Doraiswamy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## August 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The traditional neural networks have given some great results after the evolution of the kind of computational efficiency that is available today and the huge amounts of data that can be processed with this computational power. These traditional networks just take in the inputs without assuming any dependencies between them and this might be a problem when we are working on problems like natural language processing , speech/audio processing and also in vision problems where adjacent pixels are almost the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this particular capstone project ,i will be working on the language processing problem where i will train a network on a particular kind of text and then try to generate similar kind of text from the model.\n",
    "\n",
    "\n",
    "The papers that i have referred are given below:\n",
    "\n",
    "[Language Model based on Recurrent Neural Network](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    "\n",
    "[Extensions of recurrent neural network language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)\n",
    "\n",
    "[Generating Text with Recurrent Neural Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling\n",
    "\n",
    "Our goal is to build a Language Model using a Recurrent Neural Network. Let's say we have sentence of  n  words. Language Model allows us to predict the probability of observing the sentence (in a given dataset) as:\n",
    "P(w1,...,wn)=∏i=1nP(wi∣w1,...,wi−1)P(w1,...,wn)=∏i=1nP(wi∣w1,...,wi−1) \n",
    "In words, the probability of a sentence is the product of probabilities of each word given the words that came before it. So, the probability of the sentence \"He went to buy some chocolate\" would be the probability of \"chocolate\" given \"He went to buy some\", multiplied by the probability of \"some\" given \"He went to buy\", and so on.\n",
    "Why is that useful? Why would we want to assign a probability to observing a sentence?\n",
    "First, such a model can be used as a scoring mechanism. For example, a Machine Translation system typically generates multiple candidates for an input sentence. You could use a language model to pick the most probable sentence. Intuitively, the most probable sentence is likely to be grammatically correct. Similar scoring happens in speech recognition systems.\n",
    "But solving the Language Modeling problem also has a cool side effect. Because we can predict the probability of a word given the preceding words, we are able to generate new text. It's a generative model. Given an existing sequence of words we sample a next word from the predicted probabilities, and repeat the process until we have a full sentence.__ And this is exactly what we are  going to do i.e: Generate new text__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a special case of RNN's called [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) since vanilla RNN's cannot hold on to the memory for a long time and suffers from Vanishing and Exploding gradient problem. Its just a simple manipulation of vanilla RNN and there are many different variants in Rnn's , the other famous one being gated recurrent units [GRU](https://perso.ens-lyon.fr/tristan.sterin/papers/An_Intrinsic_Difference_Between_Vanilla_GRU_Sterin_Farrugia_Gripon.pdf)\n",
    "\n",
    "So lets get started with the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tis section i will tell the steps in brief that we will perform to get the desired results from the built model.\n",
    "\n",
    "1. Import all the necessary libraries and perform pre-processing.\n",
    "2. Create mini-batches for faster training purposes.\n",
    "3. Feed these batches to the ML model and train it using theses data.\n",
    "4. Compute the training loss and optimize it by tuning the hyper-parameters which will best suit the network.\n",
    "5. Perform Sampling to generate the new text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go in detail with every single step and generate new text. __Sounds exciting__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-processing\n",
    "\n",
    "Since our input is text and we cannot just input them to our model as it understands only numbers and not text, we'll load the text file and convert it into integers for our network to use. We will create a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reddit.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'body\\n\"I joined a new league this year and they have different scoring rules than I\\'m used to. It\\'s a'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66, 79, 68, 89,  1,  4, 41,  2, 74, 79, 73, 78, 69, 68,  2, 65,  2,\n",
       "       78, 69, 87,  2, 76, 69, 65, 71, 85, 69,  2, 84, 72, 73, 83,  2, 89,\n",
       "       69, 65, 82,  2, 65, 78, 68,  2, 84, 72, 69, 89,  2, 72, 65, 86, 69,\n",
       "        2, 68, 73, 70, 70, 69, 82, 69, 78, 84,  2, 83, 67, 79, 82, 73, 78,\n",
       "       71,  2, 82, 85, 76, 69, 83,  2, 84, 72, 65, 78,  2, 41,  9, 77,  2,\n",
       "       85, 83, 69, 68,  2, 84, 79, 16,  2, 41, 84,  9, 83,  2, 65], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating Mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our text encoded into one long sequence of integers, Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass encoded into this function and get our batch generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why we do batching operation is to utilize the matrix operation efficiently during the training so the RNN is training on multiple sequences in parallel which will reduce our training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array arr, you divide the length of arr by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches*batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:,n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:,:-1], y[:,-1] = x[:,1:], x[:, 0] \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets have a batch_size of 500 and sequence steps of 50 and call our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 500,50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[66 79 68 89  1  4 41  2 74 79]\n",
      " [69 67 72  2 67 65 82 68 83  2]\n",
      " [79  2 65 78 68  2 65 82 69 14]\n",
      " [84 72 69  2 79 84 72 69 82 83]\n",
      " [ 2 73 78 86 79 76 86 69 68  2]\n",
      " [73 82  2 79 87 78  2 82 69 80]\n",
      " [86 69  2 35 85 76 84 85 82 69]\n",
      " [82 83  2 82 69 66 85 73 76 68]\n",
      " [68  2 84 72 69  2 73 78 73 84]\n",
      " [82 65 84 83  2 73 83  2 19 20]]\n",
      "\n",
      "y\n",
      " [[79 68 89  1  4 41  2 74 79 73]\n",
      " [67 72  2 67 65 82 68 83  2 74]\n",
      " [ 2 65 78 68  2 65 82 69 14  2]\n",
      " [72 69  2 79 84 72 69 82 83  2]\n",
      " [73 78 86 79 76 86 69 68  2 66]\n",
      " [82  2 79 87 78  2 82 69 80 82]\n",
      " [69  2 35 85 76 84 85 82 69 14]\n",
      " [83  2 82 69 66 85 73 76 68  2]\n",
      " [ 2 84 72 69  2 73 78 73 84 73]\n",
      " [65 84 83  2 73 83  2 19 20 25]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will be divided into 3 different parts\n",
    "\n",
    "1. create input place holders\n",
    "2. build the LSTM cells.\n",
    "3. Define the RNN output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the input placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called keep_prob. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the LSTM cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "             lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "\n",
    "where num_units is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with\n",
    "\n",
    "             tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with tf.contrib.rnn.MultiRNNCell. With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "              tf.contrib.rnn.MultiRNNCell([cell]*num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell outputs\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]*num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    \n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    \n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1, lstm_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "Then we run the logits and targets through tf.nn.softmax_cross_entropy_with_logits and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped =  tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal RNNs have have issues of gradients exploding and gradient vanishing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we will use tf.nn.dynamic_rnn  This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as final_state so we can pass it to the first LSTM cell in the the next mini-batch run. For tf.nn.dynamic_rnn, we pass in the cell and initial state we get from build_lstm, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits =  build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. batch_size - Number of sequences running through the network in one pass.\n",
    "2. num_steps - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "3. lstm_size - The number of units in the hidden layers.\n",
    "4. num_layers - Number of hidden LSTM layers to use\n",
    "5. learning_rate - Learning rate for training\n",
    "6. keep_prob - The dropout keep probability when training. If you're network is overfitting, try decreasing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 500         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15...  Training Step: 1...  Training loss: 6.1200...  0.2299 sec/batch\n",
      "Epoch: 1/15...  Training Step: 2...  Training loss: 6.0304...  0.1727 sec/batch\n",
      "Epoch: 1/15...  Training Step: 3...  Training loss: 4.7055...  0.1593 sec/batch\n",
      "Epoch: 1/15...  Training Step: 4...  Training loss: 3.9120...  0.1469 sec/batch\n",
      "Epoch: 1/15...  Training Step: 5...  Training loss: 3.6852...  0.1392 sec/batch\n",
      "Epoch: 1/15...  Training Step: 6...  Training loss: 3.5991...  0.1354 sec/batch\n",
      "Epoch: 1/15...  Training Step: 7...  Training loss: 3.5895...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 8...  Training loss: 3.5589...  0.1355 sec/batch\n",
      "Epoch: 1/15...  Training Step: 9...  Training loss: 3.5036...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 10...  Training loss: 3.4853...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 11...  Training loss: 3.4519...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 12...  Training loss: 3.4404...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 13...  Training loss: 3.4245...  0.1323 sec/batch\n",
      "Epoch: 1/15...  Training Step: 14...  Training loss: 3.4002...  0.1325 sec/batch\n",
      "Epoch: 1/15...  Training Step: 15...  Training loss: 3.3835...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 16...  Training loss: 3.3882...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 17...  Training loss: 3.3889...  0.1298 sec/batch\n",
      "Epoch: 1/15...  Training Step: 18...  Training loss: 3.3745...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 19...  Training loss: 3.3856...  0.1325 sec/batch\n",
      "Epoch: 1/15...  Training Step: 20...  Training loss: 3.3606...  0.1330 sec/batch\n",
      "Epoch: 1/15...  Training Step: 21...  Training loss: 3.3401...  0.1313 sec/batch\n",
      "Epoch: 1/15...  Training Step: 22...  Training loss: 3.3112...  0.1351 sec/batch\n",
      "Epoch: 1/15...  Training Step: 23...  Training loss: 3.3369...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 24...  Training loss: 3.3025...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 25...  Training loss: 3.3534...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 26...  Training loss: 3.3090...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 27...  Training loss: 3.3098...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 28...  Training loss: 3.3201...  0.1344 sec/batch\n",
      "Epoch: 1/15...  Training Step: 29...  Training loss: 3.3311...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 30...  Training loss: 3.3197...  0.1352 sec/batch\n",
      "Epoch: 1/15...  Training Step: 31...  Training loss: 3.3100...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 32...  Training loss: 3.3026...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 33...  Training loss: 3.2748...  0.1357 sec/batch\n",
      "Epoch: 1/15...  Training Step: 34...  Training loss: 3.2774...  0.1312 sec/batch\n",
      "Epoch: 1/15...  Training Step: 35...  Training loss: 3.2715...  0.1299 sec/batch\n",
      "Epoch: 1/15...  Training Step: 36...  Training loss: 3.2654...  0.1322 sec/batch\n",
      "Epoch: 1/15...  Training Step: 37...  Training loss: 3.2643...  0.1345 sec/batch\n",
      "Epoch: 1/15...  Training Step: 38...  Training loss: 3.2369...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 39...  Training loss: 3.2496...  0.1356 sec/batch\n",
      "Epoch: 1/15...  Training Step: 40...  Training loss: 3.2684...  0.1314 sec/batch\n",
      "Epoch: 1/15...  Training Step: 41...  Training loss: 3.2746...  0.1796 sec/batch\n",
      "Epoch: 1/15...  Training Step: 42...  Training loss: 3.2721...  0.1838 sec/batch\n",
      "Epoch: 1/15...  Training Step: 43...  Training loss: 3.2548...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 44...  Training loss: 3.2594...  0.1330 sec/batch\n",
      "Epoch: 1/15...  Training Step: 45...  Training loss: 3.2753...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 46...  Training loss: 3.2639...  0.1290 sec/batch\n",
      "Epoch: 1/15...  Training Step: 47...  Training loss: 3.2218...  0.1327 sec/batch\n",
      "Epoch: 1/15...  Training Step: 48...  Training loss: 3.2328...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 49...  Training loss: 3.2180...  0.1306 sec/batch\n",
      "Epoch: 1/15...  Training Step: 50...  Training loss: 3.2058...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 51...  Training loss: 3.2065...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 52...  Training loss: 3.2092...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 53...  Training loss: 3.2038...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 54...  Training loss: 3.2039...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 55...  Training loss: 3.2193...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 56...  Training loss: 3.2088...  0.1348 sec/batch\n",
      "Epoch: 1/15...  Training Step: 57...  Training loss: 3.1924...  0.1352 sec/batch\n",
      "Epoch: 1/15...  Training Step: 58...  Training loss: 3.2052...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 59...  Training loss: 3.1873...  0.1346 sec/batch\n",
      "Epoch: 1/15...  Training Step: 60...  Training loss: 3.1846...  0.1337 sec/batch\n",
      "Epoch: 1/15...  Training Step: 61...  Training loss: 3.1865...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 62...  Training loss: 3.2015...  0.1328 sec/batch\n",
      "Epoch: 1/15...  Training Step: 63...  Training loss: 3.1518...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 64...  Training loss: 3.1695...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 65...  Training loss: 3.1488...  0.1334 sec/batch\n",
      "Epoch: 1/15...  Training Step: 66...  Training loss: 3.1811...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 67...  Training loss: 3.1608...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 68...  Training loss: 3.1400...  0.1315 sec/batch\n",
      "Epoch: 1/15...  Training Step: 69...  Training loss: 3.1397...  0.1350 sec/batch\n",
      "Epoch: 1/15...  Training Step: 70...  Training loss: 3.1415...  0.1314 sec/batch\n",
      "Epoch: 1/15...  Training Step: 71...  Training loss: 3.1233...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 72...  Training loss: 3.1024...  0.1350 sec/batch\n",
      "Epoch: 1/15...  Training Step: 73...  Training loss: 3.1045...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 74...  Training loss: 3.1031...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 75...  Training loss: 3.0807...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 76...  Training loss: 3.0756...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 77...  Training loss: 3.1086...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 78...  Training loss: 3.0826...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 79...  Training loss: 3.0448...  0.1334 sec/batch\n",
      "Epoch: 1/15...  Training Step: 80...  Training loss: 3.0207...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 81...  Training loss: 3.0399...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 82...  Training loss: 3.0490...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 83...  Training loss: 3.0256...  0.1328 sec/batch\n",
      "Epoch: 1/15...  Training Step: 84...  Training loss: 3.0140...  0.1354 sec/batch\n",
      "Epoch: 1/15...  Training Step: 85...  Training loss: 3.0117...  0.1325 sec/batch\n",
      "Epoch: 1/15...  Training Step: 86...  Training loss: 2.9914...  0.1328 sec/batch\n",
      "Epoch: 1/15...  Training Step: 87...  Training loss: 2.9935...  0.1302 sec/batch\n",
      "Epoch: 1/15...  Training Step: 88...  Training loss: 2.9729...  0.1351 sec/batch\n",
      "Epoch: 1/15...  Training Step: 89...  Training loss: 2.9669...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 90...  Training loss: 2.9731...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 91...  Training loss: 2.9594...  0.1322 sec/batch\n",
      "Epoch: 1/15...  Training Step: 92...  Training loss: 2.9604...  0.1298 sec/batch\n",
      "Epoch: 1/15...  Training Step: 93...  Training loss: 2.9714...  0.1310 sec/batch\n",
      "Epoch: 1/15...  Training Step: 94...  Training loss: 2.9315...  0.1296 sec/batch\n",
      "Epoch: 1/15...  Training Step: 95...  Training loss: 2.9165...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 96...  Training loss: 2.9409...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 97...  Training loss: 2.9301...  0.1330 sec/batch\n",
      "Epoch: 1/15...  Training Step: 98...  Training loss: 2.9264...  0.1346 sec/batch\n",
      "Epoch: 1/15...  Training Step: 99...  Training loss: 2.8996...  0.1337 sec/batch\n",
      "Epoch: 1/15...  Training Step: 100...  Training loss: 2.9133...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 101...  Training loss: 2.9042...  0.1290 sec/batch\n",
      "Epoch: 1/15...  Training Step: 102...  Training loss: 2.8870...  0.1317 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15...  Training Step: 103...  Training loss: 2.8949...  0.1304 sec/batch\n",
      "Epoch: 1/15...  Training Step: 104...  Training loss: 2.8600...  0.1368 sec/batch\n",
      "Epoch: 1/15...  Training Step: 105...  Training loss: 2.8875...  0.1351 sec/batch\n",
      "Epoch: 1/15...  Training Step: 106...  Training loss: 2.8722...  0.1289 sec/batch\n",
      "Epoch: 1/15...  Training Step: 107...  Training loss: 2.8572...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 108...  Training loss: 2.8460...  0.1303 sec/batch\n",
      "Epoch: 1/15...  Training Step: 109...  Training loss: 2.8669...  0.1328 sec/batch\n",
      "Epoch: 1/15...  Training Step: 110...  Training loss: 2.8448...  0.1310 sec/batch\n",
      "Epoch: 1/15...  Training Step: 111...  Training loss: 2.8700...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 112...  Training loss: 2.8127...  0.1350 sec/batch\n",
      "Epoch: 1/15...  Training Step: 113...  Training loss: 2.8311...  0.1355 sec/batch\n",
      "Epoch: 1/15...  Training Step: 114...  Training loss: 2.8185...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 115...  Training loss: 2.8393...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 116...  Training loss: 2.8064...  0.1319 sec/batch\n",
      "Epoch: 1/15...  Training Step: 117...  Training loss: 2.7985...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 118...  Training loss: 2.8117...  0.1323 sec/batch\n",
      "Epoch: 1/15...  Training Step: 119...  Training loss: 2.8094...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 120...  Training loss: 2.7722...  0.1321 sec/batch\n",
      "Epoch: 1/15...  Training Step: 121...  Training loss: 2.7690...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 122...  Training loss: 2.7959...  0.1334 sec/batch\n",
      "Epoch: 1/15...  Training Step: 123...  Training loss: 2.8018...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 124...  Training loss: 2.7786...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 125...  Training loss: 2.7714...  0.1318 sec/batch\n",
      "Epoch: 1/15...  Training Step: 126...  Training loss: 2.7592...  0.1303 sec/batch\n",
      "Epoch: 1/15...  Training Step: 127...  Training loss: 2.7705...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 128...  Training loss: 2.7710...  0.1312 sec/batch\n",
      "Epoch: 1/15...  Training Step: 129...  Training loss: 2.7600...  0.1348 sec/batch\n",
      "Epoch: 1/15...  Training Step: 130...  Training loss: 2.7953...  0.1330 sec/batch\n",
      "Epoch: 1/15...  Training Step: 131...  Training loss: 2.7543...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 132...  Training loss: 2.7448...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 133...  Training loss: 2.7435...  0.1347 sec/batch\n",
      "Epoch: 1/15...  Training Step: 134...  Training loss: 2.7728...  0.1348 sec/batch\n",
      "Epoch: 1/15...  Training Step: 135...  Training loss: 2.7520...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 136...  Training loss: 2.7224...  0.1312 sec/batch\n",
      "Epoch: 1/15...  Training Step: 137...  Training loss: 2.7243...  0.1355 sec/batch\n",
      "Epoch: 1/15...  Training Step: 138...  Training loss: 2.7122...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 139...  Training loss: 2.7338...  0.1347 sec/batch\n",
      "Epoch: 1/15...  Training Step: 140...  Training loss: 2.7253...  0.1314 sec/batch\n",
      "Epoch: 1/15...  Training Step: 141...  Training loss: 2.7300...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 142...  Training loss: 2.7023...  0.1287 sec/batch\n",
      "Epoch: 1/15...  Training Step: 143...  Training loss: 2.6960...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 144...  Training loss: 2.7048...  0.1349 sec/batch\n",
      "Epoch: 1/15...  Training Step: 145...  Training loss: 2.7057...  0.1345 sec/batch\n",
      "Epoch: 1/15...  Training Step: 146...  Training loss: 2.7129...  0.1327 sec/batch\n",
      "Epoch: 1/15...  Training Step: 147...  Training loss: 2.6914...  0.1349 sec/batch\n",
      "Epoch: 1/15...  Training Step: 148...  Training loss: 2.7143...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 149...  Training loss: 2.7050...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 150...  Training loss: 2.6755...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 151...  Training loss: 2.6897...  0.1344 sec/batch\n",
      "Epoch: 1/15...  Training Step: 152...  Training loss: 2.6821...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 153...  Training loss: 2.6968...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 154...  Training loss: 2.6907...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 155...  Training loss: 2.6708...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 156...  Training loss: 2.6803...  0.1349 sec/batch\n",
      "Epoch: 1/15...  Training Step: 157...  Training loss: 2.6648...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 158...  Training loss: 2.6813...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 159...  Training loss: 2.6805...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 160...  Training loss: 2.6668...  0.1309 sec/batch\n",
      "Epoch: 1/15...  Training Step: 161...  Training loss: 2.6706...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 162...  Training loss: 2.6361...  0.1358 sec/batch\n",
      "Epoch: 1/15...  Training Step: 163...  Training loss: 2.6482...  0.1353 sec/batch\n",
      "Epoch: 1/15...  Training Step: 164...  Training loss: 2.6302...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 165...  Training loss: 2.6417...  0.1318 sec/batch\n",
      "Epoch: 1/15...  Training Step: 166...  Training loss: 2.6432...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 167...  Training loss: 2.6429...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 168...  Training loss: 2.6354...  0.1357 sec/batch\n",
      "Epoch: 1/15...  Training Step: 169...  Training loss: 2.6625...  0.1356 sec/batch\n",
      "Epoch: 1/15...  Training Step: 170...  Training loss: 2.6541...  0.1304 sec/batch\n",
      "Epoch: 1/15...  Training Step: 171...  Training loss: 2.6483...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 172...  Training loss: 2.6412...  0.1325 sec/batch\n",
      "Epoch: 1/15...  Training Step: 173...  Training loss: 2.6268...  0.1346 sec/batch\n",
      "Epoch: 1/15...  Training Step: 174...  Training loss: 2.6322...  0.1310 sec/batch\n",
      "Epoch: 1/15...  Training Step: 175...  Training loss: 2.6028...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 176...  Training loss: 2.6006...  0.1321 sec/batch\n",
      "Epoch: 1/15...  Training Step: 177...  Training loss: 2.6052...  0.1346 sec/batch\n",
      "Epoch: 1/15...  Training Step: 178...  Training loss: 2.6067...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 179...  Training loss: 2.5928...  0.1348 sec/batch\n",
      "Epoch: 1/15...  Training Step: 180...  Training loss: 2.6110...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 181...  Training loss: 2.5766...  0.1313 sec/batch\n",
      "Epoch: 1/15...  Training Step: 182...  Training loss: 2.5986...  0.1293 sec/batch\n",
      "Epoch: 1/15...  Training Step: 183...  Training loss: 2.5876...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 184...  Training loss: 2.5717...  0.1349 sec/batch\n",
      "Epoch: 1/15...  Training Step: 185...  Training loss: 2.5709...  0.1355 sec/batch\n",
      "Epoch: 1/15...  Training Step: 186...  Training loss: 2.5732...  0.1307 sec/batch\n",
      "Epoch: 1/15...  Training Step: 187...  Training loss: 2.5866...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 188...  Training loss: 2.5643...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 189...  Training loss: 2.5577...  0.1307 sec/batch\n",
      "Epoch: 1/15...  Training Step: 190...  Training loss: 2.5552...  0.1345 sec/batch\n",
      "Epoch: 1/15...  Training Step: 191...  Training loss: 2.5621...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 192...  Training loss: 2.5658...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 193...  Training loss: 2.5506...  0.1327 sec/batch\n",
      "Epoch: 1/15...  Training Step: 194...  Training loss: 2.5686...  0.1314 sec/batch\n",
      "Epoch: 1/15...  Training Step: 195...  Training loss: 2.5507...  0.1347 sec/batch\n",
      "Epoch: 1/15...  Training Step: 196...  Training loss: 2.5643...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 197...  Training loss: 2.5788...  0.1353 sec/batch\n",
      "Epoch: 1/15...  Training Step: 198...  Training loss: 2.5744...  0.1299 sec/batch\n",
      "Epoch: 1/15...  Training Step: 199...  Training loss: 2.5867...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 200...  Training loss: 2.5838...  0.1327 sec/batch\n",
      "Epoch: 1/15...  Training Step: 201...  Training loss: 2.5601...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 202...  Training loss: 2.5658...  0.1342 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15...  Training Step: 203...  Training loss: 2.5973...  0.1405 sec/batch\n",
      "Epoch: 1/15...  Training Step: 204...  Training loss: 2.5948...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 205...  Training loss: 2.5859...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 206...  Training loss: 2.5636...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 207...  Training loss: 2.5652...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 208...  Training loss: 2.5560...  0.1301 sec/batch\n",
      "Epoch: 1/15...  Training Step: 209...  Training loss: 2.5394...  0.1374 sec/batch\n",
      "Epoch: 1/15...  Training Step: 210...  Training loss: 2.5567...  0.1362 sec/batch\n",
      "Epoch: 1/15...  Training Step: 211...  Training loss: 2.5413...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 212...  Training loss: 2.5325...  0.1358 sec/batch\n",
      "Epoch: 1/15...  Training Step: 213...  Training loss: 2.5506...  0.1341 sec/batch\n",
      "Epoch: 1/15...  Training Step: 214...  Training loss: 2.5296...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 215...  Training loss: 2.5273...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 216...  Training loss: 2.5284...  0.1311 sec/batch\n",
      "Epoch: 1/15...  Training Step: 217...  Training loss: 2.5126...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 218...  Training loss: 2.5326...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 219...  Training loss: 2.5326...  0.1302 sec/batch\n",
      "Epoch: 1/15...  Training Step: 220...  Training loss: 2.5249...  0.1351 sec/batch\n",
      "Epoch: 1/15...  Training Step: 221...  Training loss: 2.5161...  0.1304 sec/batch\n",
      "Epoch: 1/15...  Training Step: 222...  Training loss: 2.5129...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 223...  Training loss: 2.5042...  0.1365 sec/batch\n",
      "Epoch: 1/15...  Training Step: 224...  Training loss: 2.5084...  0.1359 sec/batch\n",
      "Epoch: 1/15...  Training Step: 225...  Training loss: 2.5158...  0.1308 sec/batch\n",
      "Epoch: 1/15...  Training Step: 226...  Training loss: 2.5359...  0.1352 sec/batch\n",
      "Epoch: 1/15...  Training Step: 227...  Training loss: 2.5325...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 228...  Training loss: 2.5105...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 229...  Training loss: 2.5084...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 230...  Training loss: 2.5079...  0.1318 sec/batch\n",
      "Epoch: 1/15...  Training Step: 231...  Training loss: 2.4897...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 232...  Training loss: 2.4895...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 233...  Training loss: 2.5029...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 234...  Training loss: 2.4918...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 235...  Training loss: 2.4789...  0.1313 sec/batch\n",
      "Epoch: 1/15...  Training Step: 236...  Training loss: 2.4864...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 237...  Training loss: 2.4972...  0.1323 sec/batch\n",
      "Epoch: 1/15...  Training Step: 238...  Training loss: 2.4891...  0.1350 sec/batch\n",
      "Epoch: 1/15...  Training Step: 239...  Training loss: 2.4912...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 240...  Training loss: 2.5019...  0.1312 sec/batch\n",
      "Epoch: 1/15...  Training Step: 241...  Training loss: 2.4925...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 242...  Training loss: 2.4913...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 243...  Training loss: 2.4543...  0.1819 sec/batch\n",
      "Epoch: 1/15...  Training Step: 244...  Training loss: 2.4790...  0.1835 sec/batch\n",
      "Epoch: 1/15...  Training Step: 245...  Training loss: 2.4703...  0.1340 sec/batch\n",
      "Epoch: 1/15...  Training Step: 246...  Training loss: 2.4386...  0.1349 sec/batch\n",
      "Epoch: 1/15...  Training Step: 247...  Training loss: 2.5035...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 248...  Training loss: 2.4624...  0.1310 sec/batch\n",
      "Epoch: 1/15...  Training Step: 249...  Training loss: 2.4736...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 250...  Training loss: 2.4785...  0.1299 sec/batch\n",
      "Epoch: 1/15...  Training Step: 251...  Training loss: 2.4557...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 252...  Training loss: 2.4622...  0.1331 sec/batch\n",
      "Epoch: 1/15...  Training Step: 253...  Training loss: 2.4744...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 254...  Training loss: 2.4614...  0.1334 sec/batch\n",
      "Epoch: 1/15...  Training Step: 255...  Training loss: 2.4551...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 256...  Training loss: 2.4614...  0.1319 sec/batch\n",
      "Epoch: 1/15...  Training Step: 257...  Training loss: 2.4851...  0.1308 sec/batch\n",
      "Epoch: 1/15...  Training Step: 258...  Training loss: 2.4925...  0.1337 sec/batch\n",
      "Epoch: 1/15...  Training Step: 259...  Training loss: 2.4582...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 260...  Training loss: 2.4434...  0.1777 sec/batch\n",
      "Epoch: 1/15...  Training Step: 261...  Training loss: 2.4643...  0.1849 sec/batch\n",
      "Epoch: 1/15...  Training Step: 262...  Training loss: 2.4142...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 263...  Training loss: 2.4581...  0.1325 sec/batch\n",
      "Epoch: 1/15...  Training Step: 264...  Training loss: 2.4307...  0.1345 sec/batch\n",
      "Epoch: 1/15...  Training Step: 265...  Training loss: 2.4362...  0.1337 sec/batch\n",
      "Epoch: 1/15...  Training Step: 266...  Training loss: 2.4400...  0.1343 sec/batch\n",
      "Epoch: 1/15...  Training Step: 267...  Training loss: 2.4432...  0.1342 sec/batch\n",
      "Epoch: 1/15...  Training Step: 268...  Training loss: 2.4447...  0.1327 sec/batch\n",
      "Epoch: 1/15...  Training Step: 269...  Training loss: 2.4602...  0.1322 sec/batch\n",
      "Epoch: 1/15...  Training Step: 270...  Training loss: 2.4606...  0.1304 sec/batch\n",
      "Epoch: 1/15...  Training Step: 271...  Training loss: 2.4290...  0.1330 sec/batch\n",
      "Epoch: 1/15...  Training Step: 272...  Training loss: 2.4136...  0.1326 sec/batch\n",
      "Epoch: 1/15...  Training Step: 273...  Training loss: 2.4293...  0.1363 sec/batch\n",
      "Epoch: 1/15...  Training Step: 274...  Training loss: 2.3950...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 275...  Training loss: 2.4214...  0.1345 sec/batch\n",
      "Epoch: 1/15...  Training Step: 276...  Training loss: 2.4457...  0.1321 sec/batch\n",
      "Epoch: 1/15...  Training Step: 277...  Training loss: 2.4070...  0.1767 sec/batch\n",
      "Epoch: 1/15...  Training Step: 278...  Training loss: 2.4092...  0.1844 sec/batch\n",
      "Epoch: 1/15...  Training Step: 279...  Training loss: 2.4187...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 280...  Training loss: 2.4206...  0.1310 sec/batch\n",
      "Epoch: 1/15...  Training Step: 281...  Training loss: 2.4380...  0.1346 sec/batch\n",
      "Epoch: 1/15...  Training Step: 282...  Training loss: 2.4391...  0.1317 sec/batch\n",
      "Epoch: 1/15...  Training Step: 283...  Training loss: 2.4312...  0.1308 sec/batch\n",
      "Epoch: 1/15...  Training Step: 284...  Training loss: 2.4194...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 285...  Training loss: 2.4051...  0.1337 sec/batch\n",
      "Epoch: 1/15...  Training Step: 286...  Training loss: 2.4147...  0.1339 sec/batch\n",
      "Epoch: 1/15...  Training Step: 287...  Training loss: 2.4559...  0.1305 sec/batch\n",
      "Epoch: 1/15...  Training Step: 288...  Training loss: 2.4118...  0.1334 sec/batch\n",
      "Epoch: 1/15...  Training Step: 289...  Training loss: 2.4219...  0.1308 sec/batch\n",
      "Epoch: 1/15...  Training Step: 290...  Training loss: 2.4306...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 291...  Training loss: 2.4301...  0.1316 sec/batch\n",
      "Epoch: 1/15...  Training Step: 292...  Training loss: 2.4528...  0.1289 sec/batch\n",
      "Epoch: 1/15...  Training Step: 293...  Training loss: 2.4569...  0.1338 sec/batch\n",
      "Epoch: 1/15...  Training Step: 294...  Training loss: 2.4512...  0.1324 sec/batch\n",
      "Epoch: 1/15...  Training Step: 295...  Training loss: 2.4353...  0.1335 sec/batch\n",
      "Epoch: 1/15...  Training Step: 296...  Training loss: 2.4535...  0.1332 sec/batch\n",
      "Epoch: 1/15...  Training Step: 297...  Training loss: 2.4083...  0.1360 sec/batch\n",
      "Epoch: 1/15...  Training Step: 298...  Training loss: 2.4056...  0.1329 sec/batch\n",
      "Epoch: 1/15...  Training Step: 299...  Training loss: 2.4270...  0.1336 sec/batch\n",
      "Epoch: 1/15...  Training Step: 300...  Training loss: 2.4252...  0.1320 sec/batch\n",
      "Epoch: 1/15...  Training Step: 301...  Training loss: 2.3930...  0.1333 sec/batch\n",
      "Epoch: 1/15...  Training Step: 302...  Training loss: 2.4264...  0.1335 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15...  Training Step: 303...  Training loss: 2.4117...  0.1322 sec/batch\n",
      "Epoch: 2/15...  Training Step: 304...  Training loss: 2.4720...  0.1788 sec/batch\n",
      "Epoch: 2/15...  Training Step: 305...  Training loss: 2.4112...  0.1845 sec/batch\n",
      "Epoch: 2/15...  Training Step: 306...  Training loss: 2.3948...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 307...  Training loss: 2.4153...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 308...  Training loss: 2.4052...  0.1313 sec/batch\n",
      "Epoch: 2/15...  Training Step: 309...  Training loss: 2.4302...  0.1349 sec/batch\n",
      "Epoch: 2/15...  Training Step: 310...  Training loss: 2.4071...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 311...  Training loss: 2.4105...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 312...  Training loss: 2.3766...  0.1302 sec/batch\n",
      "Epoch: 2/15...  Training Step: 313...  Training loss: 2.3929...  0.1329 sec/batch\n",
      "Epoch: 2/15...  Training Step: 314...  Training loss: 2.3757...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 315...  Training loss: 2.3898...  0.1357 sec/batch\n",
      "Epoch: 2/15...  Training Step: 316...  Training loss: 2.4000...  0.1347 sec/batch\n",
      "Epoch: 2/15...  Training Step: 317...  Training loss: 2.3990...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 318...  Training loss: 2.3906...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 319...  Training loss: 2.3997...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 320...  Training loss: 2.4246...  0.1344 sec/batch\n",
      "Epoch: 2/15...  Training Step: 321...  Training loss: 2.4124...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 322...  Training loss: 2.4369...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 323...  Training loss: 2.4224...  0.1308 sec/batch\n",
      "Epoch: 2/15...  Training Step: 324...  Training loss: 2.4142...  0.1348 sec/batch\n",
      "Epoch: 2/15...  Training Step: 325...  Training loss: 2.3947...  0.1344 sec/batch\n",
      "Epoch: 2/15...  Training Step: 326...  Training loss: 2.4191...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 327...  Training loss: 2.3973...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 328...  Training loss: 2.4367...  0.1323 sec/batch\n",
      "Epoch: 2/15...  Training Step: 329...  Training loss: 2.4061...  0.1353 sec/batch\n",
      "Epoch: 2/15...  Training Step: 330...  Training loss: 2.3989...  0.1329 sec/batch\n",
      "Epoch: 2/15...  Training Step: 331...  Training loss: 2.4173...  0.1321 sec/batch\n",
      "Epoch: 2/15...  Training Step: 332...  Training loss: 2.4239...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 333...  Training loss: 2.4029...  0.1315 sec/batch\n",
      "Epoch: 2/15...  Training Step: 334...  Training loss: 2.3920...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 335...  Training loss: 2.4042...  0.1324 sec/batch\n",
      "Epoch: 2/15...  Training Step: 336...  Training loss: 2.3889...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 337...  Training loss: 2.3809...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 338...  Training loss: 2.3807...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 339...  Training loss: 2.3771...  0.1347 sec/batch\n",
      "Epoch: 2/15...  Training Step: 340...  Training loss: 2.3898...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 341...  Training loss: 2.3644...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 342...  Training loss: 2.3802...  0.1358 sec/batch\n",
      "Epoch: 2/15...  Training Step: 343...  Training loss: 2.3734...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 344...  Training loss: 2.3744...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 345...  Training loss: 2.3687...  0.1353 sec/batch\n",
      "Epoch: 2/15...  Training Step: 346...  Training loss: 2.3736...  0.1302 sec/batch\n",
      "Epoch: 2/15...  Training Step: 347...  Training loss: 2.3716...  0.1342 sec/batch\n",
      "Epoch: 2/15...  Training Step: 348...  Training loss: 2.3769...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 349...  Training loss: 2.3478...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 350...  Training loss: 2.3414...  0.1328 sec/batch\n",
      "Epoch: 2/15...  Training Step: 351...  Training loss: 2.3668...  0.1373 sec/batch\n",
      "Epoch: 2/15...  Training Step: 352...  Training loss: 2.3463...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 353...  Training loss: 2.3256...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 354...  Training loss: 2.3468...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 355...  Training loss: 2.3398...  0.1309 sec/batch\n",
      "Epoch: 2/15...  Training Step: 356...  Training loss: 2.3373...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 357...  Training loss: 2.3355...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 358...  Training loss: 2.3392...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 359...  Training loss: 2.3357...  0.1353 sec/batch\n",
      "Epoch: 2/15...  Training Step: 360...  Training loss: 2.3362...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 361...  Training loss: 2.3269...  0.1304 sec/batch\n",
      "Epoch: 2/15...  Training Step: 362...  Training loss: 2.3201...  0.1799 sec/batch\n",
      "Epoch: 2/15...  Training Step: 363...  Training loss: 2.3142...  0.1842 sec/batch\n",
      "Epoch: 2/15...  Training Step: 364...  Training loss: 2.3232...  0.1309 sec/batch\n",
      "Epoch: 2/15...  Training Step: 365...  Training loss: 2.3351...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 366...  Training loss: 2.3103...  0.1363 sec/batch\n",
      "Epoch: 2/15...  Training Step: 367...  Training loss: 2.3480...  0.1339 sec/batch\n",
      "Epoch: 2/15...  Training Step: 368...  Training loss: 2.3483...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 369...  Training loss: 2.3671...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 370...  Training loss: 2.3549...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 371...  Training loss: 2.3332...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 372...  Training loss: 2.3430...  0.1353 sec/batch\n",
      "Epoch: 2/15...  Training Step: 373...  Training loss: 2.3296...  0.1355 sec/batch\n",
      "Epoch: 2/15...  Training Step: 374...  Training loss: 2.3512...  0.1320 sec/batch\n",
      "Epoch: 2/15...  Training Step: 375...  Training loss: 2.3263...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 376...  Training loss: 2.3285...  0.1358 sec/batch\n",
      "Epoch: 2/15...  Training Step: 377...  Training loss: 2.3373...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 378...  Training loss: 2.3179...  0.1342 sec/batch\n",
      "Epoch: 2/15...  Training Step: 379...  Training loss: 2.3359...  0.1352 sec/batch\n",
      "Epoch: 2/15...  Training Step: 380...  Training loss: 2.3500...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 381...  Training loss: 2.3580...  0.1291 sec/batch\n",
      "Epoch: 2/15...  Training Step: 382...  Training loss: 2.3210...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 383...  Training loss: 2.3172...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 384...  Training loss: 2.3372...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 385...  Training loss: 2.3495...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 386...  Training loss: 2.3195...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 387...  Training loss: 2.3173...  0.1320 sec/batch\n",
      "Epoch: 2/15...  Training Step: 388...  Training loss: 2.3265...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 389...  Training loss: 2.3108...  0.1314 sec/batch\n",
      "Epoch: 2/15...  Training Step: 390...  Training loss: 2.3155...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 391...  Training loss: 2.3149...  0.1310 sec/batch\n",
      "Epoch: 2/15...  Training Step: 392...  Training loss: 2.3016...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 393...  Training loss: 2.3255...  0.1328 sec/batch\n",
      "Epoch: 2/15...  Training Step: 394...  Training loss: 2.3054...  0.1349 sec/batch\n",
      "Epoch: 2/15...  Training Step: 395...  Training loss: 2.3224...  0.1309 sec/batch\n",
      "Epoch: 2/15...  Training Step: 396...  Training loss: 2.3273...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 397...  Training loss: 2.2947...  0.1348 sec/batch\n",
      "Epoch: 2/15...  Training Step: 398...  Training loss: 2.2952...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 399...  Training loss: 2.3016...  0.1323 sec/batch\n",
      "Epoch: 2/15...  Training Step: 400...  Training loss: 2.3196...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 401...  Training loss: 2.3242...  0.1357 sec/batch\n",
      "Epoch: 2/15...  Training Step: 402...  Training loss: 2.2978...  0.1344 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15...  Training Step: 403...  Training loss: 2.3110...  0.1351 sec/batch\n",
      "Epoch: 2/15...  Training Step: 404...  Training loss: 2.3035...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 405...  Training loss: 2.2991...  0.1351 sec/batch\n",
      "Epoch: 2/15...  Training Step: 406...  Training loss: 2.2978...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 407...  Training loss: 2.2908...  0.1345 sec/batch\n",
      "Epoch: 2/15...  Training Step: 408...  Training loss: 2.3083...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 409...  Training loss: 2.3099...  0.1308 sec/batch\n",
      "Epoch: 2/15...  Training Step: 410...  Training loss: 2.3012...  0.1801 sec/batch\n",
      "Epoch: 2/15...  Training Step: 411...  Training loss: 2.3036...  0.1843 sec/batch\n",
      "Epoch: 2/15...  Training Step: 412...  Training loss: 2.3200...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 413...  Training loss: 2.3108...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 414...  Training loss: 2.3301...  0.1305 sec/batch\n",
      "Epoch: 2/15...  Training Step: 415...  Training loss: 2.2905...  0.1354 sec/batch\n",
      "Epoch: 2/15...  Training Step: 416...  Training loss: 2.2919...  0.1339 sec/batch\n",
      "Epoch: 2/15...  Training Step: 417...  Training loss: 2.3011...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 418...  Training loss: 2.3035...  0.1354 sec/batch\n",
      "Epoch: 2/15...  Training Step: 419...  Training loss: 2.3039...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 420...  Training loss: 2.3052...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 421...  Training loss: 2.3013...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 422...  Training loss: 2.3133...  0.1304 sec/batch\n",
      "Epoch: 2/15...  Training Step: 423...  Training loss: 2.2674...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 424...  Training loss: 2.2672...  0.1315 sec/batch\n",
      "Epoch: 2/15...  Training Step: 425...  Training loss: 2.3079...  0.1316 sec/batch\n",
      "Epoch: 2/15...  Training Step: 426...  Training loss: 2.3057...  0.1296 sec/batch\n",
      "Epoch: 2/15...  Training Step: 427...  Training loss: 2.3058...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 428...  Training loss: 2.3039...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 429...  Training loss: 2.2984...  0.1323 sec/batch\n",
      "Epoch: 2/15...  Training Step: 430...  Training loss: 2.2985...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 431...  Training loss: 2.3277...  0.1344 sec/batch\n",
      "Epoch: 2/15...  Training Step: 432...  Training loss: 2.3032...  0.1356 sec/batch\n",
      "Epoch: 2/15...  Training Step: 433...  Training loss: 2.3362...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 434...  Training loss: 2.3025...  0.1352 sec/batch\n",
      "Epoch: 2/15...  Training Step: 435...  Training loss: 2.3067...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 436...  Training loss: 2.3021...  0.1349 sec/batch\n",
      "Epoch: 2/15...  Training Step: 437...  Training loss: 2.3409...  0.1342 sec/batch\n",
      "Epoch: 2/15...  Training Step: 438...  Training loss: 2.3079...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 439...  Training loss: 2.2905...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 440...  Training loss: 2.2995...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 441...  Training loss: 2.2895...  0.1329 sec/batch\n",
      "Epoch: 2/15...  Training Step: 442...  Training loss: 2.3074...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 443...  Training loss: 2.2987...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 444...  Training loss: 2.2922...  0.1319 sec/batch\n",
      "Epoch: 2/15...  Training Step: 445...  Training loss: 2.2698...  0.1819 sec/batch\n",
      "Epoch: 2/15...  Training Step: 446...  Training loss: 2.2719...  0.1837 sec/batch\n",
      "Epoch: 2/15...  Training Step: 447...  Training loss: 2.2957...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 448...  Training loss: 2.2700...  0.1323 sec/batch\n",
      "Epoch: 2/15...  Training Step: 449...  Training loss: 2.2942...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 450...  Training loss: 2.2745...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 451...  Training loss: 2.2956...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 452...  Training loss: 2.2983...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 453...  Training loss: 2.2659...  0.1314 sec/batch\n",
      "Epoch: 2/15...  Training Step: 454...  Training loss: 2.2912...  0.1322 sec/batch\n",
      "Epoch: 2/15...  Training Step: 455...  Training loss: 2.2765...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 456...  Training loss: 2.2846...  0.1349 sec/batch\n",
      "Epoch: 2/15...  Training Step: 457...  Training loss: 2.2929...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 458...  Training loss: 2.2703...  0.1303 sec/batch\n",
      "Epoch: 2/15...  Training Step: 459...  Training loss: 2.2910...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 460...  Training loss: 2.2799...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 461...  Training loss: 2.2908...  0.1325 sec/batch\n",
      "Epoch: 2/15...  Training Step: 462...  Training loss: 2.2877...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 463...  Training loss: 2.2869...  0.1348 sec/batch\n",
      "Epoch: 2/15...  Training Step: 464...  Training loss: 2.2705...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 465...  Training loss: 2.2655...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 466...  Training loss: 2.2626...  0.1315 sec/batch\n",
      "Epoch: 2/15...  Training Step: 467...  Training loss: 2.2643...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 468...  Training loss: 2.2644...  0.1301 sec/batch\n",
      "Epoch: 2/15...  Training Step: 469...  Training loss: 2.2916...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 470...  Training loss: 2.2601...  0.1320 sec/batch\n",
      "Epoch: 2/15...  Training Step: 471...  Training loss: 2.2589...  0.1317 sec/batch\n",
      "Epoch: 2/15...  Training Step: 472...  Training loss: 2.2810...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 473...  Training loss: 2.2841...  0.1343 sec/batch\n",
      "Epoch: 2/15...  Training Step: 474...  Training loss: 2.2664...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 475...  Training loss: 2.2734...  0.1355 sec/batch\n",
      "Epoch: 2/15...  Training Step: 476...  Training loss: 2.2628...  0.1357 sec/batch\n",
      "Epoch: 2/15...  Training Step: 477...  Training loss: 2.2660...  0.1365 sec/batch\n",
      "Epoch: 2/15...  Training Step: 478...  Training loss: 2.2577...  0.1818 sec/batch\n",
      "Epoch: 2/15...  Training Step: 479...  Training loss: 2.2384...  0.1844 sec/batch\n",
      "Epoch: 2/15...  Training Step: 480...  Training loss: 2.2625...  0.1365 sec/batch\n",
      "Epoch: 2/15...  Training Step: 481...  Training loss: 2.2474...  0.1326 sec/batch\n",
      "Epoch: 2/15...  Training Step: 482...  Training loss: 2.2482...  0.1305 sec/batch\n",
      "Epoch: 2/15...  Training Step: 483...  Training loss: 2.2457...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 484...  Training loss: 2.2456...  0.1307 sec/batch\n",
      "Epoch: 2/15...  Training Step: 485...  Training loss: 2.2626...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 486...  Training loss: 2.2618...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 487...  Training loss: 2.2353...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 488...  Training loss: 2.2547...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 489...  Training loss: 2.2444...  0.1325 sec/batch\n",
      "Epoch: 2/15...  Training Step: 490...  Training loss: 2.2522...  0.1312 sec/batch\n",
      "Epoch: 2/15...  Training Step: 491...  Training loss: 2.2447...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 492...  Training loss: 2.2335...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 493...  Training loss: 2.2454...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 494...  Training loss: 2.2370...  0.1351 sec/batch\n",
      "Epoch: 2/15...  Training Step: 495...  Training loss: 2.2427...  0.1359 sec/batch\n",
      "Epoch: 2/15...  Training Step: 496...  Training loss: 2.2270...  0.1308 sec/batch\n",
      "Epoch: 2/15...  Training Step: 497...  Training loss: 2.2607...  0.1309 sec/batch\n",
      "Epoch: 2/15...  Training Step: 498...  Training loss: 2.2263...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 499...  Training loss: 2.2363...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 500...  Training loss: 2.2635...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 501...  Training loss: 2.2468...  0.1381 sec/batch\n",
      "Epoch: 2/15...  Training Step: 502...  Training loss: 2.2471...  0.1338 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15...  Training Step: 503...  Training loss: 2.2573...  0.1298 sec/batch\n",
      "Epoch: 2/15...  Training Step: 504...  Training loss: 2.2432...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 505...  Training loss: 2.2435...  0.1329 sec/batch\n",
      "Epoch: 2/15...  Training Step: 506...  Training loss: 2.2740...  0.1348 sec/batch\n",
      "Epoch: 2/15...  Training Step: 507...  Training loss: 2.2814...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 508...  Training loss: 2.2759...  0.1333 sec/batch\n",
      "Epoch: 2/15...  Training Step: 509...  Training loss: 2.2574...  0.1363 sec/batch\n",
      "Epoch: 2/15...  Training Step: 510...  Training loss: 2.2653...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 511...  Training loss: 2.2601...  0.1328 sec/batch\n",
      "Epoch: 2/15...  Training Step: 512...  Training loss: 2.2543...  0.1347 sec/batch\n",
      "Epoch: 2/15...  Training Step: 513...  Training loss: 2.2538...  0.1345 sec/batch\n",
      "Epoch: 2/15...  Training Step: 514...  Training loss: 2.2402...  0.1363 sec/batch\n",
      "Epoch: 2/15...  Training Step: 515...  Training loss: 2.2359...  0.1303 sec/batch\n",
      "Epoch: 2/15...  Training Step: 516...  Training loss: 2.2510...  0.1354 sec/batch\n",
      "Epoch: 2/15...  Training Step: 517...  Training loss: 2.2402...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 518...  Training loss: 2.2436...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 519...  Training loss: 2.2444...  0.1895 sec/batch\n",
      "Epoch: 2/15...  Training Step: 520...  Training loss: 2.2281...  0.1771 sec/batch\n",
      "Epoch: 2/15...  Training Step: 521...  Training loss: 2.2480...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 522...  Training loss: 2.2439...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 523...  Training loss: 2.2378...  0.1321 sec/batch\n",
      "Epoch: 2/15...  Training Step: 524...  Training loss: 2.2379...  0.1315 sec/batch\n",
      "Epoch: 2/15...  Training Step: 525...  Training loss: 2.2376...  0.1289 sec/batch\n",
      "Epoch: 2/15...  Training Step: 526...  Training loss: 2.2208...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 527...  Training loss: 2.2210...  0.1312 sec/batch\n",
      "Epoch: 2/15...  Training Step: 528...  Training loss: 2.2358...  0.1313 sec/batch\n",
      "Epoch: 2/15...  Training Step: 529...  Training loss: 2.2468...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 530...  Training loss: 2.2448...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 531...  Training loss: 2.2328...  0.1306 sec/batch\n",
      "Epoch: 2/15...  Training Step: 532...  Training loss: 2.2459...  0.1339 sec/batch\n",
      "Epoch: 2/15...  Training Step: 533...  Training loss: 2.2457...  0.1313 sec/batch\n",
      "Epoch: 2/15...  Training Step: 534...  Training loss: 2.2254...  0.1302 sec/batch\n",
      "Epoch: 2/15...  Training Step: 535...  Training loss: 2.2359...  0.1358 sec/batch\n",
      "Epoch: 2/15...  Training Step: 536...  Training loss: 2.2295...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 537...  Training loss: 2.2289...  0.1308 sec/batch\n",
      "Epoch: 2/15...  Training Step: 538...  Training loss: 2.2180...  0.1324 sec/batch\n",
      "Epoch: 2/15...  Training Step: 539...  Training loss: 2.2231...  0.1319 sec/batch\n",
      "Epoch: 2/15...  Training Step: 540...  Training loss: 2.2326...  0.1299 sec/batch\n",
      "Epoch: 2/15...  Training Step: 541...  Training loss: 2.2385...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 542...  Training loss: 2.2283...  0.1359 sec/batch\n",
      "Epoch: 2/15...  Training Step: 543...  Training loss: 2.2436...  0.1322 sec/batch\n",
      "Epoch: 2/15...  Training Step: 544...  Training loss: 2.2438...  0.1345 sec/batch\n",
      "Epoch: 2/15...  Training Step: 545...  Training loss: 2.2232...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 546...  Training loss: 2.1965...  0.1308 sec/batch\n",
      "Epoch: 2/15...  Training Step: 547...  Training loss: 2.2277...  0.1342 sec/batch\n",
      "Epoch: 2/15...  Training Step: 548...  Training loss: 2.2048...  0.1327 sec/batch\n",
      "Epoch: 2/15...  Training Step: 549...  Training loss: 2.1899...  0.1300 sec/batch\n",
      "Epoch: 2/15...  Training Step: 550...  Training loss: 2.2478...  0.1300 sec/batch\n",
      "Epoch: 2/15...  Training Step: 551...  Training loss: 2.1976...  0.1324 sec/batch\n",
      "Epoch: 2/15...  Training Step: 552...  Training loss: 2.2322...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 553...  Training loss: 2.2247...  0.1314 sec/batch\n",
      "Epoch: 2/15...  Training Step: 554...  Training loss: 2.2011...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 555...  Training loss: 2.2167...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 556...  Training loss: 2.2188...  0.1294 sec/batch\n",
      "Epoch: 2/15...  Training Step: 557...  Training loss: 2.1935...  0.1334 sec/batch\n",
      "Epoch: 2/15...  Training Step: 558...  Training loss: 2.2052...  0.1297 sec/batch\n",
      "Epoch: 2/15...  Training Step: 559...  Training loss: 2.2122...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 560...  Training loss: 2.2233...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 561...  Training loss: 2.2435...  0.1351 sec/batch\n",
      "Epoch: 2/15...  Training Step: 562...  Training loss: 2.2020...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 563...  Training loss: 2.2191...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 564...  Training loss: 2.2196...  0.1320 sec/batch\n",
      "Epoch: 2/15...  Training Step: 565...  Training loss: 2.1742...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 566...  Training loss: 2.2099...  0.1340 sec/batch\n",
      "Epoch: 2/15...  Training Step: 567...  Training loss: 2.1930...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 568...  Training loss: 2.1924...  0.1299 sec/batch\n",
      "Epoch: 2/15...  Training Step: 569...  Training loss: 2.2156...  0.1315 sec/batch\n",
      "Epoch: 2/15...  Training Step: 570...  Training loss: 2.2013...  0.1339 sec/batch\n",
      "Epoch: 2/15...  Training Step: 571...  Training loss: 2.2094...  0.1341 sec/batch\n",
      "Epoch: 2/15...  Training Step: 572...  Training loss: 2.2157...  0.1338 sec/batch\n",
      "Epoch: 2/15...  Training Step: 573...  Training loss: 2.2137...  0.1323 sec/batch\n",
      "Epoch: 2/15...  Training Step: 574...  Training loss: 2.1888...  0.1344 sec/batch\n",
      "Epoch: 2/15...  Training Step: 575...  Training loss: 2.1891...  0.1346 sec/batch\n",
      "Epoch: 2/15...  Training Step: 576...  Training loss: 2.2073...  0.1313 sec/batch\n",
      "Epoch: 2/15...  Training Step: 577...  Training loss: 2.1600...  0.1316 sec/batch\n",
      "Epoch: 2/15...  Training Step: 578...  Training loss: 2.1895...  0.1297 sec/batch\n",
      "Epoch: 2/15...  Training Step: 579...  Training loss: 2.2094...  0.1297 sec/batch\n",
      "Epoch: 2/15...  Training Step: 580...  Training loss: 2.1707...  0.1295 sec/batch\n",
      "Epoch: 2/15...  Training Step: 581...  Training loss: 2.1709...  0.1360 sec/batch\n",
      "Epoch: 2/15...  Training Step: 582...  Training loss: 2.1879...  0.1356 sec/batch\n",
      "Epoch: 2/15...  Training Step: 583...  Training loss: 2.1887...  0.1339 sec/batch\n",
      "Epoch: 2/15...  Training Step: 584...  Training loss: 2.2006...  0.1300 sec/batch\n",
      "Epoch: 2/15...  Training Step: 585...  Training loss: 2.2026...  0.1318 sec/batch\n",
      "Epoch: 2/15...  Training Step: 586...  Training loss: 2.2059...  0.1344 sec/batch\n",
      "Epoch: 2/15...  Training Step: 587...  Training loss: 2.1875...  0.1337 sec/batch\n",
      "Epoch: 2/15...  Training Step: 588...  Training loss: 2.1772...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 589...  Training loss: 2.1877...  0.1335 sec/batch\n",
      "Epoch: 2/15...  Training Step: 590...  Training loss: 2.2293...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 591...  Training loss: 2.1831...  0.1331 sec/batch\n",
      "Epoch: 2/15...  Training Step: 592...  Training loss: 2.2026...  0.1320 sec/batch\n",
      "Epoch: 2/15...  Training Step: 593...  Training loss: 2.2063...  0.1317 sec/batch\n",
      "Epoch: 2/15...  Training Step: 594...  Training loss: 2.2099...  0.1319 sec/batch\n",
      "Epoch: 2/15...  Training Step: 595...  Training loss: 2.2334...  0.1352 sec/batch\n",
      "Epoch: 2/15...  Training Step: 596...  Training loss: 2.2389...  0.1332 sec/batch\n",
      "Epoch: 2/15...  Training Step: 597...  Training loss: 2.2391...  0.1301 sec/batch\n",
      "Epoch: 2/15...  Training Step: 598...  Training loss: 2.2020...  0.1350 sec/batch\n",
      "Epoch: 2/15...  Training Step: 599...  Training loss: 2.2368...  0.1336 sec/batch\n",
      "Epoch: 2/15...  Training Step: 600...  Training loss: 2.1911...  0.1280 sec/batch\n",
      "Epoch: 2/15...  Training Step: 601...  Training loss: 2.1862...  0.1316 sec/batch\n",
      "Epoch: 2/15...  Training Step: 602...  Training loss: 2.2061...  0.1351 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/15...  Training Step: 603...  Training loss: 2.2046...  0.1316 sec/batch\n",
      "Epoch: 2/15...  Training Step: 604...  Training loss: 2.1843...  0.1330 sec/batch\n",
      "Epoch: 2/15...  Training Step: 605...  Training loss: 2.2015...  0.1303 sec/batch\n",
      "Epoch: 2/15...  Training Step: 606...  Training loss: 2.1977...  0.1357 sec/batch\n",
      "Epoch: 3/15...  Training Step: 607...  Training loss: 2.2580...  0.1310 sec/batch\n",
      "Epoch: 3/15...  Training Step: 608...  Training loss: 2.2120...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 609...  Training loss: 2.1770...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 610...  Training loss: 2.1983...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 611...  Training loss: 2.1949...  0.1333 sec/batch\n",
      "Epoch: 3/15...  Training Step: 612...  Training loss: 2.2196...  0.1287 sec/batch\n",
      "Epoch: 3/15...  Training Step: 613...  Training loss: 2.2023...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 614...  Training loss: 2.2047...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 615...  Training loss: 2.1682...  0.1315 sec/batch\n",
      "Epoch: 3/15...  Training Step: 616...  Training loss: 2.1815...  0.1315 sec/batch\n",
      "Epoch: 3/15...  Training Step: 617...  Training loss: 2.1684...  0.1352 sec/batch\n",
      "Epoch: 3/15...  Training Step: 618...  Training loss: 2.1788...  0.1826 sec/batch\n",
      "Epoch: 3/15...  Training Step: 619...  Training loss: 2.1995...  0.1838 sec/batch\n",
      "Epoch: 3/15...  Training Step: 620...  Training loss: 2.1902...  0.1320 sec/batch\n",
      "Epoch: 3/15...  Training Step: 621...  Training loss: 2.2023...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 622...  Training loss: 2.1899...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 623...  Training loss: 2.2231...  0.1349 sec/batch\n",
      "Epoch: 3/15...  Training Step: 624...  Training loss: 2.2159...  0.1314 sec/batch\n",
      "Epoch: 3/15...  Training Step: 625...  Training loss: 2.2308...  0.1346 sec/batch\n",
      "Epoch: 3/15...  Training Step: 626...  Training loss: 2.2227...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 627...  Training loss: 2.2108...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 628...  Training loss: 2.1993...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 629...  Training loss: 2.2127...  0.1300 sec/batch\n",
      "Epoch: 3/15...  Training Step: 630...  Training loss: 2.1849...  0.1350 sec/batch\n",
      "Epoch: 3/15...  Training Step: 631...  Training loss: 2.2319...  0.1307 sec/batch\n",
      "Epoch: 3/15...  Training Step: 632...  Training loss: 2.2102...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 633...  Training loss: 2.1930...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 634...  Training loss: 2.2183...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 635...  Training loss: 2.2361...  0.1318 sec/batch\n",
      "Epoch: 3/15...  Training Step: 636...  Training loss: 2.1962...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 637...  Training loss: 2.1967...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 638...  Training loss: 2.2023...  0.1347 sec/batch\n",
      "Epoch: 3/15...  Training Step: 639...  Training loss: 2.1882...  0.1320 sec/batch\n",
      "Epoch: 3/15...  Training Step: 640...  Training loss: 2.1888...  0.1357 sec/batch\n",
      "Epoch: 3/15...  Training Step: 641...  Training loss: 2.1920...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 642...  Training loss: 2.1761...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 643...  Training loss: 2.2014...  0.1299 sec/batch\n",
      "Epoch: 3/15...  Training Step: 644...  Training loss: 2.1595...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 645...  Training loss: 2.1933...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 646...  Training loss: 2.1849...  0.1304 sec/batch\n",
      "Epoch: 3/15...  Training Step: 647...  Training loss: 2.1916...  0.1322 sec/batch\n",
      "Epoch: 3/15...  Training Step: 648...  Training loss: 2.1767...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 649...  Training loss: 2.1873...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 650...  Training loss: 2.1707...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 651...  Training loss: 2.1834...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 652...  Training loss: 2.1626...  0.1324 sec/batch\n",
      "Epoch: 3/15...  Training Step: 653...  Training loss: 2.1606...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 654...  Training loss: 2.1708...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 655...  Training loss: 2.1524...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 656...  Training loss: 2.1399...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 657...  Training loss: 2.1624...  0.1357 sec/batch\n",
      "Epoch: 3/15...  Training Step: 658...  Training loss: 2.1634...  0.1298 sec/batch\n",
      "Epoch: 3/15...  Training Step: 659...  Training loss: 2.1634...  0.1325 sec/batch\n",
      "Epoch: 3/15...  Training Step: 660...  Training loss: 2.1538...  0.1366 sec/batch\n",
      "Epoch: 3/15...  Training Step: 661...  Training loss: 2.1560...  0.1351 sec/batch\n",
      "Epoch: 3/15...  Training Step: 662...  Training loss: 2.1567...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 663...  Training loss: 2.1609...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 664...  Training loss: 2.1637...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 665...  Training loss: 2.1448...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 666...  Training loss: 2.1463...  0.1313 sec/batch\n",
      "Epoch: 3/15...  Training Step: 667...  Training loss: 2.1503...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 668...  Training loss: 2.1699...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 669...  Training loss: 2.1416...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 670...  Training loss: 2.1658...  0.1301 sec/batch\n",
      "Epoch: 3/15...  Training Step: 671...  Training loss: 2.1744...  0.1308 sec/batch\n",
      "Epoch: 3/15...  Training Step: 672...  Training loss: 2.2088...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 673...  Training loss: 2.1823...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 674...  Training loss: 2.1616...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 675...  Training loss: 2.1728...  0.1318 sec/batch\n",
      "Epoch: 3/15...  Training Step: 676...  Training loss: 2.1493...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 677...  Training loss: 2.1794...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 678...  Training loss: 2.1556...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 679...  Training loss: 2.1701...  0.1362 sec/batch\n",
      "Epoch: 3/15...  Training Step: 680...  Training loss: 2.1631...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 681...  Training loss: 2.1499...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 682...  Training loss: 2.1702...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 683...  Training loss: 2.1762...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 684...  Training loss: 2.1931...  0.1304 sec/batch\n",
      "Epoch: 3/15...  Training Step: 685...  Training loss: 2.1585...  0.1302 sec/batch\n",
      "Epoch: 3/15...  Training Step: 686...  Training loss: 2.1610...  0.1347 sec/batch\n",
      "Epoch: 3/15...  Training Step: 687...  Training loss: 2.1771...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 688...  Training loss: 2.1832...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 689...  Training loss: 2.1626...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 690...  Training loss: 2.1519...  0.1323 sec/batch\n",
      "Epoch: 3/15...  Training Step: 691...  Training loss: 2.1692...  0.1316 sec/batch\n",
      "Epoch: 3/15...  Training Step: 692...  Training loss: 2.1481...  0.1314 sec/batch\n",
      "Epoch: 3/15...  Training Step: 693...  Training loss: 2.1562...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 694...  Training loss: 2.1590...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 695...  Training loss: 2.1506...  0.1311 sec/batch\n",
      "Epoch: 3/15...  Training Step: 696...  Training loss: 2.1658...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 697...  Training loss: 2.1408...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 698...  Training loss: 2.1615...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 699...  Training loss: 2.1648...  0.1313 sec/batch\n",
      "Epoch: 3/15...  Training Step: 700...  Training loss: 2.1308...  0.1304 sec/batch\n",
      "Epoch: 3/15...  Training Step: 701...  Training loss: 2.1464...  0.1333 sec/batch\n",
      "Epoch: 3/15...  Training Step: 702...  Training loss: 2.1415...  0.1332 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15...  Training Step: 703...  Training loss: 2.1571...  0.1285 sec/batch\n",
      "Epoch: 3/15...  Training Step: 704...  Training loss: 2.1681...  0.1306 sec/batch\n",
      "Epoch: 3/15...  Training Step: 705...  Training loss: 2.1375...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 706...  Training loss: 2.1518...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 707...  Training loss: 2.1527...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 708...  Training loss: 2.1436...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 709...  Training loss: 2.1499...  0.1322 sec/batch\n",
      "Epoch: 3/15...  Training Step: 710...  Training loss: 2.1368...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 711...  Training loss: 2.1480...  0.1313 sec/batch\n",
      "Epoch: 3/15...  Training Step: 712...  Training loss: 2.1551...  0.1307 sec/batch\n",
      "Epoch: 3/15...  Training Step: 713...  Training loss: 2.1504...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 714...  Training loss: 2.1493...  0.1315 sec/batch\n",
      "Epoch: 3/15...  Training Step: 715...  Training loss: 2.1645...  0.1327 sec/batch\n",
      "Epoch: 3/15...  Training Step: 716...  Training loss: 2.1619...  0.1325 sec/batch\n",
      "Epoch: 3/15...  Training Step: 717...  Training loss: 2.1831...  0.1311 sec/batch\n",
      "Epoch: 3/15...  Training Step: 718...  Training loss: 2.1359...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 719...  Training loss: 2.1357...  0.1301 sec/batch\n",
      "Epoch: 3/15...  Training Step: 720...  Training loss: 2.1483...  0.1322 sec/batch\n",
      "Epoch: 3/15...  Training Step: 721...  Training loss: 2.1599...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 722...  Training loss: 2.1598...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 723...  Training loss: 2.1540...  0.1350 sec/batch\n",
      "Epoch: 3/15...  Training Step: 724...  Training loss: 2.1542...  0.1333 sec/batch\n",
      "Epoch: 3/15...  Training Step: 725...  Training loss: 2.1656...  0.1314 sec/batch\n",
      "Epoch: 3/15...  Training Step: 726...  Training loss: 2.1265...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 727...  Training loss: 2.1226...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 728...  Training loss: 2.1604...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 729...  Training loss: 2.1545...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 730...  Training loss: 2.1532...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 731...  Training loss: 2.1668...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 732...  Training loss: 2.1538...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 733...  Training loss: 2.1659...  0.1301 sec/batch\n",
      "Epoch: 3/15...  Training Step: 734...  Training loss: 2.1848...  0.1303 sec/batch\n",
      "Epoch: 3/15...  Training Step: 735...  Training loss: 2.1588...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 736...  Training loss: 2.1864...  0.1318 sec/batch\n",
      "Epoch: 3/15...  Training Step: 737...  Training loss: 2.1649...  0.1347 sec/batch\n",
      "Epoch: 3/15...  Training Step: 738...  Training loss: 2.1694...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 739...  Training loss: 2.1615...  0.1302 sec/batch\n",
      "Epoch: 3/15...  Training Step: 740...  Training loss: 2.1907...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 741...  Training loss: 2.1656...  0.1337 sec/batch\n",
      "Epoch: 3/15...  Training Step: 742...  Training loss: 2.1433...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 743...  Training loss: 2.1818...  0.1325 sec/batch\n",
      "Epoch: 3/15...  Training Step: 744...  Training loss: 2.1570...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 745...  Training loss: 2.1594...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 746...  Training loss: 2.1646...  0.1305 sec/batch\n",
      "Epoch: 3/15...  Training Step: 747...  Training loss: 2.1406...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 748...  Training loss: 2.1246...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 749...  Training loss: 2.1276...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 750...  Training loss: 2.1487...  0.1314 sec/batch\n",
      "Epoch: 3/15...  Training Step: 751...  Training loss: 2.1297...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 752...  Training loss: 2.1638...  0.1337 sec/batch\n",
      "Epoch: 3/15...  Training Step: 753...  Training loss: 2.1390...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 754...  Training loss: 2.1622...  0.1323 sec/batch\n",
      "Epoch: 3/15...  Training Step: 755...  Training loss: 2.1580...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 756...  Training loss: 2.1337...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 757...  Training loss: 2.1605...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 758...  Training loss: 2.1503...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 759...  Training loss: 2.1464...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 760...  Training loss: 2.1592...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 761...  Training loss: 2.1415...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 762...  Training loss: 2.1558...  0.1342 sec/batch\n",
      "Epoch: 3/15...  Training Step: 763...  Training loss: 2.1441...  0.1795 sec/batch\n",
      "Epoch: 3/15...  Training Step: 764...  Training loss: 2.1507...  0.1840 sec/batch\n",
      "Epoch: 3/15...  Training Step: 765...  Training loss: 2.1495...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 766...  Training loss: 2.1504...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 767...  Training loss: 2.1307...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 768...  Training loss: 2.1292...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 769...  Training loss: 2.1289...  0.1349 sec/batch\n",
      "Epoch: 3/15...  Training Step: 770...  Training loss: 2.1337...  0.1318 sec/batch\n",
      "Epoch: 3/15...  Training Step: 771...  Training loss: 2.1264...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 772...  Training loss: 2.1588...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 773...  Training loss: 2.1284...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 774...  Training loss: 2.1325...  0.1310 sec/batch\n",
      "Epoch: 3/15...  Training Step: 775...  Training loss: 2.1523...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 776...  Training loss: 2.1485...  0.1326 sec/batch\n",
      "Epoch: 3/15...  Training Step: 777...  Training loss: 2.1394...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 778...  Training loss: 2.1445...  0.1311 sec/batch\n",
      "Epoch: 3/15...  Training Step: 779...  Training loss: 2.1324...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 780...  Training loss: 2.1328...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 781...  Training loss: 2.1283...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 782...  Training loss: 2.1169...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 783...  Training loss: 2.1365...  0.1349 sec/batch\n",
      "Epoch: 3/15...  Training Step: 784...  Training loss: 2.1211...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 785...  Training loss: 2.1225...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 786...  Training loss: 2.1307...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 787...  Training loss: 2.1251...  0.1311 sec/batch\n",
      "Epoch: 3/15...  Training Step: 788...  Training loss: 2.1431...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 789...  Training loss: 2.1345...  0.1337 sec/batch\n",
      "Epoch: 3/15...  Training Step: 790...  Training loss: 2.1160...  0.1315 sec/batch\n",
      "Epoch: 3/15...  Training Step: 791...  Training loss: 2.1241...  0.1343 sec/batch\n",
      "Epoch: 3/15...  Training Step: 792...  Training loss: 2.1286...  0.1342 sec/batch\n",
      "Epoch: 3/15...  Training Step: 793...  Training loss: 2.1280...  0.1287 sec/batch\n",
      "Epoch: 3/15...  Training Step: 794...  Training loss: 2.1208...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 795...  Training loss: 2.1185...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 796...  Training loss: 2.1184...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 797...  Training loss: 2.1136...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 798...  Training loss: 2.1140...  0.1346 sec/batch\n",
      "Epoch: 3/15...  Training Step: 799...  Training loss: 2.1068...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 800...  Training loss: 2.1329...  0.1327 sec/batch\n",
      "Epoch: 3/15...  Training Step: 801...  Training loss: 2.1008...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 802...  Training loss: 2.1049...  0.1341 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15...  Training Step: 803...  Training loss: 2.1392...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 804...  Training loss: 2.1263...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 805...  Training loss: 2.1289...  0.1357 sec/batch\n",
      "Epoch: 3/15...  Training Step: 806...  Training loss: 2.1389...  0.1315 sec/batch\n",
      "Epoch: 3/15...  Training Step: 807...  Training loss: 2.1306...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 808...  Training loss: 2.1332...  0.1309 sec/batch\n",
      "Epoch: 3/15...  Training Step: 809...  Training loss: 2.1547...  0.1299 sec/batch\n",
      "Epoch: 3/15...  Training Step: 810...  Training loss: 2.1687...  0.1326 sec/batch\n",
      "Epoch: 3/15...  Training Step: 811...  Training loss: 2.1582...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 812...  Training loss: 2.1381...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 813...  Training loss: 2.1551...  0.1327 sec/batch\n",
      "Epoch: 3/15...  Training Step: 814...  Training loss: 2.1554...  0.1350 sec/batch\n",
      "Epoch: 3/15...  Training Step: 815...  Training loss: 2.1441...  0.1298 sec/batch\n",
      "Epoch: 3/15...  Training Step: 816...  Training loss: 2.1429...  0.1306 sec/batch\n",
      "Epoch: 3/15...  Training Step: 817...  Training loss: 2.1265...  0.1320 sec/batch\n",
      "Epoch: 3/15...  Training Step: 818...  Training loss: 2.1282...  0.1307 sec/batch\n",
      "Epoch: 3/15...  Training Step: 819...  Training loss: 2.1380...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 820...  Training loss: 2.1191...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 821...  Training loss: 2.1246...  0.1311 sec/batch\n",
      "Epoch: 3/15...  Training Step: 822...  Training loss: 2.1386...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 823...  Training loss: 2.1201...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 824...  Training loss: 2.1220...  0.1353 sec/batch\n",
      "Epoch: 3/15...  Training Step: 825...  Training loss: 2.1286...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 826...  Training loss: 2.1262...  0.1306 sec/batch\n",
      "Epoch: 3/15...  Training Step: 827...  Training loss: 2.1321...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 828...  Training loss: 2.1191...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 829...  Training loss: 2.1180...  0.1352 sec/batch\n",
      "Epoch: 3/15...  Training Step: 830...  Training loss: 2.1154...  0.1319 sec/batch\n",
      "Epoch: 3/15...  Training Step: 831...  Training loss: 2.1233...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 832...  Training loss: 2.1272...  0.1300 sec/batch\n",
      "Epoch: 3/15...  Training Step: 833...  Training loss: 2.1312...  0.1351 sec/batch\n",
      "Epoch: 3/15...  Training Step: 834...  Training loss: 2.1178...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 835...  Training loss: 2.1371...  0.1339 sec/batch\n",
      "Epoch: 3/15...  Training Step: 836...  Training loss: 2.1508...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 837...  Training loss: 2.1132...  0.1796 sec/batch\n",
      "Epoch: 3/15...  Training Step: 838...  Training loss: 2.1157...  0.1835 sec/batch\n",
      "Epoch: 3/15...  Training Step: 839...  Training loss: 2.1204...  0.1324 sec/batch\n",
      "Epoch: 3/15...  Training Step: 840...  Training loss: 2.1328...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 841...  Training loss: 2.1133...  0.1324 sec/batch\n",
      "Epoch: 3/15...  Training Step: 842...  Training loss: 2.1176...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 843...  Training loss: 2.1312...  0.1313 sec/batch\n",
      "Epoch: 3/15...  Training Step: 844...  Training loss: 2.1361...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 845...  Training loss: 2.1302...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 846...  Training loss: 2.1314...  0.1353 sec/batch\n",
      "Epoch: 3/15...  Training Step: 847...  Training loss: 2.1319...  0.1337 sec/batch\n",
      "Epoch: 3/15...  Training Step: 848...  Training loss: 2.1162...  0.1349 sec/batch\n",
      "Epoch: 3/15...  Training Step: 849...  Training loss: 2.0940...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 850...  Training loss: 2.1248...  0.1367 sec/batch\n",
      "Epoch: 3/15...  Training Step: 851...  Training loss: 2.0971...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 852...  Training loss: 2.0845...  0.1337 sec/batch\n",
      "Epoch: 3/15...  Training Step: 853...  Training loss: 2.1440...  0.1314 sec/batch\n",
      "Epoch: 3/15...  Training Step: 854...  Training loss: 2.0970...  0.1333 sec/batch\n",
      "Epoch: 3/15...  Training Step: 855...  Training loss: 2.1141...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 856...  Training loss: 2.1198...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 857...  Training loss: 2.0919...  0.1341 sec/batch\n",
      "Epoch: 3/15...  Training Step: 858...  Training loss: 2.1070...  0.1308 sec/batch\n",
      "Epoch: 3/15...  Training Step: 859...  Training loss: 2.1134...  0.1357 sec/batch\n",
      "Epoch: 3/15...  Training Step: 860...  Training loss: 2.0945...  0.1316 sec/batch\n",
      "Epoch: 3/15...  Training Step: 861...  Training loss: 2.0959...  0.1791 sec/batch\n",
      "Epoch: 3/15...  Training Step: 862...  Training loss: 2.1012...  0.1836 sec/batch\n",
      "Epoch: 3/15...  Training Step: 863...  Training loss: 2.1209...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 864...  Training loss: 2.1442...  0.1344 sec/batch\n",
      "Epoch: 3/15...  Training Step: 865...  Training loss: 2.0938...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 866...  Training loss: 2.1164...  0.1348 sec/batch\n",
      "Epoch: 3/15...  Training Step: 867...  Training loss: 2.1237...  0.1305 sec/batch\n",
      "Epoch: 3/15...  Training Step: 868...  Training loss: 2.0703...  0.1305 sec/batch\n",
      "Epoch: 3/15...  Training Step: 869...  Training loss: 2.1059...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 870...  Training loss: 2.0916...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 871...  Training loss: 2.0955...  0.1340 sec/batch\n",
      "Epoch: 3/15...  Training Step: 872...  Training loss: 2.1057...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 873...  Training loss: 2.0926...  0.1326 sec/batch\n",
      "Epoch: 3/15...  Training Step: 874...  Training loss: 2.1117...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 875...  Training loss: 2.1130...  0.1332 sec/batch\n",
      "Epoch: 3/15...  Training Step: 876...  Training loss: 2.1200...  0.1324 sec/batch\n",
      "Epoch: 3/15...  Training Step: 877...  Training loss: 2.0926...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 878...  Training loss: 2.0966...  0.1300 sec/batch\n",
      "Epoch: 3/15...  Training Step: 879...  Training loss: 2.1068...  0.1328 sec/batch\n",
      "Epoch: 3/15...  Training Step: 880...  Training loss: 2.0531...  0.1310 sec/batch\n",
      "Epoch: 3/15...  Training Step: 881...  Training loss: 2.1007...  0.1303 sec/batch\n",
      "Epoch: 3/15...  Training Step: 882...  Training loss: 2.1126...  0.1320 sec/batch\n",
      "Epoch: 3/15...  Training Step: 883...  Training loss: 2.0740...  0.1356 sec/batch\n",
      "Epoch: 3/15...  Training Step: 884...  Training loss: 2.0832...  0.1334 sec/batch\n",
      "Epoch: 3/15...  Training Step: 885...  Training loss: 2.0915...  0.1320 sec/batch\n",
      "Epoch: 3/15...  Training Step: 886...  Training loss: 2.0932...  0.1331 sec/batch\n",
      "Epoch: 3/15...  Training Step: 887...  Training loss: 2.1037...  0.1317 sec/batch\n",
      "Epoch: 3/15...  Training Step: 888...  Training loss: 2.1030...  0.1330 sec/batch\n",
      "Epoch: 3/15...  Training Step: 889...  Training loss: 2.1079...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 890...  Training loss: 2.0960...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 891...  Training loss: 2.0852...  0.1336 sec/batch\n",
      "Epoch: 3/15...  Training Step: 892...  Training loss: 2.0961...  0.1329 sec/batch\n",
      "Epoch: 3/15...  Training Step: 893...  Training loss: 2.1330...  0.1306 sec/batch\n",
      "Epoch: 3/15...  Training Step: 894...  Training loss: 2.0932...  0.1352 sec/batch\n",
      "Epoch: 3/15...  Training Step: 895...  Training loss: 2.0980...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 896...  Training loss: 2.1202...  0.1345 sec/batch\n",
      "Epoch: 3/15...  Training Step: 897...  Training loss: 2.1223...  0.1321 sec/batch\n",
      "Epoch: 3/15...  Training Step: 898...  Training loss: 2.1446...  0.1312 sec/batch\n",
      "Epoch: 3/15...  Training Step: 899...  Training loss: 2.1423...  0.1335 sec/batch\n",
      "Epoch: 3/15...  Training Step: 900...  Training loss: 2.1518...  0.1351 sec/batch\n",
      "Epoch: 3/15...  Training Step: 901...  Training loss: 2.1078...  0.1326 sec/batch\n",
      "Epoch: 3/15...  Training Step: 902...  Training loss: 2.1447...  0.1331 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/15...  Training Step: 903...  Training loss: 2.1066...  0.1355 sec/batch\n",
      "Epoch: 3/15...  Training Step: 904...  Training loss: 2.0932...  0.1353 sec/batch\n",
      "Epoch: 3/15...  Training Step: 905...  Training loss: 2.1146...  0.1323 sec/batch\n",
      "Epoch: 3/15...  Training Step: 906...  Training loss: 2.1114...  0.1323 sec/batch\n",
      "Epoch: 3/15...  Training Step: 907...  Training loss: 2.1096...  0.1353 sec/batch\n",
      "Epoch: 3/15...  Training Step: 908...  Training loss: 2.1060...  0.1338 sec/batch\n",
      "Epoch: 3/15...  Training Step: 909...  Training loss: 2.1002...  0.1314 sec/batch\n",
      "Epoch: 4/15...  Training Step: 910...  Training loss: 2.1731...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 911...  Training loss: 2.1257...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 912...  Training loss: 2.0932...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 913...  Training loss: 2.1089...  0.1298 sec/batch\n",
      "Epoch: 4/15...  Training Step: 914...  Training loss: 2.1104...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 915...  Training loss: 2.1229...  0.1346 sec/batch\n",
      "Epoch: 4/15...  Training Step: 916...  Training loss: 2.1138...  0.1345 sec/batch\n",
      "Epoch: 4/15...  Training Step: 917...  Training loss: 2.1167...  0.1309 sec/batch\n",
      "Epoch: 4/15...  Training Step: 918...  Training loss: 2.0770...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 919...  Training loss: 2.0845...  0.1303 sec/batch\n",
      "Epoch: 4/15...  Training Step: 920...  Training loss: 2.0690...  0.1350 sec/batch\n",
      "Epoch: 4/15...  Training Step: 921...  Training loss: 2.0939...  0.1795 sec/batch\n",
      "Epoch: 4/15...  Training Step: 922...  Training loss: 2.1102...  0.1839 sec/batch\n",
      "Epoch: 4/15...  Training Step: 923...  Training loss: 2.1004...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 924...  Training loss: 2.1149...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 925...  Training loss: 2.1055...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 926...  Training loss: 2.1287...  0.1350 sec/batch\n",
      "Epoch: 4/15...  Training Step: 927...  Training loss: 2.1162...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 928...  Training loss: 2.1430...  0.1344 sec/batch\n",
      "Epoch: 4/15...  Training Step: 929...  Training loss: 2.1360...  0.1345 sec/batch\n",
      "Epoch: 4/15...  Training Step: 930...  Training loss: 2.1268...  0.1353 sec/batch\n",
      "Epoch: 4/15...  Training Step: 931...  Training loss: 2.1075...  0.1360 sec/batch\n",
      "Epoch: 4/15...  Training Step: 932...  Training loss: 2.1172...  0.1331 sec/batch\n",
      "Epoch: 4/15...  Training Step: 933...  Training loss: 2.1068...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 934...  Training loss: 2.1396...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 935...  Training loss: 2.1258...  0.1352 sec/batch\n",
      "Epoch: 4/15...  Training Step: 936...  Training loss: 2.1082...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 937...  Training loss: 2.1288...  0.1358 sec/batch\n",
      "Epoch: 4/15...  Training Step: 938...  Training loss: 2.1405...  0.1346 sec/batch\n",
      "Epoch: 4/15...  Training Step: 939...  Training loss: 2.1134...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 940...  Training loss: 2.1149...  0.1352 sec/batch\n",
      "Epoch: 4/15...  Training Step: 941...  Training loss: 2.1088...  0.1367 sec/batch\n",
      "Epoch: 4/15...  Training Step: 942...  Training loss: 2.1024...  0.1325 sec/batch\n",
      "Epoch: 4/15...  Training Step: 943...  Training loss: 2.1095...  0.1346 sec/batch\n",
      "Epoch: 4/15...  Training Step: 944...  Training loss: 2.1028...  0.1317 sec/batch\n",
      "Epoch: 4/15...  Training Step: 945...  Training loss: 2.0924...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 946...  Training loss: 2.1122...  0.1346 sec/batch\n",
      "Epoch: 4/15...  Training Step: 947...  Training loss: 2.0822...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 948...  Training loss: 2.1058...  0.1325 sec/batch\n",
      "Epoch: 4/15...  Training Step: 949...  Training loss: 2.1007...  0.1358 sec/batch\n",
      "Epoch: 4/15...  Training Step: 950...  Training loss: 2.1003...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 951...  Training loss: 2.0941...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 952...  Training loss: 2.1009...  0.1337 sec/batch\n",
      "Epoch: 4/15...  Training Step: 953...  Training loss: 2.0877...  0.1313 sec/batch\n",
      "Epoch: 4/15...  Training Step: 954...  Training loss: 2.0987...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 955...  Training loss: 2.0769...  0.1348 sec/batch\n",
      "Epoch: 4/15...  Training Step: 956...  Training loss: 2.0729...  0.1314 sec/batch\n",
      "Epoch: 4/15...  Training Step: 957...  Training loss: 2.0818...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 958...  Training loss: 2.0771...  0.1308 sec/batch\n",
      "Epoch: 4/15...  Training Step: 959...  Training loss: 2.0582...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 960...  Training loss: 2.0792...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 961...  Training loss: 2.0816...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 962...  Training loss: 2.0744...  0.1771 sec/batch\n",
      "Epoch: 4/15...  Training Step: 963...  Training loss: 2.0720...  0.1844 sec/batch\n",
      "Epoch: 4/15...  Training Step: 964...  Training loss: 2.0765...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 965...  Training loss: 2.0782...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 966...  Training loss: 2.0807...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 967...  Training loss: 2.0852...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 968...  Training loss: 2.0625...  0.1318 sec/batch\n",
      "Epoch: 4/15...  Training Step: 969...  Training loss: 2.0604...  0.1337 sec/batch\n",
      "Epoch: 4/15...  Training Step: 970...  Training loss: 2.0662...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 971...  Training loss: 2.0946...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 972...  Training loss: 2.0631...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 973...  Training loss: 2.0855...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 974...  Training loss: 2.0945...  0.1328 sec/batch\n",
      "Epoch: 4/15...  Training Step: 975...  Training loss: 2.1185...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 976...  Training loss: 2.0971...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 977...  Training loss: 2.0727...  0.1298 sec/batch\n",
      "Epoch: 4/15...  Training Step: 978...  Training loss: 2.0880...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 979...  Training loss: 2.0710...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 980...  Training loss: 2.0996...  0.1302 sec/batch\n",
      "Epoch: 4/15...  Training Step: 981...  Training loss: 2.0862...  0.1305 sec/batch\n",
      "Epoch: 4/15...  Training Step: 982...  Training loss: 2.0799...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 983...  Training loss: 2.0806...  0.1344 sec/batch\n",
      "Epoch: 4/15...  Training Step: 984...  Training loss: 2.0624...  0.1350 sec/batch\n",
      "Epoch: 4/15...  Training Step: 985...  Training loss: 2.0929...  0.1308 sec/batch\n",
      "Epoch: 4/15...  Training Step: 986...  Training loss: 2.0945...  0.1355 sec/batch\n",
      "Epoch: 4/15...  Training Step: 987...  Training loss: 2.1155...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 988...  Training loss: 2.0930...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 989...  Training loss: 2.0832...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 990...  Training loss: 2.1078...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 991...  Training loss: 2.1078...  0.1296 sec/batch\n",
      "Epoch: 4/15...  Training Step: 992...  Training loss: 2.0824...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 993...  Training loss: 2.0735...  0.1352 sec/batch\n",
      "Epoch: 4/15...  Training Step: 994...  Training loss: 2.0912...  0.1314 sec/batch\n",
      "Epoch: 4/15...  Training Step: 995...  Training loss: 2.0715...  0.1320 sec/batch\n",
      "Epoch: 4/15...  Training Step: 996...  Training loss: 2.0788...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 997...  Training loss: 2.0797...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 998...  Training loss: 2.0753...  0.1360 sec/batch\n",
      "Epoch: 4/15...  Training Step: 999...  Training loss: 2.0849...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1000...  Training loss: 2.0697...  0.1298 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1001...  Training loss: 2.0899...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1002...  Training loss: 2.0812...  0.1330 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15...  Training Step: 1003...  Training loss: 2.0544...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1004...  Training loss: 2.0559...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1005...  Training loss: 2.0672...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1006...  Training loss: 2.0747...  0.1353 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1007...  Training loss: 2.0930...  0.1795 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1008...  Training loss: 2.0601...  0.1842 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1009...  Training loss: 2.0745...  0.1318 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1010...  Training loss: 2.0874...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1011...  Training loss: 2.0763...  0.1301 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1012...  Training loss: 2.0824...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1013...  Training loss: 2.0614...  0.1294 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1014...  Training loss: 2.0764...  0.1344 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1015...  Training loss: 2.0816...  0.1301 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1016...  Training loss: 2.0805...  0.1313 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1017...  Training loss: 2.0731...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1018...  Training loss: 2.0911...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1019...  Training loss: 2.0804...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1020...  Training loss: 2.0987...  0.1359 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1021...  Training loss: 2.0729...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1022...  Training loss: 2.0592...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1023...  Training loss: 2.0756...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1024...  Training loss: 2.0921...  0.1348 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1025...  Training loss: 2.0811...  0.1324 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1026...  Training loss: 2.0829...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1027...  Training loss: 2.0826...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1028...  Training loss: 2.0938...  0.1309 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1029...  Training loss: 2.0531...  0.1313 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1030...  Training loss: 2.0568...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1031...  Training loss: 2.0871...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1032...  Training loss: 2.0838...  0.1323 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1033...  Training loss: 2.0865...  0.1348 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1034...  Training loss: 2.0936...  0.1331 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1035...  Training loss: 2.0939...  0.1297 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1036...  Training loss: 2.0854...  0.1320 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1037...  Training loss: 2.1175...  0.1343 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1038...  Training loss: 2.0894...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1039...  Training loss: 2.1232...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1040...  Training loss: 2.0919...  0.1310 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1041...  Training loss: 2.0911...  0.1314 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1042...  Training loss: 2.0899...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1043...  Training loss: 2.1247...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1044...  Training loss: 2.1000...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1045...  Training loss: 2.0648...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1046...  Training loss: 2.1130...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1047...  Training loss: 2.0852...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1048...  Training loss: 2.0811...  0.1322 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1049...  Training loss: 2.0915...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1050...  Training loss: 2.0694...  0.1817 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1051...  Training loss: 2.0539...  0.1839 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1052...  Training loss: 2.0530...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1053...  Training loss: 2.0765...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1054...  Training loss: 2.0550...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1055...  Training loss: 2.0855...  0.1355 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1056...  Training loss: 2.0670...  0.1345 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1057...  Training loss: 2.0929...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1058...  Training loss: 2.0800...  0.1319 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1059...  Training loss: 2.0616...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1060...  Training loss: 2.0887...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1061...  Training loss: 2.0725...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1062...  Training loss: 2.0772...  0.1305 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1063...  Training loss: 2.0896...  0.1359 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1064...  Training loss: 2.0729...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1065...  Training loss: 2.0855...  0.1345 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1066...  Training loss: 2.0727...  0.1361 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1067...  Training loss: 2.0878...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1068...  Training loss: 2.0848...  0.1317 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1069...  Training loss: 2.0842...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1070...  Training loss: 2.0560...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1071...  Training loss: 2.0557...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1072...  Training loss: 2.0598...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1073...  Training loss: 2.0647...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1074...  Training loss: 2.0562...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1075...  Training loss: 2.0963...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1076...  Training loss: 2.0585...  0.1319 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1077...  Training loss: 2.0667...  0.1328 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1078...  Training loss: 2.0726...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1079...  Training loss: 2.0749...  0.1290 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1080...  Training loss: 2.0691...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1081...  Training loss: 2.0735...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1082...  Training loss: 2.0681...  0.1328 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1083...  Training loss: 2.0616...  0.1328 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1084...  Training loss: 2.0654...  0.1331 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1085...  Training loss: 2.0394...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1086...  Training loss: 2.0675...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1087...  Training loss: 2.0531...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1088...  Training loss: 2.0594...  0.1352 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1089...  Training loss: 2.0533...  0.1315 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1090...  Training loss: 2.0587...  0.1354 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1091...  Training loss: 2.0749...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1092...  Training loss: 2.0657...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1093...  Training loss: 2.0515...  0.1307 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1094...  Training loss: 2.0625...  0.1320 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1095...  Training loss: 2.0577...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1096...  Training loss: 2.0604...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1097...  Training loss: 2.0556...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1098...  Training loss: 2.0529...  0.1325 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1099...  Training loss: 2.0503...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1100...  Training loss: 2.0484...  0.1337 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1101...  Training loss: 2.0503...  0.1328 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1102...  Training loss: 2.0491...  0.1335 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15...  Training Step: 1103...  Training loss: 2.0675...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1104...  Training loss: 2.0436...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1105...  Training loss: 2.0442...  0.1348 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1106...  Training loss: 2.0756...  0.1304 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1107...  Training loss: 2.0658...  0.1811 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1108...  Training loss: 2.0654...  0.1839 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1109...  Training loss: 2.0735...  0.1345 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1110...  Training loss: 2.0705...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1111...  Training loss: 2.0643...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1112...  Training loss: 2.0827...  0.1344 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1113...  Training loss: 2.1046...  0.1299 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1114...  Training loss: 2.1023...  0.1326 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1115...  Training loss: 2.0803...  0.1334 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1116...  Training loss: 2.0906...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1117...  Training loss: 2.0811...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1118...  Training loss: 2.0791...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1119...  Training loss: 2.0807...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1120...  Training loss: 2.0609...  0.1316 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1121...  Training loss: 2.0632...  0.1304 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1122...  Training loss: 2.0762...  0.1325 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1123...  Training loss: 2.0616...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1124...  Training loss: 2.0709...  0.1356 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1125...  Training loss: 2.0763...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1126...  Training loss: 2.0496...  0.1300 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1127...  Training loss: 2.0676...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1128...  Training loss: 2.0711...  0.1350 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1129...  Training loss: 2.0687...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1130...  Training loss: 2.0664...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1131...  Training loss: 2.0695...  0.1309 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1132...  Training loss: 2.0544...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1133...  Training loss: 2.0609...  0.1365 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1134...  Training loss: 2.0560...  0.1321 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1135...  Training loss: 2.0706...  0.1348 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1136...  Training loss: 2.0713...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1137...  Training loss: 2.0623...  0.1309 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1138...  Training loss: 2.0746...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1139...  Training loss: 2.0813...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1140...  Training loss: 2.0496...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1141...  Training loss: 2.0573...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1142...  Training loss: 2.0550...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1143...  Training loss: 2.0650...  0.1310 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1144...  Training loss: 2.0548...  0.1318 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1145...  Training loss: 2.0526...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1146...  Training loss: 2.0619...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1147...  Training loss: 2.0755...  0.1337 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1148...  Training loss: 2.0613...  0.1355 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1149...  Training loss: 2.0665...  0.1303 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1150...  Training loss: 2.0662...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1151...  Training loss: 2.0649...  0.1337 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1152...  Training loss: 2.0366...  0.1309 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1153...  Training loss: 2.0679...  0.1293 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1154...  Training loss: 2.0360...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1155...  Training loss: 2.0287...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1156...  Training loss: 2.0740...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1157...  Training loss: 2.0340...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1158...  Training loss: 2.0623...  0.1331 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1159...  Training loss: 2.0646...  0.1797 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1160...  Training loss: 2.0247...  0.1840 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1161...  Training loss: 2.0548...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1162...  Training loss: 2.0520...  0.1336 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1163...  Training loss: 2.0390...  0.1344 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1164...  Training loss: 2.0311...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1165...  Training loss: 2.0473...  0.1354 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1166...  Training loss: 2.0600...  0.1332 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1167...  Training loss: 2.0756...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1168...  Training loss: 2.0422...  0.1343 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1169...  Training loss: 2.0575...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1170...  Training loss: 2.0630...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1171...  Training loss: 2.0128...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1172...  Training loss: 2.0418...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1173...  Training loss: 2.0304...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1174...  Training loss: 2.0285...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1175...  Training loss: 2.0460...  0.1354 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1176...  Training loss: 2.0408...  0.1353 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1177...  Training loss: 2.0452...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1178...  Training loss: 2.0570...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1179...  Training loss: 2.0625...  0.1317 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1180...  Training loss: 2.0288...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1181...  Training loss: 2.0344...  0.1329 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1182...  Training loss: 2.0479...  0.1306 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1183...  Training loss: 1.9974...  0.1333 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1184...  Training loss: 2.0322...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1185...  Training loss: 2.0559...  0.1313 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1186...  Training loss: 2.0207...  0.1340 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1187...  Training loss: 2.0125...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1188...  Training loss: 2.0328...  0.1308 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1189...  Training loss: 2.0314...  0.1341 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1190...  Training loss: 2.0464...  0.1317 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1191...  Training loss: 2.0435...  0.1312 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1192...  Training loss: 2.0472...  0.1338 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1193...  Training loss: 2.0271...  0.1331 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1194...  Training loss: 2.0205...  0.1308 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1195...  Training loss: 2.0396...  0.1342 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1196...  Training loss: 2.0732...  0.1324 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1197...  Training loss: 2.0371...  0.1323 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1198...  Training loss: 2.0449...  0.1347 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1199...  Training loss: 2.0575...  0.1330 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1200...  Training loss: 2.0691...  0.1325 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1201...  Training loss: 2.0858...  0.1324 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1202...  Training loss: 2.0876...  0.1341 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/15...  Training Step: 1203...  Training loss: 2.0956...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1204...  Training loss: 2.0540...  0.1351 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1205...  Training loss: 2.0963...  0.1339 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1206...  Training loss: 2.0451...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1207...  Training loss: 2.0398...  0.1349 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1208...  Training loss: 2.0544...  0.1335 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1209...  Training loss: 2.0546...  0.1311 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1210...  Training loss: 2.0460...  0.1343 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1211...  Training loss: 2.0516...  0.1327 sec/batch\n",
      "Epoch: 4/15...  Training Step: 1212...  Training loss: 2.0458...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1213...  Training loss: 2.1156...  0.1308 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1214...  Training loss: 2.0639...  0.1318 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1215...  Training loss: 2.0365...  0.1314 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1216...  Training loss: 2.0452...  0.1317 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1217...  Training loss: 2.0480...  0.1320 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1218...  Training loss: 2.0665...  0.1302 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1219...  Training loss: 2.0539...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1220...  Training loss: 2.0600...  0.1322 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1221...  Training loss: 2.0140...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1222...  Training loss: 2.0254...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1223...  Training loss: 2.0100...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1224...  Training loss: 2.0354...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1225...  Training loss: 2.0485...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1226...  Training loss: 2.0450...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1227...  Training loss: 2.0548...  0.1324 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1228...  Training loss: 2.0551...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1229...  Training loss: 2.0757...  0.1314 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1230...  Training loss: 2.0659...  0.1295 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1231...  Training loss: 2.0882...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1232...  Training loss: 2.0860...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1233...  Training loss: 2.0629...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1234...  Training loss: 2.0525...  0.1320 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1235...  Training loss: 2.0638...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1236...  Training loss: 2.0481...  0.1824 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1237...  Training loss: 2.0853...  0.1837 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1238...  Training loss: 2.0697...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1239...  Training loss: 2.0500...  0.1319 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1240...  Training loss: 2.0642...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1241...  Training loss: 2.0816...  0.1363 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1242...  Training loss: 2.0652...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1243...  Training loss: 2.0563...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1244...  Training loss: 2.0556...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1245...  Training loss: 2.0461...  0.1349 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1246...  Training loss: 2.0499...  0.1317 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1247...  Training loss: 2.0447...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1248...  Training loss: 2.0362...  0.1353 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1249...  Training loss: 2.0539...  0.1349 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1250...  Training loss: 2.0236...  0.1307 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1251...  Training loss: 2.0467...  0.1311 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1252...  Training loss: 2.0420...  0.1305 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1253...  Training loss: 2.0438...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1254...  Training loss: 2.0414...  0.1345 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1255...  Training loss: 2.0503...  0.1300 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1256...  Training loss: 2.0249...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1257...  Training loss: 2.0436...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1258...  Training loss: 2.0217...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1259...  Training loss: 2.0188...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1260...  Training loss: 2.0294...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1261...  Training loss: 2.0239...  0.1303 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1262...  Training loss: 2.0074...  0.1319 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1263...  Training loss: 2.0281...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1264...  Training loss: 2.0315...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1265...  Training loss: 2.0286...  0.1328 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1266...  Training loss: 2.0223...  0.1317 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1267...  Training loss: 2.0287...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1268...  Training loss: 2.0251...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1269...  Training loss: 2.0265...  0.1317 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1270...  Training loss: 2.0292...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1271...  Training loss: 2.0109...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1272...  Training loss: 2.0078...  0.1315 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1273...  Training loss: 2.0102...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1274...  Training loss: 2.0375...  0.1345 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1275...  Training loss: 2.0180...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1276...  Training loss: 2.0335...  0.1353 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1277...  Training loss: 2.0411...  0.1338 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1278...  Training loss: 2.0747...  0.1294 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1279...  Training loss: 2.0523...  0.1357 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1280...  Training loss: 2.0193...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1281...  Training loss: 2.0344...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1282...  Training loss: 2.0190...  0.1358 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1283...  Training loss: 2.0472...  0.1338 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1284...  Training loss: 2.0332...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1285...  Training loss: 2.0243...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1286...  Training loss: 2.0294...  0.1342 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1287...  Training loss: 2.0146...  0.1298 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1288...  Training loss: 2.0379...  0.1355 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1289...  Training loss: 2.0555...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1290...  Training loss: 2.0689...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1291...  Training loss: 2.0372...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1292...  Training loss: 2.0270...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1293...  Training loss: 2.0519...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1294...  Training loss: 2.0550...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1295...  Training loss: 2.0373...  0.1313 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1296...  Training loss: 2.0231...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1297...  Training loss: 2.0361...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1298...  Training loss: 2.0171...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1299...  Training loss: 2.0261...  0.1351 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1300...  Training loss: 2.0285...  0.1319 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1301...  Training loss: 2.0244...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1302...  Training loss: 2.0410...  0.1346 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15...  Training Step: 1303...  Training loss: 2.0051...  0.1340 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1304...  Training loss: 2.0367...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1305...  Training loss: 2.0287...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1306...  Training loss: 2.0113...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1307...  Training loss: 2.0137...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1308...  Training loss: 2.0239...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1309...  Training loss: 2.0340...  0.1328 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1310...  Training loss: 2.0356...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1311...  Training loss: 2.0074...  0.1291 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1312...  Training loss: 2.0183...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1313...  Training loss: 2.0337...  0.1308 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1314...  Training loss: 2.0231...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1315...  Training loss: 2.0203...  0.1351 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1316...  Training loss: 2.0133...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1317...  Training loss: 2.0253...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1318...  Training loss: 2.0279...  0.1321 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1319...  Training loss: 2.0259...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1320...  Training loss: 2.0252...  0.1287 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1321...  Training loss: 2.0444...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1322...  Training loss: 2.0370...  0.1324 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1323...  Training loss: 2.0492...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1324...  Training loss: 2.0154...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1325...  Training loss: 2.0180...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1326...  Training loss: 2.0368...  0.1305 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1327...  Training loss: 2.0329...  0.1321 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1328...  Training loss: 2.0352...  0.1300 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1329...  Training loss: 2.0283...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1330...  Training loss: 2.0290...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1331...  Training loss: 2.0462...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1332...  Training loss: 2.0094...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1333...  Training loss: 2.0067...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1334...  Training loss: 2.0403...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1335...  Training loss: 2.0438...  0.1308 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1336...  Training loss: 2.0326...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1337...  Training loss: 2.0412...  0.1313 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1338...  Training loss: 2.0385...  0.1338 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1339...  Training loss: 2.0349...  0.1298 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1340...  Training loss: 2.0708...  0.1297 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1341...  Training loss: 2.0453...  0.1321 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1342...  Training loss: 2.0707...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1343...  Training loss: 2.0439...  0.1319 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1344...  Training loss: 2.0441...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1345...  Training loss: 2.0449...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1346...  Training loss: 2.0740...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1347...  Training loss: 2.0538...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1348...  Training loss: 2.0211...  0.1313 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1349...  Training loss: 2.0584...  0.1323 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1350...  Training loss: 2.0340...  0.1297 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1351...  Training loss: 2.0439...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1352...  Training loss: 2.0372...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1353...  Training loss: 2.0286...  0.1298 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1354...  Training loss: 2.0026...  0.1323 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1355...  Training loss: 2.0015...  0.1311 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1356...  Training loss: 2.0290...  0.1321 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1357...  Training loss: 2.0028...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1358...  Training loss: 2.0398...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1359...  Training loss: 2.0247...  0.1305 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1360...  Training loss: 2.0456...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1361...  Training loss: 2.0355...  0.1830 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1362...  Training loss: 2.0141...  0.1832 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1363...  Training loss: 2.0422...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1364...  Training loss: 2.0205...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1365...  Training loss: 2.0298...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1366...  Training loss: 2.0447...  0.1298 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1367...  Training loss: 2.0250...  0.1347 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1368...  Training loss: 2.0368...  0.1340 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1369...  Training loss: 2.0257...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1370...  Training loss: 2.0300...  0.1340 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1371...  Training loss: 2.0403...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1372...  Training loss: 2.0317...  0.1352 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1373...  Training loss: 2.0068...  0.1328 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1374...  Training loss: 2.0153...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1375...  Training loss: 2.0123...  0.1338 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1376...  Training loss: 2.0248...  0.1299 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1377...  Training loss: 2.0057...  0.1320 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1378...  Training loss: 2.0420...  0.1311 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1379...  Training loss: 2.0183...  0.1303 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1380...  Training loss: 2.0248...  0.1319 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1381...  Training loss: 2.0354...  0.1328 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1382...  Training loss: 2.0335...  0.1353 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1383...  Training loss: 2.0162...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1384...  Training loss: 2.0294...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1385...  Training loss: 2.0243...  0.1310 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1386...  Training loss: 2.0210...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1387...  Training loss: 2.0087...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1388...  Training loss: 2.0019...  0.1342 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1389...  Training loss: 2.0270...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1390...  Training loss: 2.0109...  0.1290 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1391...  Training loss: 2.0096...  0.1315 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1392...  Training loss: 2.0125...  0.1352 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1393...  Training loss: 2.0083...  0.1310 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1394...  Training loss: 2.0240...  0.1328 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1395...  Training loss: 2.0131...  0.1310 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1396...  Training loss: 2.0193...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1397...  Training loss: 2.0170...  0.1309 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1398...  Training loss: 2.0194...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1399...  Training loss: 2.0120...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1400...  Training loss: 2.0075...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1401...  Training loss: 2.0168...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1402...  Training loss: 2.0177...  0.1347 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15...  Training Step: 1403...  Training loss: 1.9955...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1404...  Training loss: 2.0170...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1405...  Training loss: 1.9999...  0.1341 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1406...  Training loss: 2.0255...  0.1332 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1407...  Training loss: 1.9987...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1408...  Training loss: 1.9930...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1409...  Training loss: 2.0328...  0.1323 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1410...  Training loss: 2.0140...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1411...  Training loss: 2.0105...  0.1342 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1412...  Training loss: 2.0255...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1413...  Training loss: 2.0178...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1414...  Training loss: 2.0206...  0.1370 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1415...  Training loss: 2.0406...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1416...  Training loss: 2.0597...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1417...  Training loss: 2.0565...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1418...  Training loss: 2.0308...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1419...  Training loss: 2.0504...  0.1353 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1420...  Training loss: 2.0342...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1421...  Training loss: 2.0416...  0.1293 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1422...  Training loss: 2.0290...  0.1307 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1423...  Training loss: 2.0174...  0.1302 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1424...  Training loss: 2.0237...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1425...  Training loss: 2.0383...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1426...  Training loss: 2.0201...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1427...  Training loss: 2.0172...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1428...  Training loss: 2.0403...  0.1334 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1429...  Training loss: 2.0160...  0.1342 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1430...  Training loss: 2.0259...  0.1321 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1431...  Training loss: 2.0292...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1432...  Training loss: 2.0247...  0.1320 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1433...  Training loss: 2.0186...  0.1311 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1434...  Training loss: 2.0302...  0.1352 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1435...  Training loss: 2.0149...  0.1302 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1436...  Training loss: 2.0133...  0.1343 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1437...  Training loss: 2.0204...  0.1345 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1438...  Training loss: 2.0274...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1439...  Training loss: 2.0315...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1440...  Training loss: 2.0174...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1441...  Training loss: 2.0405...  0.1315 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1442...  Training loss: 2.0465...  0.1314 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1443...  Training loss: 2.0022...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1444...  Training loss: 2.0169...  0.1342 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1445...  Training loss: 2.0122...  0.1310 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1446...  Training loss: 2.0359...  0.1324 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1447...  Training loss: 2.0110...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1448...  Training loss: 2.0134...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1449...  Training loss: 2.0225...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1450...  Training loss: 2.0340...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1451...  Training loss: 2.0191...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1452...  Training loss: 2.0362...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1453...  Training loss: 2.0296...  0.1779 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1454...  Training loss: 2.0158...  0.1838 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1455...  Training loss: 1.9920...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1456...  Training loss: 2.0248...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1457...  Training loss: 1.9961...  0.1312 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1458...  Training loss: 1.9882...  0.1325 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1459...  Training loss: 2.0339...  0.1351 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1460...  Training loss: 1.9863...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1461...  Training loss: 2.0181...  0.1352 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1462...  Training loss: 2.0218...  0.1303 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1463...  Training loss: 1.9907...  0.1304 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1464...  Training loss: 2.0200...  0.1335 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1465...  Training loss: 2.0041...  0.1301 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1466...  Training loss: 1.9912...  0.1307 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1467...  Training loss: 1.9944...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1468...  Training loss: 2.0037...  0.1340 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1469...  Training loss: 2.0251...  0.1340 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1470...  Training loss: 2.0302...  0.1326 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1471...  Training loss: 2.0017...  0.1345 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1472...  Training loss: 2.0134...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1473...  Training loss: 2.0199...  0.1358 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1474...  Training loss: 1.9770...  0.1348 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1475...  Training loss: 2.0045...  0.1338 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1476...  Training loss: 1.9933...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1477...  Training loss: 1.9896...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1478...  Training loss: 2.0030...  0.1354 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1479...  Training loss: 1.9930...  0.1344 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1480...  Training loss: 2.0097...  0.1317 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1481...  Training loss: 2.0147...  0.1350 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1482...  Training loss: 2.0164...  0.1352 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1483...  Training loss: 1.9818...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1484...  Training loss: 1.9941...  0.1314 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1485...  Training loss: 2.0120...  0.1304 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1486...  Training loss: 1.9525...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1487...  Training loss: 1.9925...  0.1365 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1488...  Training loss: 2.0109...  0.1316 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1489...  Training loss: 1.9757...  0.1345 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1490...  Training loss: 1.9757...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1491...  Training loss: 1.9837...  0.1359 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1492...  Training loss: 2.0015...  0.1315 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1493...  Training loss: 1.9956...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1494...  Training loss: 2.0008...  0.1330 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1495...  Training loss: 2.0111...  0.1333 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1496...  Training loss: 1.9943...  0.1339 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1497...  Training loss: 1.9718...  0.1320 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1498...  Training loss: 2.0016...  0.1329 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1499...  Training loss: 2.0298...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1500...  Training loss: 1.9934...  0.1355 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1501...  Training loss: 1.9974...  0.1309 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1502...  Training loss: 2.0193...  0.1326 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/15...  Training Step: 1503...  Training loss: 2.0257...  0.1322 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1504...  Training loss: 2.0468...  0.1310 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1505...  Training loss: 2.0537...  0.1336 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1506...  Training loss: 2.0527...  0.1337 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1507...  Training loss: 2.0124...  0.1353 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1508...  Training loss: 2.0509...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1509...  Training loss: 2.0067...  0.1351 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1510...  Training loss: 2.0007...  0.1349 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1511...  Training loss: 2.0154...  0.1346 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1512...  Training loss: 2.0198...  0.1327 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1513...  Training loss: 2.0146...  0.1302 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1514...  Training loss: 2.0097...  0.1331 sec/batch\n",
      "Epoch: 5/15...  Training Step: 1515...  Training loss: 2.0032...  0.1355 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1516...  Training loss: 2.0760...  0.1311 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1517...  Training loss: 2.0360...  0.1318 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1518...  Training loss: 1.9998...  0.1358 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1519...  Training loss: 2.0098...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1520...  Training loss: 2.0158...  0.1340 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1521...  Training loss: 2.0425...  0.1315 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1522...  Training loss: 2.0142...  0.1352 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1523...  Training loss: 2.0255...  0.1311 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1524...  Training loss: 1.9789...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1525...  Training loss: 1.9884...  0.1304 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1526...  Training loss: 1.9789...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1527...  Training loss: 1.9994...  0.1320 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1528...  Training loss: 2.0164...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1529...  Training loss: 2.0037...  0.1315 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1530...  Training loss: 2.0196...  0.1352 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1531...  Training loss: 2.0154...  0.1325 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1532...  Training loss: 2.0403...  0.1291 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1533...  Training loss: 2.0245...  0.1311 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1534...  Training loss: 2.0445...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1535...  Training loss: 2.0483...  0.1339 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1536...  Training loss: 2.0189...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1537...  Training loss: 2.0197...  0.1320 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1538...  Training loss: 2.0268...  0.1308 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1539...  Training loss: 2.0151...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1540...  Training loss: 2.0505...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1541...  Training loss: 2.0390...  0.1338 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1542...  Training loss: 2.0112...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1543...  Training loss: 2.0279...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1544...  Training loss: 2.0416...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1545...  Training loss: 2.0192...  0.1334 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1546...  Training loss: 2.0123...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1547...  Training loss: 2.0170...  0.1345 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1548...  Training loss: 2.0081...  0.1313 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1549...  Training loss: 2.0122...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1550...  Training loss: 2.0046...  0.1339 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1551...  Training loss: 1.9971...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1552...  Training loss: 2.0155...  0.1343 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1553...  Training loss: 1.9907...  0.1305 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1554...  Training loss: 2.0085...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1555...  Training loss: 2.0063...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1556...  Training loss: 2.0097...  0.1314 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1557...  Training loss: 1.9919...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1558...  Training loss: 2.0189...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1559...  Training loss: 1.9856...  0.1347 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1560...  Training loss: 2.0032...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1561...  Training loss: 1.9858...  0.1304 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1562...  Training loss: 1.9785...  0.1340 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1563...  Training loss: 1.9926...  0.1319 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1564...  Training loss: 1.9893...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1565...  Training loss: 1.9647...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1566...  Training loss: 1.9866...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1567...  Training loss: 1.9853...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1568...  Training loss: 1.9922...  0.1318 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1569...  Training loss: 1.9831...  0.1296 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1570...  Training loss: 1.9939...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1571...  Training loss: 1.9880...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1572...  Training loss: 1.9895...  0.1307 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1573...  Training loss: 1.9908...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1574...  Training loss: 1.9752...  0.1338 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1575...  Training loss: 1.9721...  0.1334 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1576...  Training loss: 1.9813...  0.1329 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1577...  Training loss: 2.0044...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1578...  Training loss: 1.9723...  0.1359 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1579...  Training loss: 1.9990...  0.1322 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1580...  Training loss: 2.0150...  0.1299 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1581...  Training loss: 2.0286...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1582...  Training loss: 2.0155...  0.1308 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1583...  Training loss: 1.9794...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1584...  Training loss: 1.9951...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1585...  Training loss: 1.9798...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1586...  Training loss: 2.0105...  0.1304 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1587...  Training loss: 1.9909...  0.1355 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1588...  Training loss: 1.9902...  0.1343 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1589...  Training loss: 1.9946...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1590...  Training loss: 1.9768...  0.1768 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1591...  Training loss: 2.0045...  0.1843 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1592...  Training loss: 2.0090...  0.1305 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1593...  Training loss: 2.0203...  0.1334 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1594...  Training loss: 2.0101...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1595...  Training loss: 1.9968...  0.1355 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1596...  Training loss: 2.0214...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1597...  Training loss: 2.0197...  0.1313 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1598...  Training loss: 2.0035...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1599...  Training loss: 1.9957...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1600...  Training loss: 1.9991...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1601...  Training loss: 1.9730...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1602...  Training loss: 1.9847...  0.1348 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15...  Training Step: 1603...  Training loss: 1.9957...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1604...  Training loss: 1.9810...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1605...  Training loss: 1.9987...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1606...  Training loss: 1.9810...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1607...  Training loss: 2.0006...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1608...  Training loss: 1.9869...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1609...  Training loss: 1.9715...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1610...  Training loss: 1.9712...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1611...  Training loss: 1.9888...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1612...  Training loss: 2.0014...  0.1314 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1613...  Training loss: 2.0036...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1614...  Training loss: 1.9753...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1615...  Training loss: 1.9791...  0.1300 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1616...  Training loss: 1.9938...  0.1325 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1617...  Training loss: 1.9940...  0.1340 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1618...  Training loss: 1.9906...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1619...  Training loss: 1.9678...  0.1348 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1620...  Training loss: 1.9940...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1621...  Training loss: 1.9987...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1622...  Training loss: 1.9934...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1623...  Training loss: 1.9897...  0.1312 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1624...  Training loss: 2.0100...  0.1358 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1625...  Training loss: 2.0044...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1626...  Training loss: 2.0166...  0.1324 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1627...  Training loss: 1.9819...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1628...  Training loss: 1.9794...  0.1299 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1629...  Training loss: 1.9964...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1630...  Training loss: 2.0104...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1631...  Training loss: 1.9931...  0.1348 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1632...  Training loss: 1.9960...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1633...  Training loss: 1.9981...  0.1320 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1634...  Training loss: 2.0161...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1635...  Training loss: 1.9718...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1636...  Training loss: 1.9662...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1637...  Training loss: 2.0031...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1638...  Training loss: 2.0000...  0.1307 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1639...  Training loss: 1.9965...  0.1322 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1640...  Training loss: 2.0073...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1641...  Training loss: 2.0036...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1642...  Training loss: 2.0047...  0.1349 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1643...  Training loss: 2.0343...  0.1308 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1644...  Training loss: 2.0099...  0.1340 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1645...  Training loss: 2.0270...  0.1356 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1646...  Training loss: 2.0137...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1647...  Training loss: 2.0143...  0.1309 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1648...  Training loss: 2.0133...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1649...  Training loss: 2.0326...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1650...  Training loss: 2.0201...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1651...  Training loss: 1.9907...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1652...  Training loss: 2.0243...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1653...  Training loss: 2.0006...  0.1343 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1654...  Training loss: 2.0108...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1655...  Training loss: 2.0008...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1656...  Training loss: 1.9856...  0.1806 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1657...  Training loss: 1.9703...  0.1837 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1658...  Training loss: 1.9717...  0.1303 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1659...  Training loss: 1.9970...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1660...  Training loss: 1.9712...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1661...  Training loss: 2.0115...  0.1340 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1662...  Training loss: 1.9901...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1663...  Training loss: 2.0169...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1664...  Training loss: 1.9954...  0.1349 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1665...  Training loss: 1.9805...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1666...  Training loss: 2.0070...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1667...  Training loss: 1.9967...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1668...  Training loss: 1.9948...  0.1301 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1669...  Training loss: 2.0089...  0.1345 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1670...  Training loss: 1.9900...  0.1302 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1671...  Training loss: 2.0008...  0.1320 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1672...  Training loss: 2.0003...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1673...  Training loss: 2.0015...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1674...  Training loss: 2.0067...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1675...  Training loss: 2.0021...  0.1357 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1676...  Training loss: 1.9789...  0.1295 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1677...  Training loss: 1.9853...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1678...  Training loss: 1.9795...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1679...  Training loss: 1.9916...  0.1356 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1680...  Training loss: 1.9712...  0.1363 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1681...  Training loss: 2.0127...  0.1355 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1682...  Training loss: 1.9789...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1683...  Training loss: 1.9895...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1684...  Training loss: 1.9995...  0.1314 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1685...  Training loss: 1.9953...  0.1290 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1686...  Training loss: 1.9913...  0.1302 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1687...  Training loss: 1.9955...  0.1357 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1688...  Training loss: 1.9909...  0.1318 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1689...  Training loss: 1.9845...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1690...  Training loss: 1.9847...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1691...  Training loss: 1.9717...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1692...  Training loss: 1.9889...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1693...  Training loss: 1.9752...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1694...  Training loss: 1.9763...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1695...  Training loss: 1.9764...  0.1322 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1696...  Training loss: 1.9785...  0.1349 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1697...  Training loss: 1.9949...  0.1309 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1698...  Training loss: 1.9906...  0.1317 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1699...  Training loss: 1.9744...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1700...  Training loss: 1.9784...  0.1298 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1701...  Training loss: 1.9841...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1702...  Training loss: 1.9762...  0.1296 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15...  Training Step: 1703...  Training loss: 1.9735...  0.1329 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1704...  Training loss: 1.9839...  0.1365 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1705...  Training loss: 1.9824...  0.1343 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1706...  Training loss: 1.9617...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1707...  Training loss: 1.9838...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1708...  Training loss: 1.9822...  0.1352 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1709...  Training loss: 1.9937...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1710...  Training loss: 1.9652...  0.1325 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1711...  Training loss: 1.9662...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1712...  Training loss: 2.0051...  0.1334 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1713...  Training loss: 1.9944...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1714...  Training loss: 1.9754...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1715...  Training loss: 2.0028...  0.1300 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1716...  Training loss: 1.9809...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1717...  Training loss: 1.9880...  0.1363 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1718...  Training loss: 2.0039...  0.1348 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1719...  Training loss: 2.0344...  0.1348 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1720...  Training loss: 2.0271...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1721...  Training loss: 1.9955...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1722...  Training loss: 2.0131...  0.1871 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1723...  Training loss: 2.0039...  0.1775 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1724...  Training loss: 2.0078...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1725...  Training loss: 1.9988...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1726...  Training loss: 1.9919...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1727...  Training loss: 1.9880...  0.1326 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1728...  Training loss: 2.0004...  0.1329 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1729...  Training loss: 1.9883...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1730...  Training loss: 1.9908...  0.1338 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1731...  Training loss: 2.0114...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1732...  Training loss: 1.9834...  0.1322 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1733...  Training loss: 1.9875...  0.1315 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1734...  Training loss: 1.9957...  0.1302 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1735...  Training loss: 1.9959...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1736...  Training loss: 1.9898...  0.1345 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1737...  Training loss: 1.9932...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1738...  Training loss: 1.9833...  0.1354 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1739...  Training loss: 1.9812...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1740...  Training loss: 1.9806...  0.1339 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1741...  Training loss: 1.9950...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1742...  Training loss: 2.0000...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1743...  Training loss: 1.9746...  0.1317 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1744...  Training loss: 2.0028...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1745...  Training loss: 2.0127...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1746...  Training loss: 1.9710...  0.1339 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1747...  Training loss: 1.9883...  0.1295 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1748...  Training loss: 1.9842...  0.1336 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1749...  Training loss: 1.9993...  0.1334 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1750...  Training loss: 1.9815...  0.1299 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1751...  Training loss: 1.9837...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1752...  Training loss: 1.9898...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1753...  Training loss: 2.0027...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1754...  Training loss: 1.9907...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1755...  Training loss: 1.9978...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1756...  Training loss: 1.9985...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1757...  Training loss: 1.9839...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1758...  Training loss: 1.9644...  0.1321 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1759...  Training loss: 2.0002...  0.1338 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1760...  Training loss: 1.9681...  0.1315 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1761...  Training loss: 1.9644...  0.1363 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1762...  Training loss: 2.0033...  0.1312 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1763...  Training loss: 1.9541...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1764...  Training loss: 1.9881...  0.1319 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1765...  Training loss: 2.0000...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1766...  Training loss: 1.9652...  0.1367 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1767...  Training loss: 1.9851...  0.1335 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1768...  Training loss: 1.9704...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1769...  Training loss: 1.9599...  0.1308 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1770...  Training loss: 1.9605...  0.1352 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1771...  Training loss: 1.9742...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1772...  Training loss: 1.9887...  0.1359 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1773...  Training loss: 2.0129...  0.1307 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1774...  Training loss: 1.9630...  0.1351 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1775...  Training loss: 1.9853...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1776...  Training loss: 1.9848...  0.1356 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1777...  Training loss: 1.9432...  0.1303 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1778...  Training loss: 1.9771...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1779...  Training loss: 1.9561...  0.1344 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1780...  Training loss: 1.9561...  0.1331 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1781...  Training loss: 1.9745...  0.1356 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1782...  Training loss: 1.9696...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1783...  Training loss: 1.9772...  0.1328 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1784...  Training loss: 1.9779...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1785...  Training loss: 1.9879...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1786...  Training loss: 1.9594...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1787...  Training loss: 1.9574...  0.1345 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1788...  Training loss: 1.9786...  0.1350 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1789...  Training loss: 1.9328...  0.1323 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1790...  Training loss: 1.9645...  0.1306 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1791...  Training loss: 1.9783...  0.1804 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1792...  Training loss: 1.9483...  0.1838 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1793...  Training loss: 1.9442...  0.1316 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1794...  Training loss: 1.9555...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1795...  Training loss: 1.9613...  0.1300 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1796...  Training loss: 1.9670...  0.1356 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1797...  Training loss: 1.9757...  0.1346 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1798...  Training loss: 1.9799...  0.1343 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1799...  Training loss: 1.9692...  0.1318 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1800...  Training loss: 1.9552...  0.1342 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1801...  Training loss: 1.9735...  0.1354 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1802...  Training loss: 1.9966...  0.1310 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/15...  Training Step: 1803...  Training loss: 1.9619...  0.1294 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1804...  Training loss: 1.9740...  0.1339 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1805...  Training loss: 1.9834...  0.1353 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1806...  Training loss: 1.9876...  0.1314 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1807...  Training loss: 2.0157...  0.1337 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1808...  Training loss: 2.0235...  0.1332 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1809...  Training loss: 2.0249...  0.1313 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1810...  Training loss: 1.9859...  0.1333 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1811...  Training loss: 2.0223...  0.1315 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1812...  Training loss: 1.9810...  0.1330 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1813...  Training loss: 1.9735...  0.1341 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1814...  Training loss: 1.9831...  0.1310 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1815...  Training loss: 1.9871...  0.1354 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1816...  Training loss: 1.9846...  0.1338 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1817...  Training loss: 1.9784...  0.1327 sec/batch\n",
      "Epoch: 6/15...  Training Step: 1818...  Training loss: 1.9752...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1819...  Training loss: 2.0492...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1820...  Training loss: 1.9979...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1821...  Training loss: 1.9650...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1822...  Training loss: 1.9789...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1823...  Training loss: 1.9793...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1824...  Training loss: 2.0014...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1825...  Training loss: 1.9906...  0.1366 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1826...  Training loss: 1.9954...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1827...  Training loss: 1.9487...  0.1362 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1828...  Training loss: 1.9581...  0.1356 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1829...  Training loss: 1.9462...  0.1299 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1830...  Training loss: 1.9689...  0.1371 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1831...  Training loss: 1.9848...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1832...  Training loss: 1.9795...  0.1305 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1833...  Training loss: 1.9805...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1834...  Training loss: 1.9902...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1835...  Training loss: 2.0103...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1836...  Training loss: 1.9892...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1837...  Training loss: 2.0156...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1838...  Training loss: 2.0217...  0.1300 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1839...  Training loss: 1.9942...  0.1331 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1840...  Training loss: 1.9955...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1841...  Training loss: 1.9958...  0.1314 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1842...  Training loss: 1.9803...  0.1358 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1843...  Training loss: 2.0201...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1844...  Training loss: 2.0056...  0.1320 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1845...  Training loss: 1.9744...  0.1304 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1846...  Training loss: 2.0001...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1847...  Training loss: 2.0232...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1848...  Training loss: 1.9889...  0.1319 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1849...  Training loss: 1.9859...  0.1325 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1850...  Training loss: 1.9879...  0.1304 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1851...  Training loss: 1.9797...  0.1357 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1852...  Training loss: 1.9867...  0.1349 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1853...  Training loss: 1.9814...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1854...  Training loss: 1.9721...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1855...  Training loss: 1.9899...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1856...  Training loss: 1.9642...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1857...  Training loss: 1.9818...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1858...  Training loss: 1.9780...  0.1316 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1859...  Training loss: 1.9801...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1860...  Training loss: 1.9669...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1861...  Training loss: 1.9834...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1862...  Training loss: 1.9546...  0.1354 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1863...  Training loss: 1.9788...  0.1304 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1864...  Training loss: 1.9610...  0.1301 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1865...  Training loss: 1.9457...  0.1325 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1866...  Training loss: 1.9683...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1867...  Training loss: 1.9555...  0.1310 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1868...  Training loss: 1.9357...  0.1330 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1869...  Training loss: 1.9514...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1870...  Training loss: 1.9678...  0.1299 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1871...  Training loss: 1.9650...  0.1331 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1872...  Training loss: 1.9544...  0.1326 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1873...  Training loss: 1.9612...  0.1347 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1874...  Training loss: 1.9642...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1875...  Training loss: 1.9594...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1876...  Training loss: 1.9701...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1877...  Training loss: 1.9526...  0.1361 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1878...  Training loss: 1.9502...  0.1358 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1879...  Training loss: 1.9464...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1880...  Training loss: 1.9743...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1881...  Training loss: 1.9488...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1882...  Training loss: 1.9715...  0.1331 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1883...  Training loss: 1.9820...  0.1330 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1884...  Training loss: 2.0056...  0.1319 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1885...  Training loss: 1.9864...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1886...  Training loss: 1.9498...  0.1312 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1887...  Training loss: 1.9680...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1888...  Training loss: 1.9508...  0.1326 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1889...  Training loss: 1.9829...  0.1337 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1890...  Training loss: 1.9759...  0.1343 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1891...  Training loss: 1.9624...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1892...  Training loss: 1.9707...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1893...  Training loss: 1.9401...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1894...  Training loss: 1.9817...  0.1303 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1895...  Training loss: 1.9875...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1896...  Training loss: 2.0070...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1897...  Training loss: 1.9710...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1898...  Training loss: 1.9710...  0.1361 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1899...  Training loss: 1.9885...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1900...  Training loss: 1.9898...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1901...  Training loss: 1.9715...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1902...  Training loss: 1.9703...  0.1323 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15...  Training Step: 1903...  Training loss: 1.9793...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1904...  Training loss: 1.9551...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1905...  Training loss: 1.9638...  0.1320 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1906...  Training loss: 1.9636...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1907...  Training loss: 1.9602...  0.1307 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1908...  Training loss: 1.9873...  0.1306 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1909...  Training loss: 1.9518...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1910...  Training loss: 1.9782...  0.1311 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1911...  Training loss: 1.9650...  0.1299 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1912...  Training loss: 1.9464...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1913...  Training loss: 1.9480...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1914...  Training loss: 1.9579...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1915...  Training loss: 1.9743...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1916...  Training loss: 1.9711...  0.1295 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1917...  Training loss: 1.9504...  0.1290 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1918...  Training loss: 1.9608...  0.1305 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1919...  Training loss: 1.9701...  0.1351 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1920...  Training loss: 1.9712...  0.1342 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1921...  Training loss: 1.9736...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1922...  Training loss: 1.9540...  0.1320 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1923...  Training loss: 1.9715...  0.1337 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1924...  Training loss: 1.9713...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1925...  Training loss: 1.9717...  0.1301 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1926...  Training loss: 1.9674...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1927...  Training loss: 1.9860...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1928...  Training loss: 1.9860...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1929...  Training loss: 1.9963...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1930...  Training loss: 1.9598...  0.1326 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1931...  Training loss: 1.9520...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1932...  Training loss: 1.9707...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1933...  Training loss: 1.9829...  0.1318 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1934...  Training loss: 1.9740...  0.1321 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1935...  Training loss: 1.9714...  0.1355 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1936...  Training loss: 1.9682...  0.1330 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1937...  Training loss: 1.9868...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1938...  Training loss: 1.9416...  0.1321 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1939...  Training loss: 1.9431...  0.1290 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1940...  Training loss: 1.9711...  0.1343 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1941...  Training loss: 1.9745...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1942...  Training loss: 1.9778...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1943...  Training loss: 1.9873...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1944...  Training loss: 1.9762...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1945...  Training loss: 1.9837...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1946...  Training loss: 2.0163...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1947...  Training loss: 1.9807...  0.1351 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1948...  Training loss: 2.0103...  0.1327 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1949...  Training loss: 1.9870...  0.1299 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1950...  Training loss: 1.9902...  0.1324 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1951...  Training loss: 1.9917...  0.1306 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1952...  Training loss: 2.0122...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1953...  Training loss: 1.9806...  0.1347 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1954...  Training loss: 1.9627...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1955...  Training loss: 1.9988...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1956...  Training loss: 1.9714...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1957...  Training loss: 1.9890...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1958...  Training loss: 1.9825...  0.1354 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1959...  Training loss: 1.9596...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1960...  Training loss: 1.9454...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1961...  Training loss: 1.9486...  0.1337 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1962...  Training loss: 1.9758...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1963...  Training loss: 1.9484...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1964...  Training loss: 1.9788...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1965...  Training loss: 1.9668...  0.1309 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1966...  Training loss: 1.9979...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1967...  Training loss: 1.9815...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1968...  Training loss: 1.9571...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1969...  Training loss: 1.9905...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1970...  Training loss: 1.9741...  0.1333 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1971...  Training loss: 1.9674...  0.1343 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1972...  Training loss: 1.9885...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1973...  Training loss: 1.9723...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1974...  Training loss: 1.9839...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1975...  Training loss: 1.9703...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1976...  Training loss: 1.9672...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1977...  Training loss: 1.9890...  0.1359 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1978...  Training loss: 1.9787...  0.1324 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1979...  Training loss: 1.9545...  0.1319 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1980...  Training loss: 1.9631...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1981...  Training loss: 1.9538...  0.1355 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1982...  Training loss: 1.9667...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1983...  Training loss: 1.9475...  0.1310 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1984...  Training loss: 1.9906...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1985...  Training loss: 1.9556...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1986...  Training loss: 1.9707...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1987...  Training loss: 1.9741...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1988...  Training loss: 1.9754...  0.1292 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1989...  Training loss: 1.9664...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1990...  Training loss: 1.9705...  0.1322 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1991...  Training loss: 1.9610...  0.1347 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1992...  Training loss: 1.9560...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1993...  Training loss: 1.9544...  0.1353 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1994...  Training loss: 1.9520...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1995...  Training loss: 1.9584...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1996...  Training loss: 1.9567...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1997...  Training loss: 1.9527...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1998...  Training loss: 1.9583...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 1999...  Training loss: 1.9546...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2000...  Training loss: 1.9708...  0.1362 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2001...  Training loss: 1.9596...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2002...  Training loss: 1.9522...  0.1305 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15...  Training Step: 2003...  Training loss: 1.9634...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2004...  Training loss: 1.9625...  0.1359 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2005...  Training loss: 1.9457...  0.1287 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2006...  Training loss: 1.9470...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2007...  Training loss: 1.9549...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2008...  Training loss: 1.9654...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2009...  Training loss: 1.9431...  0.1301 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2010...  Training loss: 1.9546...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2011...  Training loss: 1.9504...  0.1344 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2012...  Training loss: 1.9648...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2013...  Training loss: 1.9397...  0.1307 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2014...  Training loss: 1.9368...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2015...  Training loss: 1.9738...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2016...  Training loss: 1.9644...  0.1337 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2017...  Training loss: 1.9599...  0.1347 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2018...  Training loss: 1.9703...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2019...  Training loss: 1.9568...  0.1331 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2020...  Training loss: 1.9649...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2021...  Training loss: 1.9809...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2022...  Training loss: 2.0028...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2023...  Training loss: 2.0001...  0.1339 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2024...  Training loss: 1.9731...  0.1298 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2025...  Training loss: 1.9875...  0.1313 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2026...  Training loss: 1.9763...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2027...  Training loss: 1.9806...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2028...  Training loss: 1.9746...  0.1331 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2029...  Training loss: 1.9665...  0.1339 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2030...  Training loss: 1.9695...  0.1330 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2031...  Training loss: 1.9711...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2032...  Training loss: 1.9624...  0.1361 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2033...  Training loss: 1.9716...  0.1310 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2034...  Training loss: 1.9779...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2035...  Training loss: 1.9569...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2036...  Training loss: 1.9653...  0.1341 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2037...  Training loss: 1.9738...  0.1314 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2038...  Training loss: 1.9688...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2039...  Training loss: 1.9659...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2040...  Training loss: 1.9676...  0.1310 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2041...  Training loss: 1.9564...  0.1322 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2042...  Training loss: 1.9601...  0.1327 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2043...  Training loss: 1.9620...  0.1319 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2044...  Training loss: 1.9739...  0.1300 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2045...  Training loss: 1.9671...  0.1324 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2046...  Training loss: 1.9565...  0.1353 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2047...  Training loss: 1.9852...  0.1316 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2048...  Training loss: 1.9910...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2049...  Training loss: 1.9551...  0.1321 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2050...  Training loss: 1.9656...  0.1351 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2051...  Training loss: 1.9608...  0.1364 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2052...  Training loss: 1.9748...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2053...  Training loss: 1.9586...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2054...  Training loss: 1.9626...  0.1312 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2055...  Training loss: 1.9642...  0.1357 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2056...  Training loss: 1.9704...  0.1312 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2057...  Training loss: 1.9625...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2058...  Training loss: 1.9735...  0.1329 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2059...  Training loss: 1.9748...  0.1354 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2060...  Training loss: 1.9650...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2061...  Training loss: 1.9447...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2062...  Training loss: 1.9680...  0.1351 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2063...  Training loss: 1.9464...  0.1330 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2064...  Training loss: 1.9371...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2065...  Training loss: 1.9789...  0.1337 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2066...  Training loss: 1.9333...  0.1295 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2067...  Training loss: 1.9596...  0.1325 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2068...  Training loss: 1.9664...  0.1324 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2069...  Training loss: 1.9393...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2070...  Training loss: 1.9516...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2071...  Training loss: 1.9483...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2072...  Training loss: 1.9407...  0.1309 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2073...  Training loss: 1.9381...  0.1343 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2074...  Training loss: 1.9453...  0.1300 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2075...  Training loss: 1.9675...  0.1332 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2076...  Training loss: 1.9894...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2077...  Training loss: 1.9377...  0.1317 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2078...  Training loss: 1.9680...  0.1346 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2079...  Training loss: 1.9646...  0.1328 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2080...  Training loss: 1.9140...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2081...  Training loss: 1.9522...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2082...  Training loss: 1.9306...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2083...  Training loss: 1.9376...  0.1322 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2084...  Training loss: 1.9483...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2085...  Training loss: 1.9366...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2086...  Training loss: 1.9511...  0.1338 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2087...  Training loss: 1.9484...  0.1349 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2088...  Training loss: 1.9689...  0.1312 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2089...  Training loss: 1.9256...  0.1306 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2090...  Training loss: 1.9398...  0.1307 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2091...  Training loss: 1.9525...  0.1327 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2092...  Training loss: 1.9011...  0.1301 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2093...  Training loss: 1.9337...  0.1310 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2094...  Training loss: 1.9536...  0.1349 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2095...  Training loss: 1.9186...  0.1319 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2096...  Training loss: 1.9225...  0.1323 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2097...  Training loss: 1.9345...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2098...  Training loss: 1.9449...  0.1313 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2099...  Training loss: 1.9421...  0.1314 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2100...  Training loss: 1.9492...  0.1356 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2101...  Training loss: 1.9560...  0.1336 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2102...  Training loss: 1.9341...  0.1340 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/15...  Training Step: 2103...  Training loss: 1.9217...  0.1294 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2104...  Training loss: 1.9490...  0.1340 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2105...  Training loss: 1.9731...  0.1304 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2106...  Training loss: 1.9440...  0.1356 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2107...  Training loss: 1.9531...  0.1348 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2108...  Training loss: 1.9746...  0.1343 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2109...  Training loss: 1.9710...  0.1350 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2110...  Training loss: 2.0021...  0.1353 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2111...  Training loss: 1.9935...  0.1315 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2112...  Training loss: 2.0094...  0.1354 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2113...  Training loss: 1.9631...  0.1311 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2114...  Training loss: 1.9985...  0.1334 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2115...  Training loss: 1.9582...  0.1352 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2116...  Training loss: 1.9522...  0.1335 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2117...  Training loss: 1.9629...  0.1313 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2118...  Training loss: 1.9660...  0.1308 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2119...  Training loss: 1.9653...  0.1292 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2120...  Training loss: 1.9628...  0.1345 sec/batch\n",
      "Epoch: 7/15...  Training Step: 2121...  Training loss: 1.9544...  0.1770 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2122...  Training loss: 2.0279...  0.1834 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2123...  Training loss: 1.9767...  0.1310 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2124...  Training loss: 1.9480...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2125...  Training loss: 1.9603...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2126...  Training loss: 1.9681...  0.1301 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2127...  Training loss: 1.9894...  0.1323 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2128...  Training loss: 1.9625...  0.1357 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2129...  Training loss: 1.9695...  0.1302 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2130...  Training loss: 1.9225...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2131...  Training loss: 1.9356...  0.1307 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2132...  Training loss: 1.9253...  0.1309 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2133...  Training loss: 1.9407...  0.1350 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2134...  Training loss: 1.9613...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2135...  Training loss: 1.9475...  0.1322 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2136...  Training loss: 1.9686...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2137...  Training loss: 1.9658...  0.1348 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2138...  Training loss: 1.9923...  0.1299 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2139...  Training loss: 1.9735...  0.1303 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2140...  Training loss: 1.9998...  0.1353 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2141...  Training loss: 1.9960...  0.1308 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2142...  Training loss: 1.9695...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2143...  Training loss: 1.9755...  0.1342 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2144...  Training loss: 1.9751...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2145...  Training loss: 1.9632...  0.1328 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2146...  Training loss: 2.0015...  0.1312 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2147...  Training loss: 1.9859...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2148...  Training loss: 1.9549...  0.1306 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2149...  Training loss: 1.9751...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2150...  Training loss: 1.9995...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2151...  Training loss: 1.9695...  0.1348 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2152...  Training loss: 1.9608...  0.1302 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2153...  Training loss: 1.9673...  0.1316 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2154...  Training loss: 1.9545...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2155...  Training loss: 1.9632...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2156...  Training loss: 1.9574...  0.1309 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2157...  Training loss: 1.9489...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2158...  Training loss: 1.9630...  0.1306 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2159...  Training loss: 1.9385...  0.1301 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2160...  Training loss: 1.9630...  0.1363 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2161...  Training loss: 1.9632...  0.1319 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2162...  Training loss: 1.9592...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2163...  Training loss: 1.9465...  0.1311 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2164...  Training loss: 1.9589...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2165...  Training loss: 1.9347...  0.1351 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2166...  Training loss: 1.9534...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2167...  Training loss: 1.9364...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2168...  Training loss: 1.9322...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2169...  Training loss: 1.9420...  0.1311 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2170...  Training loss: 1.9343...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2171...  Training loss: 1.9163...  0.1307 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2172...  Training loss: 1.9332...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2173...  Training loss: 1.9432...  0.1348 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2174...  Training loss: 1.9363...  0.1345 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2175...  Training loss: 1.9402...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2176...  Training loss: 1.9430...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2177...  Training loss: 1.9488...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2178...  Training loss: 1.9305...  0.1353 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2179...  Training loss: 1.9482...  0.1352 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2180...  Training loss: 1.9298...  0.1323 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2181...  Training loss: 1.9186...  0.1301 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2182...  Training loss: 1.9237...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2183...  Training loss: 1.9541...  0.1316 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2184...  Training loss: 1.9370...  0.1359 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2185...  Training loss: 1.9566...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2186...  Training loss: 1.9652...  0.1317 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2187...  Training loss: 1.9797...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2188...  Training loss: 1.9731...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2189...  Training loss: 1.9297...  0.1304 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2190...  Training loss: 1.9437...  0.1314 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2191...  Training loss: 1.9270...  0.1360 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2192...  Training loss: 1.9650...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2193...  Training loss: 1.9423...  0.1305 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2194...  Training loss: 1.9474...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2195...  Training loss: 1.9505...  0.1308 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2196...  Training loss: 1.9280...  0.1302 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2197...  Training loss: 1.9608...  0.1346 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2198...  Training loss: 1.9704...  0.1314 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2199...  Training loss: 1.9784...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2200...  Training loss: 1.9539...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2201...  Training loss: 1.9496...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2202...  Training loss: 1.9767...  0.1318 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15...  Training Step: 2203...  Training loss: 1.9758...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2204...  Training loss: 1.9476...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2205...  Training loss: 1.9457...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2206...  Training loss: 1.9558...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2207...  Training loss: 1.9355...  0.1359 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2208...  Training loss: 1.9451...  0.1346 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2209...  Training loss: 1.9571...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2210...  Training loss: 1.9430...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2211...  Training loss: 1.9593...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2212...  Training loss: 1.9286...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2213...  Training loss: 1.9641...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2214...  Training loss: 1.9528...  0.1302 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2215...  Training loss: 1.9253...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2216...  Training loss: 1.9302...  0.1774 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2217...  Training loss: 1.9433...  0.1839 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2218...  Training loss: 1.9545...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2219...  Training loss: 1.9515...  0.1346 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2220...  Training loss: 1.9334...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2221...  Training loss: 1.9376...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2222...  Training loss: 1.9452...  0.1358 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2223...  Training loss: 1.9420...  0.1303 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2224...  Training loss: 1.9472...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2225...  Training loss: 1.9304...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2226...  Training loss: 1.9532...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2227...  Training loss: 1.9518...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2228...  Training loss: 1.9442...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2229...  Training loss: 1.9454...  0.1316 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2230...  Training loss: 1.9733...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2231...  Training loss: 1.9666...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2232...  Training loss: 1.9732...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2233...  Training loss: 1.9403...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2234...  Training loss: 1.9389...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2235...  Training loss: 1.9533...  0.1350 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2236...  Training loss: 1.9684...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2237...  Training loss: 1.9557...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2238...  Training loss: 1.9511...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2239...  Training loss: 1.9471...  0.1353 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2240...  Training loss: 1.9640...  0.1362 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2241...  Training loss: 1.9248...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2242...  Training loss: 1.9142...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2243...  Training loss: 1.9552...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2244...  Training loss: 1.9566...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2245...  Training loss: 1.9582...  0.1346 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2246...  Training loss: 1.9685...  0.1314 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2247...  Training loss: 1.9638...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2248...  Training loss: 1.9669...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2249...  Training loss: 1.9943...  0.1320 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2250...  Training loss: 1.9680...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2251...  Training loss: 1.9875...  0.1300 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2252...  Training loss: 1.9654...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2253...  Training loss: 1.9709...  0.1352 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2254...  Training loss: 1.9648...  0.1332 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2255...  Training loss: 1.9925...  0.1312 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2256...  Training loss: 1.9719...  0.1294 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2257...  Training loss: 1.9443...  0.1295 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2258...  Training loss: 1.9846...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2259...  Training loss: 1.9614...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2260...  Training loss: 1.9638...  0.1319 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2261...  Training loss: 1.9536...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2262...  Training loss: 1.9375...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2263...  Training loss: 1.9189...  0.1332 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2264...  Training loss: 1.9248...  0.1343 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2265...  Training loss: 1.9444...  0.1295 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2266...  Training loss: 1.9296...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2267...  Training loss: 1.9566...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2268...  Training loss: 1.9526...  0.1332 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2269...  Training loss: 1.9687...  0.1328 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2270...  Training loss: 1.9576...  0.1322 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2271...  Training loss: 1.9412...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2272...  Training loss: 1.9576...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2273...  Training loss: 1.9514...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2274...  Training loss: 1.9494...  0.1318 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2275...  Training loss: 1.9654...  0.1318 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2276...  Training loss: 1.9421...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2277...  Training loss: 1.9545...  0.1321 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2278...  Training loss: 1.9509...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2279...  Training loss: 1.9529...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2280...  Training loss: 1.9670...  0.1319 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2281...  Training loss: 1.9562...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2282...  Training loss: 1.9282...  0.1311 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2283...  Training loss: 1.9305...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2284...  Training loss: 1.9348...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2285...  Training loss: 1.9511...  0.1316 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2286...  Training loss: 1.9297...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2287...  Training loss: 1.9657...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2288...  Training loss: 1.9380...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2289...  Training loss: 1.9467...  0.1328 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2290...  Training loss: 1.9410...  0.1354 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2291...  Training loss: 1.9570...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2292...  Training loss: 1.9401...  0.1347 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2293...  Training loss: 1.9448...  0.1320 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2294...  Training loss: 1.9443...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2295...  Training loss: 1.9276...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2296...  Training loss: 1.9270...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2297...  Training loss: 1.9195...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2298...  Training loss: 1.9447...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2299...  Training loss: 1.9353...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2300...  Training loss: 1.9298...  0.1361 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2301...  Training loss: 1.9423...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2302...  Training loss: 1.9371...  0.1337 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15...  Training Step: 2303...  Training loss: 1.9503...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2304...  Training loss: 1.9389...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2305...  Training loss: 1.9363...  0.1319 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2306...  Training loss: 1.9495...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2307...  Training loss: 1.9460...  0.1366 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2308...  Training loss: 1.9310...  0.1307 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2309...  Training loss: 1.9331...  0.1354 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2310...  Training loss: 1.9354...  0.1343 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2311...  Training loss: 1.9423...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2312...  Training loss: 1.9230...  0.1355 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2313...  Training loss: 1.9307...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2314...  Training loss: 1.9236...  0.1360 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2315...  Training loss: 1.9492...  0.1320 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2316...  Training loss: 1.9169...  0.1802 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2317...  Training loss: 1.9174...  0.1836 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2318...  Training loss: 1.9657...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2319...  Training loss: 1.9488...  0.1309 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2320...  Training loss: 1.9378...  0.1346 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2321...  Training loss: 1.9530...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2322...  Training loss: 1.9404...  0.1316 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2323...  Training loss: 1.9419...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2324...  Training loss: 1.9663...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2325...  Training loss: 1.9850...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2326...  Training loss: 1.9775...  0.1338 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2327...  Training loss: 1.9631...  0.1350 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2328...  Training loss: 1.9647...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2329...  Training loss: 1.9558...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2330...  Training loss: 1.9590...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2331...  Training loss: 1.9531...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2332...  Training loss: 1.9412...  0.1350 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2333...  Training loss: 1.9638...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2334...  Training loss: 1.9555...  0.1352 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2335...  Training loss: 1.9421...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2336...  Training loss: 1.9547...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2337...  Training loss: 1.9698...  0.1309 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2338...  Training loss: 1.9324...  0.1345 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2339...  Training loss: 1.9493...  0.1332 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2340...  Training loss: 1.9530...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2341...  Training loss: 1.9533...  0.1322 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2342...  Training loss: 1.9445...  0.1312 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2343...  Training loss: 1.9472...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2344...  Training loss: 1.9360...  0.1348 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2345...  Training loss: 1.9517...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2346...  Training loss: 1.9492...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2347...  Training loss: 1.9503...  0.1329 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2348...  Training loss: 1.9605...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2349...  Training loss: 1.9437...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2350...  Training loss: 1.9671...  0.1342 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2351...  Training loss: 1.9776...  0.1314 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2352...  Training loss: 1.9374...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2353...  Training loss: 1.9468...  0.1320 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2354...  Training loss: 1.9409...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2355...  Training loss: 1.9595...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2356...  Training loss: 1.9395...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2357...  Training loss: 1.9459...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2358...  Training loss: 1.9505...  0.1345 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2359...  Training loss: 1.9532...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2360...  Training loss: 1.9510...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2361...  Training loss: 1.9597...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2362...  Training loss: 1.9587...  0.1311 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2363...  Training loss: 1.9528...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2364...  Training loss: 1.9273...  0.1357 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2365...  Training loss: 1.9563...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2366...  Training loss: 1.9296...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2367...  Training loss: 1.9227...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2368...  Training loss: 1.9585...  0.1339 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2369...  Training loss: 1.9178...  0.1324 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2370...  Training loss: 1.9559...  0.1341 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2371...  Training loss: 1.9436...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2372...  Training loss: 1.9181...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2373...  Training loss: 1.9312...  0.1330 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2374...  Training loss: 1.9323...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2375...  Training loss: 1.9250...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2376...  Training loss: 1.9182...  0.1321 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2377...  Training loss: 1.9314...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2378...  Training loss: 1.9512...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2379...  Training loss: 1.9635...  0.1352 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2380...  Training loss: 1.9206...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2381...  Training loss: 1.9431...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2382...  Training loss: 1.9481...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2383...  Training loss: 1.9014...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2384...  Training loss: 1.9315...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2385...  Training loss: 1.9114...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2386...  Training loss: 1.9124...  0.1344 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2387...  Training loss: 1.9263...  0.1355 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2388...  Training loss: 1.9274...  0.1347 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2389...  Training loss: 1.9366...  0.1347 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2390...  Training loss: 1.9383...  0.1321 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2391...  Training loss: 1.9563...  0.1349 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2392...  Training loss: 1.9125...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2393...  Training loss: 1.9205...  0.1353 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2394...  Training loss: 1.9365...  0.1335 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2395...  Training loss: 1.8865...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2396...  Training loss: 1.9203...  0.1355 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2397...  Training loss: 1.9354...  0.1321 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2398...  Training loss: 1.9018...  0.1328 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2399...  Training loss: 1.9045...  0.1315 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2400...  Training loss: 1.9187...  0.1353 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2401...  Training loss: 1.9272...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2402...  Training loss: 1.9254...  0.1336 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/15...  Training Step: 2403...  Training loss: 1.9314...  0.1326 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2404...  Training loss: 1.9384...  0.1361 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2405...  Training loss: 1.9272...  0.1354 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2406...  Training loss: 1.9058...  0.1331 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2407...  Training loss: 1.9306...  0.1334 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2408...  Training loss: 1.9579...  0.1336 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2409...  Training loss: 1.9249...  0.1364 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2410...  Training loss: 1.9293...  0.1294 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2411...  Training loss: 1.9458...  0.1348 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2412...  Training loss: 1.9614...  0.1310 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2413...  Training loss: 1.9801...  0.1337 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2414...  Training loss: 1.9780...  0.1327 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2415...  Training loss: 1.9815...  0.1340 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2416...  Training loss: 1.9345...  0.1301 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2417...  Training loss: 1.9760...  0.1314 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2418...  Training loss: 1.9363...  0.1350 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2419...  Training loss: 1.9357...  0.1333 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2420...  Training loss: 1.9439...  0.1351 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2421...  Training loss: 1.9479...  0.1318 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2422...  Training loss: 1.9449...  0.1325 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2423...  Training loss: 1.9340...  0.1354 sec/batch\n",
      "Epoch: 8/15...  Training Step: 2424...  Training loss: 1.9331...  0.1309 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2425...  Training loss: 2.0091...  0.1312 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2426...  Training loss: 1.9527...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2427...  Training loss: 1.9323...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2428...  Training loss: 1.9375...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2429...  Training loss: 1.9440...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2430...  Training loss: 1.9651...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2431...  Training loss: 1.9550...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2432...  Training loss: 1.9480...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2433...  Training loss: 1.9060...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2434...  Training loss: 1.9175...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2435...  Training loss: 1.9051...  0.1307 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2436...  Training loss: 1.9248...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2437...  Training loss: 1.9466...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2438...  Training loss: 1.9423...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2439...  Training loss: 1.9538...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2440...  Training loss: 1.9489...  0.1307 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2441...  Training loss: 1.9692...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2442...  Training loss: 1.9549...  0.1324 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2443...  Training loss: 1.9765...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2444...  Training loss: 1.9745...  0.1333 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2445...  Training loss: 1.9456...  0.1320 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2446...  Training loss: 1.9430...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2447...  Training loss: 1.9584...  0.1319 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2448...  Training loss: 1.9383...  0.1299 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2449...  Training loss: 1.9781...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2450...  Training loss: 1.9694...  0.1310 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2451...  Training loss: 1.9368...  0.1357 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2452...  Training loss: 1.9613...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2453...  Training loss: 1.9752...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2454...  Training loss: 1.9530...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2455...  Training loss: 1.9472...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2456...  Training loss: 1.9484...  0.1310 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2457...  Training loss: 1.9385...  0.1321 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2458...  Training loss: 1.9447...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2459...  Training loss: 1.9323...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2460...  Training loss: 1.9291...  0.1303 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2461...  Training loss: 1.9552...  0.1316 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2462...  Training loss: 1.9262...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2463...  Training loss: 1.9430...  0.1303 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2464...  Training loss: 1.9396...  0.1308 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2465...  Training loss: 1.9427...  0.1355 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2466...  Training loss: 1.9329...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2467...  Training loss: 1.9407...  0.1316 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2468...  Training loss: 1.9191...  0.1350 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2469...  Training loss: 1.9410...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2470...  Training loss: 1.9189...  0.1355 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2471...  Training loss: 1.9142...  0.1353 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2472...  Training loss: 1.9244...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2473...  Training loss: 1.9227...  0.1350 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2474...  Training loss: 1.9044...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2475...  Training loss: 1.9208...  0.1330 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2476...  Training loss: 1.9291...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2477...  Training loss: 1.9216...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2478...  Training loss: 1.9211...  0.1327 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2479...  Training loss: 1.9228...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2480...  Training loss: 1.9265...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2481...  Training loss: 1.9171...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2482...  Training loss: 1.9248...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2483...  Training loss: 1.9111...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2484...  Training loss: 1.9081...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2485...  Training loss: 1.9083...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2486...  Training loss: 1.9362...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2487...  Training loss: 1.9156...  0.1349 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2488...  Training loss: 1.9345...  0.1356 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2489...  Training loss: 1.9396...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2490...  Training loss: 1.9706...  0.1317 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2491...  Training loss: 1.9502...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2492...  Training loss: 1.9126...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2493...  Training loss: 1.9270...  0.1314 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2494...  Training loss: 1.9149...  0.1387 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2495...  Training loss: 1.9428...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2496...  Training loss: 1.9293...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2497...  Training loss: 1.9255...  0.1358 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2498...  Training loss: 1.9294...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2499...  Training loss: 1.9101...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2500...  Training loss: 1.9383...  0.1300 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2501...  Training loss: 1.9548...  0.1359 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2502...  Training loss: 1.9633...  0.1336 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15...  Training Step: 2503...  Training loss: 1.9367...  0.1309 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2504...  Training loss: 1.9315...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2505...  Training loss: 1.9571...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2506...  Training loss: 1.9591...  0.1309 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2507...  Training loss: 1.9302...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2508...  Training loss: 1.9274...  0.1361 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2509...  Training loss: 1.9425...  0.1352 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2510...  Training loss: 1.9243...  0.1324 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2511...  Training loss: 1.9194...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2512...  Training loss: 1.9307...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2513...  Training loss: 1.9292...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2514...  Training loss: 1.9362...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2515...  Training loss: 1.9193...  0.1356 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2516...  Training loss: 1.9444...  0.1352 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2517...  Training loss: 1.9330...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2518...  Training loss: 1.9045...  0.1308 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2519...  Training loss: 1.9185...  0.1325 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2520...  Training loss: 1.9239...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2521...  Training loss: 1.9338...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2522...  Training loss: 1.9392...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2523...  Training loss: 1.9199...  0.1312 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2524...  Training loss: 1.9136...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2525...  Training loss: 1.9303...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2526...  Training loss: 1.9310...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2527...  Training loss: 1.9318...  0.1333 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2528...  Training loss: 1.9089...  0.1307 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2529...  Training loss: 1.9365...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2530...  Training loss: 1.9346...  0.1321 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2531...  Training loss: 1.9429...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2532...  Training loss: 1.9331...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2533...  Training loss: 1.9540...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2534...  Training loss: 1.9509...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2535...  Training loss: 1.9580...  0.1324 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2536...  Training loss: 1.9264...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2537...  Training loss: 1.9158...  0.1324 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2538...  Training loss: 1.9374...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2539...  Training loss: 1.9489...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2540...  Training loss: 1.9398...  0.1346 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2541...  Training loss: 1.9322...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2542...  Training loss: 1.9265...  0.1360 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2543...  Training loss: 1.9477...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2544...  Training loss: 1.9059...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2545...  Training loss: 1.8998...  0.1327 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2546...  Training loss: 1.9320...  0.1364 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2547...  Training loss: 1.9503...  0.1354 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2548...  Training loss: 1.9374...  0.1301 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2549...  Training loss: 1.9463...  0.1327 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2550...  Training loss: 1.9489...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2551...  Training loss: 1.9475...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2552...  Training loss: 1.9754...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2553...  Training loss: 1.9520...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2554...  Training loss: 1.9719...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2555...  Training loss: 1.9560...  0.1330 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2556...  Training loss: 1.9567...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2557...  Training loss: 1.9430...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2558...  Training loss: 1.9760...  0.1346 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2559...  Training loss: 1.9449...  0.1373 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2560...  Training loss: 1.9164...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2561...  Training loss: 1.9654...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2562...  Training loss: 1.9354...  0.1307 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2563...  Training loss: 1.9343...  0.1329 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2564...  Training loss: 1.9364...  0.1359 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2565...  Training loss: 1.9144...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2566...  Training loss: 1.9126...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2567...  Training loss: 1.9103...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2568...  Training loss: 1.9330...  0.1304 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2569...  Training loss: 1.9092...  0.1292 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2570...  Training loss: 1.9357...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2571...  Training loss: 1.9284...  0.1354 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2572...  Training loss: 1.9567...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2573...  Training loss: 1.9419...  0.1350 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2574...  Training loss: 1.9152...  0.1346 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2575...  Training loss: 1.9470...  0.1359 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2576...  Training loss: 1.9369...  0.1329 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2577...  Training loss: 1.9351...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2578...  Training loss: 1.9486...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2579...  Training loss: 1.9242...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2580...  Training loss: 1.9440...  0.1346 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2581...  Training loss: 1.9294...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2582...  Training loss: 1.9321...  0.1312 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2583...  Training loss: 1.9439...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2584...  Training loss: 1.9400...  0.1313 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2585...  Training loss: 1.9100...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2586...  Training loss: 1.9187...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2587...  Training loss: 1.9189...  0.1329 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2588...  Training loss: 1.9325...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2589...  Training loss: 1.9075...  0.1320 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2590...  Training loss: 1.9495...  0.1353 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2591...  Training loss: 1.9227...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2592...  Training loss: 1.9287...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2593...  Training loss: 1.9336...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2594...  Training loss: 1.9383...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2595...  Training loss: 1.9270...  0.1301 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2596...  Training loss: 1.9294...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2597...  Training loss: 1.9306...  0.1351 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2598...  Training loss: 1.9161...  0.1308 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2599...  Training loss: 1.9117...  0.1354 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2600...  Training loss: 1.9061...  0.1354 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2601...  Training loss: 1.9230...  0.1360 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2602...  Training loss: 1.9218...  0.1315 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15...  Training Step: 2603...  Training loss: 1.9138...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2604...  Training loss: 1.9094...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2605...  Training loss: 1.9112...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2606...  Training loss: 1.9372...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2607...  Training loss: 1.9296...  0.1309 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2608...  Training loss: 1.9139...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2609...  Training loss: 1.9284...  0.1325 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2610...  Training loss: 1.9219...  0.1313 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2611...  Training loss: 1.9166...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2612...  Training loss: 1.9131...  0.1348 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2613...  Training loss: 1.9163...  0.1879 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2614...  Training loss: 1.9196...  0.1771 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2615...  Training loss: 1.9025...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2616...  Training loss: 1.9167...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2617...  Training loss: 1.9171...  0.1332 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2618...  Training loss: 1.9316...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2619...  Training loss: 1.9012...  0.1319 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2620...  Training loss: 1.9050...  0.1320 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2621...  Training loss: 1.9383...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2622...  Training loss: 1.9234...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2623...  Training loss: 1.9206...  0.1356 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2624...  Training loss: 1.9395...  0.1316 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2625...  Training loss: 1.9193...  0.1343 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2626...  Training loss: 1.9317...  0.1358 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2627...  Training loss: 1.9435...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2628...  Training loss: 1.9680...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2629...  Training loss: 1.9622...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2630...  Training loss: 1.9467...  0.1346 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2631...  Training loss: 1.9567...  0.1333 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2632...  Training loss: 1.9478...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2633...  Training loss: 1.9488...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2634...  Training loss: 1.9419...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2635...  Training loss: 1.9310...  0.1305 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2636...  Training loss: 1.9340...  0.1353 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2637...  Training loss: 1.9451...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2638...  Training loss: 1.9298...  0.1349 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2639...  Training loss: 1.9330...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2640...  Training loss: 1.9485...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2641...  Training loss: 1.9284...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2642...  Training loss: 1.9302...  0.1330 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2643...  Training loss: 1.9435...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2644...  Training loss: 1.9390...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2645...  Training loss: 1.9306...  0.1314 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2646...  Training loss: 1.9325...  0.1350 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2647...  Training loss: 1.9253...  0.1352 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2648...  Training loss: 1.9219...  0.1313 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2649...  Training loss: 1.9257...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2650...  Training loss: 1.9350...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2651...  Training loss: 1.9475...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2652...  Training loss: 1.9237...  0.1329 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2653...  Training loss: 1.9538...  0.1328 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2654...  Training loss: 1.9481...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2655...  Training loss: 1.9216...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2656...  Training loss: 1.9345...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2657...  Training loss: 1.9226...  0.1327 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2658...  Training loss: 1.9378...  0.1352 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2659...  Training loss: 1.9235...  0.1355 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2660...  Training loss: 1.9211...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2661...  Training loss: 1.9302...  0.1356 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2662...  Training loss: 1.9381...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2663...  Training loss: 1.9321...  0.1330 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2664...  Training loss: 1.9499...  0.1315 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2665...  Training loss: 1.9484...  0.1357 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2666...  Training loss: 1.9324...  0.1357 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2667...  Training loss: 1.9089...  0.1319 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2668...  Training loss: 1.9332...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2669...  Training loss: 1.9133...  0.1329 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2670...  Training loss: 1.9060...  0.1340 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2671...  Training loss: 1.9478...  0.1349 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2672...  Training loss: 1.8964...  0.1349 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2673...  Training loss: 1.9340...  0.1324 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2674...  Training loss: 1.9336...  0.1316 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2675...  Training loss: 1.9044...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2676...  Training loss: 1.9232...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2677...  Training loss: 1.9127...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2678...  Training loss: 1.9065...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2679...  Training loss: 1.9054...  0.1317 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2680...  Training loss: 1.9147...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2681...  Training loss: 1.9335...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2682...  Training loss: 1.9552...  0.1317 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2683...  Training loss: 1.9085...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2684...  Training loss: 1.9334...  0.1336 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2685...  Training loss: 1.9232...  0.1305 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2686...  Training loss: 1.8846...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2687...  Training loss: 1.9152...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2688...  Training loss: 1.8945...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2689...  Training loss: 1.9049...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2690...  Training loss: 1.9196...  0.1368 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2691...  Training loss: 1.9097...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2692...  Training loss: 1.9154...  0.1319 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2693...  Training loss: 1.9235...  0.1338 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2694...  Training loss: 1.9242...  0.1333 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2695...  Training loss: 1.8913...  0.1344 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2696...  Training loss: 1.9008...  0.1361 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2697...  Training loss: 1.9227...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2698...  Training loss: 1.8615...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2699...  Training loss: 1.9081...  0.1335 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2700...  Training loss: 1.9165...  0.1359 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2701...  Training loss: 1.8858...  0.1312 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2702...  Training loss: 1.8863...  0.1330 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/15...  Training Step: 2703...  Training loss: 1.8973...  0.1319 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2704...  Training loss: 1.9093...  0.1339 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2705...  Training loss: 1.9017...  0.1357 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2706...  Training loss: 1.9175...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2707...  Training loss: 1.9229...  0.1323 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2708...  Training loss: 1.9121...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2709...  Training loss: 1.8911...  0.1354 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2710...  Training loss: 1.9196...  0.1306 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2711...  Training loss: 1.9517...  0.1334 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2712...  Training loss: 1.9104...  0.1349 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2713...  Training loss: 1.9135...  0.1347 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2714...  Training loss: 1.9330...  0.1345 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2715...  Training loss: 1.9434...  0.1325 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2716...  Training loss: 1.9587...  0.1337 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2717...  Training loss: 1.9617...  0.1342 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2718...  Training loss: 1.9607...  0.1353 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2719...  Training loss: 1.9249...  0.1331 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2720...  Training loss: 1.9589...  0.1341 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2721...  Training loss: 1.9255...  0.1352 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2722...  Training loss: 1.9221...  0.1326 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2723...  Training loss: 1.9267...  0.1322 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2724...  Training loss: 1.9299...  0.1327 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2725...  Training loss: 1.9325...  0.1318 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2726...  Training loss: 1.9281...  0.1299 sec/batch\n",
      "Epoch: 9/15...  Training Step: 2727...  Training loss: 1.9186...  0.1350 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2728...  Training loss: 1.9947...  0.1306 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2729...  Training loss: 1.9399...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2730...  Training loss: 1.9146...  0.1349 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2731...  Training loss: 1.9237...  0.1783 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2732...  Training loss: 1.9313...  0.1836 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2733...  Training loss: 1.9522...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2734...  Training loss: 1.9385...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2735...  Training loss: 1.9352...  0.1306 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2736...  Training loss: 1.8932...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2737...  Training loss: 1.8954...  0.1342 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2738...  Training loss: 1.8972...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2739...  Training loss: 1.9103...  0.1356 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2740...  Training loss: 1.9292...  0.1318 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2741...  Training loss: 1.9231...  0.1353 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2742...  Training loss: 1.9405...  0.1318 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2743...  Training loss: 1.9345...  0.1798 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2744...  Training loss: 1.9585...  0.1836 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2745...  Training loss: 1.9365...  0.1350 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2746...  Training loss: 1.9640...  0.1313 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2747...  Training loss: 1.9664...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2748...  Training loss: 1.9350...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2749...  Training loss: 1.9368...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2750...  Training loss: 1.9364...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2751...  Training loss: 1.9201...  0.1310 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2752...  Training loss: 1.9629...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2753...  Training loss: 1.9450...  0.1319 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2754...  Training loss: 1.9246...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2755...  Training loss: 1.9396...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2756...  Training loss: 1.9622...  0.1305 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2757...  Training loss: 1.9338...  0.1347 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2758...  Training loss: 1.9237...  0.1361 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2759...  Training loss: 1.9293...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2760...  Training loss: 1.9258...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2761...  Training loss: 1.9350...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2762...  Training loss: 1.9175...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2763...  Training loss: 1.9155...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2764...  Training loss: 1.9406...  0.1367 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2765...  Training loss: 1.9128...  0.1351 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2766...  Training loss: 1.9290...  0.1356 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2767...  Training loss: 1.9221...  0.1350 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2768...  Training loss: 1.9254...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2769...  Training loss: 1.9168...  0.1364 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2770...  Training loss: 1.9330...  0.1326 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2771...  Training loss: 1.8947...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2772...  Training loss: 1.9214...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2773...  Training loss: 1.9048...  0.1317 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2774...  Training loss: 1.8953...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2775...  Training loss: 1.9079...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2776...  Training loss: 1.9037...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2777...  Training loss: 1.8874...  0.1331 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2778...  Training loss: 1.9089...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2779...  Training loss: 1.9167...  0.1364 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2780...  Training loss: 1.9093...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2781...  Training loss: 1.9016...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2782...  Training loss: 1.9051...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2783...  Training loss: 1.9026...  0.1360 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2784...  Training loss: 1.8995...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2785...  Training loss: 1.9142...  0.1342 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2786...  Training loss: 1.8941...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2787...  Training loss: 1.8890...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2788...  Training loss: 1.8952...  0.1313 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2789...  Training loss: 1.9205...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2790...  Training loss: 1.8976...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2791...  Training loss: 1.9157...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2792...  Training loss: 1.9281...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2793...  Training loss: 1.9561...  0.1349 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2794...  Training loss: 1.9399...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2795...  Training loss: 1.8958...  0.1311 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2796...  Training loss: 1.9105...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2797...  Training loss: 1.9021...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2798...  Training loss: 1.9318...  0.1335 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2799...  Training loss: 1.9077...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2800...  Training loss: 1.9123...  0.1309 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15...  Training Step: 2801...  Training loss: 1.9115...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2802...  Training loss: 1.8999...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2803...  Training loss: 1.9214...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2804...  Training loss: 1.9407...  0.1309 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2805...  Training loss: 1.9423...  0.1313 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2806...  Training loss: 1.9291...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2807...  Training loss: 1.9167...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2808...  Training loss: 1.9503...  0.1347 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2809...  Training loss: 1.9448...  0.1359 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2810...  Training loss: 1.9182...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2811...  Training loss: 1.9238...  0.1333 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2812...  Training loss: 1.9230...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2813...  Training loss: 1.9113...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2814...  Training loss: 1.9052...  0.1356 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2815...  Training loss: 1.9112...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2816...  Training loss: 1.9104...  0.1351 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2817...  Training loss: 1.9244...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2818...  Training loss: 1.9076...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2819...  Training loss: 1.9260...  0.1352 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2820...  Training loss: 1.9150...  0.1373 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2821...  Training loss: 1.8947...  0.1309 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2822...  Training loss: 1.9017...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2823...  Training loss: 1.9096...  0.1351 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2824...  Training loss: 1.9203...  0.1328 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2825...  Training loss: 1.9166...  0.1317 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2826...  Training loss: 1.8978...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2827...  Training loss: 1.9036...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2828...  Training loss: 1.9095...  0.1311 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2829...  Training loss: 1.9070...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2830...  Training loss: 1.9176...  0.1342 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2831...  Training loss: 1.9091...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2832...  Training loss: 1.9186...  0.1321 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2833...  Training loss: 1.9182...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2834...  Training loss: 1.9170...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2835...  Training loss: 1.9200...  0.1360 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2836...  Training loss: 1.9346...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2837...  Training loss: 1.9413...  0.1309 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2838...  Training loss: 1.9435...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2839...  Training loss: 1.9068...  0.1317 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2840...  Training loss: 1.9021...  0.1299 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2841...  Training loss: 1.9172...  0.1365 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2842...  Training loss: 1.9333...  0.1351 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2843...  Training loss: 1.9154...  0.1355 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2844...  Training loss: 1.9186...  0.1346 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2845...  Training loss: 1.9183...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2846...  Training loss: 1.9390...  0.1342 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2847...  Training loss: 1.8909...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2848...  Training loss: 1.8946...  0.1335 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2849...  Training loss: 1.9197...  0.1299 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2850...  Training loss: 1.9198...  0.1352 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2851...  Training loss: 1.9205...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2852...  Training loss: 1.9357...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2853...  Training loss: 1.9268...  0.1347 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2854...  Training loss: 1.9261...  0.1321 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2855...  Training loss: 1.9629...  0.1333 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2856...  Training loss: 1.9339...  0.1329 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2857...  Training loss: 1.9517...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2858...  Training loss: 1.9356...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2859...  Training loss: 1.9417...  0.1317 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2860...  Training loss: 1.9366...  0.1347 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2861...  Training loss: 1.9601...  0.1342 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2862...  Training loss: 1.9348...  0.1323 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2863...  Training loss: 1.9066...  0.1353 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2864...  Training loss: 1.9526...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2865...  Training loss: 1.9248...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2866...  Training loss: 1.9340...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2867...  Training loss: 1.9235...  0.1358 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2868...  Training loss: 1.9040...  0.1362 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2869...  Training loss: 1.8885...  0.1321 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2870...  Training loss: 1.8976...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2871...  Training loss: 1.9134...  0.1361 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2872...  Training loss: 1.8900...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2873...  Training loss: 1.9241...  0.1323 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2874...  Training loss: 1.9173...  0.1305 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2875...  Training loss: 1.9407...  0.1331 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2876...  Training loss: 1.9304...  0.1365 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2877...  Training loss: 1.9041...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2878...  Training loss: 1.9247...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2879...  Training loss: 1.9198...  0.1322 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2880...  Training loss: 1.9141...  0.1362 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2881...  Training loss: 1.9306...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2882...  Training loss: 1.9047...  0.1360 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2883...  Training loss: 1.9258...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2884...  Training loss: 1.9235...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2885...  Training loss: 1.9248...  0.1360 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2886...  Training loss: 1.9310...  0.1328 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2887...  Training loss: 1.9239...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2888...  Training loss: 1.8916...  0.1362 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2889...  Training loss: 1.9064...  0.1346 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2890...  Training loss: 1.9014...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2891...  Training loss: 1.9105...  0.1357 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2892...  Training loss: 1.8912...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2893...  Training loss: 1.9400...  0.1333 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2894...  Training loss: 1.9073...  0.1355 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2895...  Training loss: 1.9169...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2896...  Training loss: 1.9214...  0.1357 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2897...  Training loss: 1.9224...  0.1356 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2898...  Training loss: 1.9117...  0.1335 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15...  Training Step: 2899...  Training loss: 1.9168...  0.1305 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2900...  Training loss: 1.9129...  0.1326 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2901...  Training loss: 1.9097...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2902...  Training loss: 1.9051...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2903...  Training loss: 1.8869...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2904...  Training loss: 1.9136...  0.1302 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2905...  Training loss: 1.9048...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2906...  Training loss: 1.8955...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2907...  Training loss: 1.8992...  0.1302 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2908...  Training loss: 1.9081...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2909...  Training loss: 1.9240...  0.1363 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2910...  Training loss: 1.9028...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2911...  Training loss: 1.9049...  0.1364 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2912...  Training loss: 1.9117...  0.1353 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2913...  Training loss: 1.9134...  0.1357 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2914...  Training loss: 1.9028...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2915...  Training loss: 1.9046...  0.1305 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2916...  Training loss: 1.9148...  0.1363 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2917...  Training loss: 1.9134...  0.1316 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2918...  Training loss: 1.8894...  0.1307 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2919...  Training loss: 1.9014...  0.1334 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2920...  Training loss: 1.8976...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2921...  Training loss: 1.9171...  0.1335 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2922...  Training loss: 1.8888...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2923...  Training loss: 1.8870...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2924...  Training loss: 1.9321...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2925...  Training loss: 1.9162...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2926...  Training loss: 1.9069...  0.1347 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2927...  Training loss: 1.9215...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2928...  Training loss: 1.9132...  0.1304 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2929...  Training loss: 1.9166...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2930...  Training loss: 1.9302...  0.1324 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2931...  Training loss: 1.9507...  0.1301 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2932...  Training loss: 1.9430...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2933...  Training loss: 1.9301...  0.1322 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2934...  Training loss: 1.9380...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2935...  Training loss: 1.9271...  0.1330 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2936...  Training loss: 1.9274...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2937...  Training loss: 1.9308...  0.1356 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2938...  Training loss: 1.9137...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2939...  Training loss: 1.9209...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2940...  Training loss: 1.9273...  0.1378 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2941...  Training loss: 1.9195...  0.1335 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2942...  Training loss: 1.9175...  0.1331 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2943...  Training loss: 1.9330...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2944...  Training loss: 1.9053...  0.1327 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2945...  Training loss: 1.9262...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2946...  Training loss: 1.9229...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2947...  Training loss: 1.9229...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2948...  Training loss: 1.9197...  0.1326 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2949...  Training loss: 1.9143...  0.1354 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2950...  Training loss: 1.9125...  0.1319 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2951...  Training loss: 1.9153...  0.1309 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2952...  Training loss: 1.9139...  0.1318 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2953...  Training loss: 1.9257...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2954...  Training loss: 1.9306...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2955...  Training loss: 1.9138...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2956...  Training loss: 1.9363...  0.1361 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2957...  Training loss: 1.9489...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2958...  Training loss: 1.9025...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2959...  Training loss: 1.9167...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2960...  Training loss: 1.9089...  0.1352 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2961...  Training loss: 1.9252...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2962...  Training loss: 1.9097...  0.1316 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2963...  Training loss: 1.9189...  0.1349 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2964...  Training loss: 1.9150...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2965...  Training loss: 1.9213...  0.1323 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2966...  Training loss: 1.9178...  0.1339 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2967...  Training loss: 1.9276...  0.1364 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2968...  Training loss: 1.9341...  0.1302 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2969...  Training loss: 1.9232...  0.1333 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2970...  Training loss: 1.8958...  0.1326 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2971...  Training loss: 1.9230...  0.1329 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2972...  Training loss: 1.9001...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2973...  Training loss: 1.8907...  0.1314 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2974...  Training loss: 1.9352...  0.1361 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2975...  Training loss: 1.8773...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2976...  Training loss: 1.9187...  0.1318 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2977...  Training loss: 1.9210...  0.1295 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2978...  Training loss: 1.8841...  0.1280 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2979...  Training loss: 1.9036...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2980...  Training loss: 1.9023...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2981...  Training loss: 1.8962...  0.1311 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2982...  Training loss: 1.8928...  0.1304 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2983...  Training loss: 1.8962...  0.1329 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2984...  Training loss: 1.9131...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2985...  Training loss: 1.9416...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2986...  Training loss: 1.8908...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2987...  Training loss: 1.9140...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2988...  Training loss: 1.9197...  0.1353 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2989...  Training loss: 1.8743...  0.1316 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2990...  Training loss: 1.8972...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2991...  Training loss: 1.8816...  0.1317 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2992...  Training loss: 1.8900...  0.1312 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2993...  Training loss: 1.8979...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2994...  Training loss: 1.8843...  0.1349 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2995...  Training loss: 1.9091...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2996...  Training loss: 1.9034...  0.1347 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/15...  Training Step: 2997...  Training loss: 1.9133...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2998...  Training loss: 1.8834...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 2999...  Training loss: 1.8869...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3000...  Training loss: 1.9093...  0.1346 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3001...  Training loss: 1.8534...  0.1332 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3002...  Training loss: 1.8952...  0.1324 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3003...  Training loss: 1.9022...  0.1331 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3004...  Training loss: 1.8783...  0.1341 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3005...  Training loss: 1.8649...  0.1345 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3006...  Training loss: 1.8765...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3007...  Training loss: 1.8934...  0.1369 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3008...  Training loss: 1.8945...  0.1343 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3009...  Training loss: 1.8965...  0.1337 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3010...  Training loss: 1.9004...  0.1361 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3011...  Training loss: 1.8982...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3012...  Training loss: 1.8678...  0.1360 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3013...  Training loss: 1.9064...  0.1353 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3014...  Training loss: 1.9287...  0.1363 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3015...  Training loss: 1.8942...  0.1324 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3016...  Training loss: 1.9033...  0.1374 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3017...  Training loss: 1.9202...  0.1315 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3018...  Training loss: 1.9334...  0.1336 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3019...  Training loss: 1.9479...  0.1344 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3020...  Training loss: 1.9562...  0.1331 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3021...  Training loss: 1.9599...  0.1322 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3022...  Training loss: 1.9130...  0.1333 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3023...  Training loss: 1.9486...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3024...  Training loss: 1.9157...  0.1325 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3025...  Training loss: 1.9093...  0.1348 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3026...  Training loss: 1.9178...  0.1307 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3027...  Training loss: 1.9205...  0.1320 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3028...  Training loss: 1.9130...  0.1340 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3029...  Training loss: 1.9067...  0.1338 sec/batch\n",
      "Epoch: 10/15...  Training Step: 3030...  Training loss: 1.9095...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3031...  Training loss: 1.9827...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3032...  Training loss: 1.9253...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3033...  Training loss: 1.9099...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3034...  Training loss: 1.9051...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3035...  Training loss: 1.9202...  0.1313 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3036...  Training loss: 1.9428...  0.1356 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3037...  Training loss: 1.9246...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3038...  Training loss: 1.9299...  0.1356 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3039...  Training loss: 1.8783...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3040...  Training loss: 1.8841...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3041...  Training loss: 1.8874...  0.1358 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3042...  Training loss: 1.8968...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3043...  Training loss: 1.9190...  0.1352 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3044...  Training loss: 1.9047...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3045...  Training loss: 1.9240...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3046...  Training loss: 1.9227...  0.1359 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3047...  Training loss: 1.9459...  0.1323 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3048...  Training loss: 1.9182...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3049...  Training loss: 1.9458...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3050...  Training loss: 1.9488...  0.1297 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3051...  Training loss: 1.9292...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3052...  Training loss: 1.9212...  0.1346 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3053...  Training loss: 1.9355...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3054...  Training loss: 1.9127...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3055...  Training loss: 1.9520...  0.1319 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3056...  Training loss: 1.9367...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3057...  Training loss: 1.9043...  0.1322 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3058...  Training loss: 1.9327...  0.1317 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3059...  Training loss: 1.9457...  0.1337 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3060...  Training loss: 1.9200...  0.1329 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3061...  Training loss: 1.9192...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3062...  Training loss: 1.9134...  0.1327 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3063...  Training loss: 1.9128...  0.1318 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3064...  Training loss: 1.9196...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3065...  Training loss: 1.9138...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3066...  Training loss: 1.9022...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3067...  Training loss: 1.9309...  0.1346 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3068...  Training loss: 1.8902...  0.1358 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3069...  Training loss: 1.9136...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3070...  Training loss: 1.9144...  0.1331 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3071...  Training loss: 1.9155...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3072...  Training loss: 1.9049...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3073...  Training loss: 1.9255...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3074...  Training loss: 1.8932...  0.1300 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3075...  Training loss: 1.9147...  0.1361 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3076...  Training loss: 1.9075...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3077...  Training loss: 1.8806...  0.1331 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3078...  Training loss: 1.8958...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3079...  Training loss: 1.8923...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3080...  Training loss: 1.8758...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3081...  Training loss: 1.8935...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3082...  Training loss: 1.8946...  0.1330 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3083...  Training loss: 1.8915...  0.1309 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3084...  Training loss: 1.8979...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3085...  Training loss: 1.8972...  0.1337 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3086...  Training loss: 1.8858...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3087...  Training loss: 1.8880...  0.1314 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3088...  Training loss: 1.9032...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3089...  Training loss: 1.8902...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3090...  Training loss: 1.8805...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3091...  Training loss: 1.8754...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3092...  Training loss: 1.9119...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3093...  Training loss: 1.8851...  0.1361 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3094...  Training loss: 1.9032...  0.1323 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15...  Training Step: 3095...  Training loss: 1.9169...  0.1352 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3096...  Training loss: 1.9434...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3097...  Training loss: 1.9200...  0.1364 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3098...  Training loss: 1.8814...  0.1346 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3099...  Training loss: 1.8933...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3100...  Training loss: 1.8868...  0.1312 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3101...  Training loss: 1.9167...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3102...  Training loss: 1.9025...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3103...  Training loss: 1.9041...  0.1329 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3104...  Training loss: 1.8982...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3105...  Training loss: 1.8797...  0.1325 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3106...  Training loss: 1.9085...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3107...  Training loss: 1.9211...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3108...  Training loss: 1.9281...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3109...  Training loss: 1.9159...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3110...  Training loss: 1.9090...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3111...  Training loss: 1.9223...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3112...  Training loss: 1.9331...  0.1317 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3113...  Training loss: 1.9140...  0.1364 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3114...  Training loss: 1.9033...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3115...  Training loss: 1.9131...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3116...  Training loss: 1.8930...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3117...  Training loss: 1.8998...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3118...  Training loss: 1.9050...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3119...  Training loss: 1.8978...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3120...  Training loss: 1.9163...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3121...  Training loss: 1.8930...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3122...  Training loss: 1.9148...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3123...  Training loss: 1.8989...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3124...  Training loss: 1.8873...  0.1300 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3125...  Training loss: 1.8841...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3126...  Training loss: 1.8928...  0.1351 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3127...  Training loss: 1.9119...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3128...  Training loss: 1.9017...  0.1355 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3129...  Training loss: 1.8854...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3130...  Training loss: 1.8912...  0.1343 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3131...  Training loss: 1.8966...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3132...  Training loss: 1.9087...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3133...  Training loss: 1.9075...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3134...  Training loss: 1.8882...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3135...  Training loss: 1.9092...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3136...  Training loss: 1.9096...  0.1360 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3137...  Training loss: 1.9101...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3138...  Training loss: 1.9078...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3139...  Training loss: 1.9246...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3140...  Training loss: 1.9183...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3141...  Training loss: 1.9296...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3142...  Training loss: 1.8909...  0.1322 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3143...  Training loss: 1.8927...  0.1314 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3144...  Training loss: 1.9158...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3145...  Training loss: 1.9210...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3146...  Training loss: 1.9124...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3147...  Training loss: 1.9082...  0.1301 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3148...  Training loss: 1.9032...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3149...  Training loss: 1.9286...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3150...  Training loss: 1.8874...  0.1343 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3151...  Training loss: 1.8832...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3152...  Training loss: 1.9157...  0.1346 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3153...  Training loss: 1.9128...  0.1326 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3154...  Training loss: 1.9082...  0.1359 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3155...  Training loss: 1.9262...  0.1320 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3156...  Training loss: 1.9164...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3157...  Training loss: 1.9212...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3158...  Training loss: 1.9491...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3159...  Training loss: 1.9259...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3160...  Training loss: 1.9370...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3161...  Training loss: 1.9263...  0.1318 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3162...  Training loss: 1.9242...  0.1337 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3163...  Training loss: 1.9336...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3164...  Training loss: 1.9440...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3165...  Training loss: 1.9221...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3166...  Training loss: 1.8963...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3167...  Training loss: 1.9479...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3168...  Training loss: 1.9189...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3169...  Training loss: 1.9219...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3170...  Training loss: 1.9128...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3171...  Training loss: 1.8892...  0.1315 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3172...  Training loss: 1.8793...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3173...  Training loss: 1.8924...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3174...  Training loss: 1.9094...  0.1306 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3175...  Training loss: 1.8828...  0.1311 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3176...  Training loss: 1.9147...  0.1351 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3177...  Training loss: 1.9044...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3178...  Training loss: 1.9264...  0.1358 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3179...  Training loss: 1.9205...  0.1348 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3180...  Training loss: 1.8927...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3181...  Training loss: 1.9191...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3182...  Training loss: 1.9172...  0.1314 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3183...  Training loss: 1.9068...  0.1327 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3184...  Training loss: 1.9278...  0.1337 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3185...  Training loss: 1.8979...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3186...  Training loss: 1.9193...  0.1320 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3187...  Training loss: 1.9175...  0.1337 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3188...  Training loss: 1.9128...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3189...  Training loss: 1.9171...  0.1322 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3190...  Training loss: 1.9166...  0.1356 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3191...  Training loss: 1.8899...  0.1327 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3192...  Training loss: 1.8922...  0.1331 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15...  Training Step: 3193...  Training loss: 1.8982...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3194...  Training loss: 1.9064...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3195...  Training loss: 1.8901...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3196...  Training loss: 1.9313...  0.1331 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3197...  Training loss: 1.8947...  0.1306 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3198...  Training loss: 1.9117...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3199...  Training loss: 1.9133...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3200...  Training loss: 1.9225...  0.1356 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3201...  Training loss: 1.8998...  0.1359 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3202...  Training loss: 1.8989...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3203...  Training loss: 1.9096...  0.1351 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3204...  Training loss: 1.8955...  0.1346 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3205...  Training loss: 1.8901...  0.1322 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3206...  Training loss: 1.8828...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3207...  Training loss: 1.9062...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3208...  Training loss: 1.8999...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3209...  Training loss: 1.8884...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3210...  Training loss: 1.8922...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3211...  Training loss: 1.8936...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3212...  Training loss: 1.9061...  0.1327 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3213...  Training loss: 1.9004...  0.1303 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3214...  Training loss: 1.8955...  0.1352 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3215...  Training loss: 1.9008...  0.1369 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3216...  Training loss: 1.9085...  0.1318 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3217...  Training loss: 1.8875...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3218...  Training loss: 1.8965...  0.1326 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3219...  Training loss: 1.8985...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3220...  Training loss: 1.8972...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3221...  Training loss: 1.8901...  0.1329 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3222...  Training loss: 1.8922...  0.1313 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3223...  Training loss: 1.8846...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3224...  Training loss: 1.9059...  0.1371 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3225...  Training loss: 1.8789...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3226...  Training loss: 1.8777...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3227...  Training loss: 1.9185...  0.1330 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3228...  Training loss: 1.9003...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3229...  Training loss: 1.8914...  0.1315 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3230...  Training loss: 1.9142...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3231...  Training loss: 1.8954...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3232...  Training loss: 1.9055...  0.1330 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3233...  Training loss: 1.9182...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3234...  Training loss: 1.9483...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3235...  Training loss: 1.9367...  0.1315 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3236...  Training loss: 1.9208...  0.1357 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3237...  Training loss: 1.9335...  0.1327 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3238...  Training loss: 1.9075...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3239...  Training loss: 1.9196...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3240...  Training loss: 1.9224...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3241...  Training loss: 1.9073...  0.1331 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3242...  Training loss: 1.9136...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3243...  Training loss: 1.9159...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3244...  Training loss: 1.9038...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3245...  Training loss: 1.9128...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3246...  Training loss: 1.9269...  0.1371 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3247...  Training loss: 1.8936...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3248...  Training loss: 1.9097...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3249...  Training loss: 1.9145...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3250...  Training loss: 1.9180...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3251...  Training loss: 1.9044...  0.1362 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3252...  Training loss: 1.9078...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3253...  Training loss: 1.8962...  0.1356 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3254...  Training loss: 1.9066...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3255...  Training loss: 1.9006...  0.1352 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3256...  Training loss: 1.9118...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3257...  Training loss: 1.9157...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3258...  Training loss: 1.9091...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3259...  Training loss: 1.9321...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3260...  Training loss: 1.9316...  0.1344 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3261...  Training loss: 1.9019...  0.1326 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3262...  Training loss: 1.9064...  0.1314 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3263...  Training loss: 1.8998...  0.1322 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3264...  Training loss: 1.9123...  0.1362 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3265...  Training loss: 1.9057...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3266...  Training loss: 1.8985...  0.1335 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3267...  Training loss: 1.9038...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3268...  Training loss: 1.9187...  0.1301 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3269...  Training loss: 1.9081...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3270...  Training loss: 1.9210...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3271...  Training loss: 1.9250...  0.1355 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3272...  Training loss: 1.9075...  0.1343 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3273...  Training loss: 1.8845...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3274...  Training loss: 1.9205...  0.1317 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3275...  Training loss: 1.8870...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3276...  Training loss: 1.8788...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3277...  Training loss: 1.9186...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3278...  Training loss: 1.8741...  0.1340 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3279...  Training loss: 1.9083...  0.1349 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3280...  Training loss: 1.9157...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3281...  Training loss: 1.8762...  0.1364 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3282...  Training loss: 1.8983...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3283...  Training loss: 1.8881...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3284...  Training loss: 1.8881...  0.1350 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3285...  Training loss: 1.8816...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3286...  Training loss: 1.8910...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3287...  Training loss: 1.9098...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3288...  Training loss: 1.9332...  0.1354 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3289...  Training loss: 1.8855...  0.1363 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3290...  Training loss: 1.9102...  0.1340 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/15...  Training Step: 3291...  Training loss: 1.9131...  0.1351 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3292...  Training loss: 1.8635...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3293...  Training loss: 1.8910...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3294...  Training loss: 1.8781...  0.1309 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3295...  Training loss: 1.8816...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3296...  Training loss: 1.8910...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3297...  Training loss: 1.8882...  0.1342 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3298...  Training loss: 1.8878...  0.1312 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3299...  Training loss: 1.8976...  0.1318 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3300...  Training loss: 1.9128...  0.1326 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3301...  Training loss: 1.8731...  0.1333 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3302...  Training loss: 1.8745...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3303...  Training loss: 1.9080...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3304...  Training loss: 1.8459...  0.1313 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3305...  Training loss: 1.8861...  0.1316 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3306...  Training loss: 1.8874...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3307...  Training loss: 1.8686...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3308...  Training loss: 1.8684...  0.1345 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3309...  Training loss: 1.8725...  0.1329 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3310...  Training loss: 1.8879...  0.1332 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3311...  Training loss: 1.8838...  0.1334 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3312...  Training loss: 1.8936...  0.1362 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3313...  Training loss: 1.8983...  0.1320 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3314...  Training loss: 1.8790...  0.1321 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3315...  Training loss: 1.8685...  0.1347 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3316...  Training loss: 1.8976...  0.1330 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3317...  Training loss: 1.9188...  0.1355 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3318...  Training loss: 1.8830...  0.1358 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3319...  Training loss: 1.8871...  0.1339 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3320...  Training loss: 1.9131...  0.1297 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3321...  Training loss: 1.9171...  0.1324 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3322...  Training loss: 1.9343...  0.1329 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3323...  Training loss: 1.9481...  0.1338 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3324...  Training loss: 1.9437...  0.1336 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3325...  Training loss: 1.9036...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3326...  Training loss: 1.9376...  0.1360 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3327...  Training loss: 1.9012...  0.1353 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3328...  Training loss: 1.9003...  0.1320 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3329...  Training loss: 1.9010...  0.1302 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3330...  Training loss: 1.9000...  0.1341 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3331...  Training loss: 1.9052...  0.1318 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3332...  Training loss: 1.8977...  0.1328 sec/batch\n",
      "Epoch: 11/15...  Training Step: 3333...  Training loss: 1.8940...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3334...  Training loss: 1.9709...  0.1370 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3335...  Training loss: 1.9219...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3336...  Training loss: 1.8907...  0.1314 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3337...  Training loss: 1.9043...  0.1359 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3338...  Training loss: 1.9133...  0.1349 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3339...  Training loss: 1.9309...  0.1331 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3340...  Training loss: 1.9147...  0.1359 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3341...  Training loss: 1.9208...  0.1369 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3342...  Training loss: 1.8675...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3343...  Training loss: 1.8670...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3344...  Training loss: 1.8693...  0.1363 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3345...  Training loss: 1.8881...  0.1310 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3346...  Training loss: 1.9095...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3347...  Training loss: 1.8959...  0.1317 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3348...  Training loss: 1.9114...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3349...  Training loss: 1.9167...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3350...  Training loss: 1.9408...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3351...  Training loss: 1.9173...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3352...  Training loss: 1.9332...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3353...  Training loss: 1.9403...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3354...  Training loss: 1.9115...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3355...  Training loss: 1.9034...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3356...  Training loss: 1.9220...  0.1330 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3357...  Training loss: 1.9023...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3358...  Training loss: 1.9439...  0.1349 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3359...  Training loss: 1.9314...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3360...  Training loss: 1.8959...  0.1326 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3361...  Training loss: 1.9237...  0.1296 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3362...  Training loss: 1.9407...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3363...  Training loss: 1.9132...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3364...  Training loss: 1.9085...  0.1357 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3365...  Training loss: 1.9155...  0.1328 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3366...  Training loss: 1.9025...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3367...  Training loss: 1.9071...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3368...  Training loss: 1.9041...  0.1368 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3369...  Training loss: 1.8810...  0.1335 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3370...  Training loss: 1.9130...  0.1346 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3371...  Training loss: 1.8862...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3372...  Training loss: 1.9052...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3373...  Training loss: 1.9053...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3374...  Training loss: 1.9023...  0.1310 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3375...  Training loss: 1.8934...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3376...  Training loss: 1.9141...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3377...  Training loss: 1.8778...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3378...  Training loss: 1.8987...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3379...  Training loss: 1.8857...  0.1315 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3380...  Training loss: 1.8675...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3381...  Training loss: 1.8840...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3382...  Training loss: 1.8852...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3383...  Training loss: 1.8602...  0.1300 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3384...  Training loss: 1.8811...  0.1313 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3385...  Training loss: 1.8821...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3386...  Training loss: 1.8814...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3387...  Training loss: 1.8776...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3388...  Training loss: 1.8843...  0.1361 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15...  Training Step: 3389...  Training loss: 1.8911...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3390...  Training loss: 1.8818...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3391...  Training loss: 1.8929...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3392...  Training loss: 1.8748...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3393...  Training loss: 1.8651...  0.1319 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3394...  Training loss: 1.8708...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3395...  Training loss: 1.9035...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3396...  Training loss: 1.8784...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3397...  Training loss: 1.8990...  0.1314 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3398...  Training loss: 1.9131...  0.1304 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3399...  Training loss: 1.9327...  0.1332 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3400...  Training loss: 1.9113...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3401...  Training loss: 1.8712...  0.1365 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3402...  Training loss: 1.8915...  0.1351 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3403...  Training loss: 1.8847...  0.1332 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3404...  Training loss: 1.9128...  0.1322 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3405...  Training loss: 1.9044...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3406...  Training loss: 1.9005...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3407...  Training loss: 1.8891...  0.1325 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3408...  Training loss: 1.8781...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3409...  Training loss: 1.9010...  0.1328 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3410...  Training loss: 1.9092...  0.1365 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3411...  Training loss: 1.9221...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3412...  Training loss: 1.9114...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3413...  Training loss: 1.8996...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3414...  Training loss: 1.9173...  0.1349 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3415...  Training loss: 1.9239...  0.1327 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3416...  Training loss: 1.8955...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3417...  Training loss: 1.8944...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3418...  Training loss: 1.8999...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3419...  Training loss: 1.8800...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3420...  Training loss: 1.8937...  0.1357 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3421...  Training loss: 1.8922...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3422...  Training loss: 1.8956...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3423...  Training loss: 1.8950...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3424...  Training loss: 1.8815...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3425...  Training loss: 1.9064...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3426...  Training loss: 1.8928...  0.1310 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3427...  Training loss: 1.8728...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3428...  Training loss: 1.8808...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3429...  Training loss: 1.8852...  0.1303 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3430...  Training loss: 1.8969...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3431...  Training loss: 1.9079...  0.1328 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3432...  Training loss: 1.8782...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3433...  Training loss: 1.8870...  0.1313 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3434...  Training loss: 1.8905...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3435...  Training loss: 1.8896...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3436...  Training loss: 1.8942...  0.1316 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3437...  Training loss: 1.8763...  0.1356 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3438...  Training loss: 1.8993...  0.1353 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3439...  Training loss: 1.9016...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3440...  Training loss: 1.9071...  0.1365 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3441...  Training loss: 1.9007...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3442...  Training loss: 1.9182...  0.1362 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3443...  Training loss: 1.9068...  0.1318 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3444...  Training loss: 1.9188...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3445...  Training loss: 1.8874...  0.1346 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3446...  Training loss: 1.8811...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3447...  Training loss: 1.9037...  0.1329 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3448...  Training loss: 1.9062...  0.1318 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3449...  Training loss: 1.8999...  0.1349 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3450...  Training loss: 1.9048...  0.1315 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3451...  Training loss: 1.8886...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3452...  Training loss: 1.9168...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3453...  Training loss: 1.8760...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3454...  Training loss: 1.8698...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3455...  Training loss: 1.8935...  0.1325 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3456...  Training loss: 1.9015...  0.1332 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3457...  Training loss: 1.8978...  0.1325 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3458...  Training loss: 1.9055...  0.1311 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3459...  Training loss: 1.9023...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3460...  Training loss: 1.9058...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3461...  Training loss: 1.9334...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3462...  Training loss: 1.9127...  0.1356 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3463...  Training loss: 1.9354...  0.1316 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3464...  Training loss: 1.9173...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3465...  Training loss: 1.9190...  0.1329 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3466...  Training loss: 1.9172...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3467...  Training loss: 1.9364...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3468...  Training loss: 1.9205...  0.1357 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3469...  Training loss: 1.8897...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3470...  Training loss: 1.9296...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3471...  Training loss: 1.9064...  0.1299 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3472...  Training loss: 1.9135...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3473...  Training loss: 1.9027...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3474...  Training loss: 1.8795...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3475...  Training loss: 1.8653...  0.1311 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3476...  Training loss: 1.8697...  0.1312 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3477...  Training loss: 1.8990...  0.1335 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3478...  Training loss: 1.8693...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3479...  Training loss: 1.9133...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3480...  Training loss: 1.8955...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3481...  Training loss: 1.9175...  0.1385 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3482...  Training loss: 1.9062...  0.1308 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3483...  Training loss: 1.8827...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3484...  Training loss: 1.9135...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3485...  Training loss: 1.9023...  0.1312 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3486...  Training loss: 1.9046...  0.1345 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15...  Training Step: 3487...  Training loss: 1.9129...  0.1317 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3488...  Training loss: 1.8894...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3489...  Training loss: 1.9030...  0.1353 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3490...  Training loss: 1.9068...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3491...  Training loss: 1.8992...  0.1325 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3492...  Training loss: 1.9089...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3493...  Training loss: 1.9032...  0.1325 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3494...  Training loss: 1.8685...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3495...  Training loss: 1.8827...  0.1314 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3496...  Training loss: 1.8890...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3497...  Training loss: 1.8950...  0.1353 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3498...  Training loss: 1.8812...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3499...  Training loss: 1.9224...  0.1317 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3500...  Training loss: 1.8793...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3501...  Training loss: 1.8959...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3502...  Training loss: 1.9000...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3503...  Training loss: 1.9053...  0.1319 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3504...  Training loss: 1.8943...  0.1326 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3505...  Training loss: 1.8915...  0.1357 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3506...  Training loss: 1.8897...  0.1346 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3507...  Training loss: 1.8831...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3508...  Training loss: 1.8875...  0.1372 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3509...  Training loss: 1.8650...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3510...  Training loss: 1.8925...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3511...  Training loss: 1.8899...  0.1311 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3512...  Training loss: 1.8723...  0.1319 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3513...  Training loss: 1.8824...  0.1361 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3514...  Training loss: 1.8825...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3515...  Training loss: 1.8975...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3516...  Training loss: 1.8918...  0.1329 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3517...  Training loss: 1.8857...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3518...  Training loss: 1.8896...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3519...  Training loss: 1.8975...  0.1353 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3520...  Training loss: 1.8800...  0.1332 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3521...  Training loss: 1.8822...  0.1351 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3522...  Training loss: 1.8827...  0.1350 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3523...  Training loss: 1.8870...  0.1330 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3524...  Training loss: 1.8720...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3525...  Training loss: 1.8862...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3526...  Training loss: 1.8798...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3527...  Training loss: 1.8987...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3528...  Training loss: 1.8758...  0.1322 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3529...  Training loss: 1.8648...  0.1356 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3530...  Training loss: 1.9089...  0.1327 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3531...  Training loss: 1.8953...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3532...  Training loss: 1.8886...  0.1327 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3533...  Training loss: 1.9023...  0.1367 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3534...  Training loss: 1.8866...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3535...  Training loss: 1.8932...  0.1341 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3536...  Training loss: 1.9177...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3537...  Training loss: 1.9323...  0.1365 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3538...  Training loss: 1.9303...  0.1317 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3539...  Training loss: 1.9091...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3540...  Training loss: 1.9200...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3541...  Training loss: 1.9049...  0.1332 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3542...  Training loss: 1.9231...  0.1368 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3543...  Training loss: 1.9077...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3544...  Training loss: 1.8957...  0.1366 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3545...  Training loss: 1.8955...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3546...  Training loss: 1.9121...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3547...  Training loss: 1.8941...  0.1316 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3548...  Training loss: 1.8996...  0.1343 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3549...  Training loss: 1.9126...  0.1362 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3550...  Training loss: 1.8871...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3551...  Training loss: 1.9003...  0.1316 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3552...  Training loss: 1.9112...  0.1329 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3553...  Training loss: 1.9020...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3554...  Training loss: 1.9024...  0.1317 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3555...  Training loss: 1.9069...  0.1330 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3556...  Training loss: 1.8906...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3557...  Training loss: 1.8962...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3558...  Training loss: 1.8958...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3559...  Training loss: 1.9136...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3560...  Training loss: 1.9044...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3561...  Training loss: 1.8896...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3562...  Training loss: 1.9237...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3563...  Training loss: 1.9271...  0.1316 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3564...  Training loss: 1.8843...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3565...  Training loss: 1.8992...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3566...  Training loss: 1.8921...  0.1327 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3567...  Training loss: 1.9104...  0.1334 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3568...  Training loss: 1.8884...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3569...  Training loss: 1.8954...  0.1346 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3570...  Training loss: 1.8978...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3571...  Training loss: 1.9130...  0.1315 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3572...  Training loss: 1.8955...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3573...  Training loss: 1.9053...  0.1311 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3574...  Training loss: 1.9151...  0.1328 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3575...  Training loss: 1.8973...  0.1358 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3576...  Training loss: 1.8760...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3577...  Training loss: 1.9044...  0.1306 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3578...  Training loss: 1.8742...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3579...  Training loss: 1.8724...  0.1331 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3580...  Training loss: 1.9145...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3581...  Training loss: 1.8638...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3582...  Training loss: 1.8917...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3583...  Training loss: 1.8995...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3584...  Training loss: 1.8671...  0.1326 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/15...  Training Step: 3585...  Training loss: 1.8931...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3586...  Training loss: 1.8734...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3587...  Training loss: 1.8760...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3588...  Training loss: 1.8661...  0.1330 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3589...  Training loss: 1.8752...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3590...  Training loss: 1.9052...  0.1321 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3591...  Training loss: 1.9282...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3592...  Training loss: 1.8709...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3593...  Training loss: 1.8911...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3594...  Training loss: 1.9014...  0.1330 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3595...  Training loss: 1.8526...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3596...  Training loss: 1.8845...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3597...  Training loss: 1.8594...  0.1323 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3598...  Training loss: 1.8660...  0.1335 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3599...  Training loss: 1.8784...  0.1351 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3600...  Training loss: 1.8778...  0.1356 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3601...  Training loss: 1.8880...  0.1355 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3602...  Training loss: 1.8843...  0.1327 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3603...  Training loss: 1.8979...  0.1344 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3604...  Training loss: 1.8654...  0.1320 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3605...  Training loss: 1.8676...  0.1351 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3606...  Training loss: 1.8912...  0.1345 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3607...  Training loss: 1.8291...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3608...  Training loss: 1.8707...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3609...  Training loss: 1.8763...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3610...  Training loss: 1.8552...  0.1354 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3611...  Training loss: 1.8555...  0.1340 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3612...  Training loss: 1.8645...  0.1347 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3613...  Training loss: 1.8774...  0.1352 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3614...  Training loss: 1.8691...  0.1356 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3615...  Training loss: 1.8840...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3616...  Training loss: 1.8798...  0.1329 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3617...  Training loss: 1.8691...  0.1336 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3618...  Training loss: 1.8548...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3619...  Training loss: 1.8829...  0.1319 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3620...  Training loss: 1.9105...  0.1309 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3621...  Training loss: 1.8758...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3622...  Training loss: 1.8830...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3623...  Training loss: 1.9060...  0.1324 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3624...  Training loss: 1.9117...  0.1339 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3625...  Training loss: 1.9259...  0.1351 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3626...  Training loss: 1.9366...  0.1359 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3627...  Training loss: 1.9429...  0.1338 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3628...  Training loss: 1.8851...  0.1333 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3629...  Training loss: 1.9335...  0.1313 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3630...  Training loss: 1.8951...  0.1348 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3631...  Training loss: 1.8867...  0.1337 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3632...  Training loss: 1.8946...  0.1362 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3633...  Training loss: 1.8988...  0.1342 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3634...  Training loss: 1.8948...  0.1346 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3635...  Training loss: 1.8865...  0.1331 sec/batch\n",
      "Epoch: 12/15...  Training Step: 3636...  Training loss: 1.8809...  0.1370 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3637...  Training loss: 1.9706...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3638...  Training loss: 1.9143...  0.1359 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3639...  Training loss: 1.8838...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3640...  Training loss: 1.8861...  0.1358 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3641...  Training loss: 1.9006...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3642...  Training loss: 1.9261...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3643...  Training loss: 1.9010...  0.1331 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3644...  Training loss: 1.9034...  0.1302 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3645...  Training loss: 1.8608...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3646...  Training loss: 1.8674...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3647...  Training loss: 1.8573...  0.1354 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3648...  Training loss: 1.8772...  0.1304 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3649...  Training loss: 1.9025...  0.1332 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3650...  Training loss: 1.8899...  0.1349 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3651...  Training loss: 1.9059...  0.1366 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3652...  Training loss: 1.9085...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3653...  Training loss: 1.9308...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3654...  Training loss: 1.9048...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3655...  Training loss: 1.9189...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3656...  Training loss: 1.9266...  0.1323 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3657...  Training loss: 1.9033...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3658...  Training loss: 1.8948...  0.1316 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3659...  Training loss: 1.9095...  0.1346 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3660...  Training loss: 1.8980...  0.1318 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3661...  Training loss: 1.9327...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3662...  Training loss: 1.9214...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3663...  Training loss: 1.8831...  0.1328 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3664...  Training loss: 1.9129...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3665...  Training loss: 1.9336...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3666...  Training loss: 1.9075...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3667...  Training loss: 1.9028...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3668...  Training loss: 1.9004...  0.1322 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3669...  Training loss: 1.8884...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3670...  Training loss: 1.9023...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3671...  Training loss: 1.8898...  0.1309 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3672...  Training loss: 1.8763...  0.1316 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3673...  Training loss: 1.9023...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3674...  Training loss: 1.8761...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3675...  Training loss: 1.8839...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3676...  Training loss: 1.8995...  0.1317 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3677...  Training loss: 1.8963...  0.1366 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3678...  Training loss: 1.8881...  0.1358 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3679...  Training loss: 1.8972...  0.1323 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3680...  Training loss: 1.8720...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3681...  Training loss: 1.8955...  0.1350 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3682...  Training loss: 1.8754...  0.1327 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15...  Training Step: 3683...  Training loss: 1.8702...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3684...  Training loss: 1.8810...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3685...  Training loss: 1.8702...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3686...  Training loss: 1.8578...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3687...  Training loss: 1.8683...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3688...  Training loss: 1.8838...  0.1349 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3689...  Training loss: 1.8804...  0.1311 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3690...  Training loss: 1.8746...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3691...  Training loss: 1.8773...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3692...  Training loss: 1.8812...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3693...  Training loss: 1.8789...  0.1324 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3694...  Training loss: 1.8862...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3695...  Training loss: 1.8726...  0.1310 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3696...  Training loss: 1.8617...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3697...  Training loss: 1.8640...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3698...  Training loss: 1.8934...  0.1360 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3699...  Training loss: 1.8703...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3700...  Training loss: 1.8896...  0.1315 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3701...  Training loss: 1.8964...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3702...  Training loss: 1.9235...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3703...  Training loss: 1.9026...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3704...  Training loss: 1.8593...  0.1299 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3705...  Training loss: 1.8824...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3706...  Training loss: 1.8648...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3707...  Training loss: 1.9039...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3708...  Training loss: 1.8848...  0.1311 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3709...  Training loss: 1.8862...  0.1323 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3710...  Training loss: 1.8819...  0.1332 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3711...  Training loss: 1.8649...  0.1314 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3712...  Training loss: 1.8923...  0.1311 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3713...  Training loss: 1.9010...  0.1346 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3714...  Training loss: 1.9171...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3715...  Training loss: 1.9016...  0.1303 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3716...  Training loss: 1.8838...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3717...  Training loss: 1.9147...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3718...  Training loss: 1.9205...  0.1314 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3719...  Training loss: 1.8845...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3720...  Training loss: 1.8830...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3721...  Training loss: 1.8940...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3722...  Training loss: 1.8748...  0.1312 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3723...  Training loss: 1.8831...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3724...  Training loss: 1.8942...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3725...  Training loss: 1.8827...  0.1325 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3726...  Training loss: 1.8996...  0.1350 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3727...  Training loss: 1.8795...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3728...  Training loss: 1.8919...  0.1305 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3729...  Training loss: 1.8814...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3730...  Training loss: 1.8672...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3731...  Training loss: 1.8666...  0.1320 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3732...  Training loss: 1.8751...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3733...  Training loss: 1.8848...  0.1358 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3734...  Training loss: 1.8913...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3735...  Training loss: 1.8638...  0.1325 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3736...  Training loss: 1.8704...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3737...  Training loss: 1.8793...  0.1361 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3738...  Training loss: 1.8840...  0.1346 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3739...  Training loss: 1.8830...  0.1312 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3740...  Training loss: 1.8768...  0.1346 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3741...  Training loss: 1.8851...  0.1316 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3742...  Training loss: 1.8822...  0.1325 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3743...  Training loss: 1.8939...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3744...  Training loss: 1.8855...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3745...  Training loss: 1.9080...  0.1354 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3746...  Training loss: 1.9032...  0.1320 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3747...  Training loss: 1.9107...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3748...  Training loss: 1.8762...  0.1354 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3749...  Training loss: 1.8706...  0.1355 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3750...  Training loss: 1.8898...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3751...  Training loss: 1.9013...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3752...  Training loss: 1.8890...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3753...  Training loss: 1.8944...  0.1361 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3754...  Training loss: 1.8905...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3755...  Training loss: 1.9126...  0.1349 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3756...  Training loss: 1.8590...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3757...  Training loss: 1.8657...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3758...  Training loss: 1.8925...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3759...  Training loss: 1.8936...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3760...  Training loss: 1.8936...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3761...  Training loss: 1.9046...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3762...  Training loss: 1.9008...  0.1332 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3763...  Training loss: 1.9008...  0.1352 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3764...  Training loss: 1.9250...  0.1313 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3765...  Training loss: 1.9071...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3766...  Training loss: 1.9184...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3767...  Training loss: 1.9070...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3768...  Training loss: 1.9151...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3769...  Training loss: 1.9084...  0.1361 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3770...  Training loss: 1.9252...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3771...  Training loss: 1.9084...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3772...  Training loss: 1.8798...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3773...  Training loss: 1.9272...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3774...  Training loss: 1.9003...  0.1328 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3775...  Training loss: 1.8959...  0.1308 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3776...  Training loss: 1.8994...  0.1331 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3777...  Training loss: 1.8691...  0.1356 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3778...  Training loss: 1.8627...  0.1350 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3779...  Training loss: 1.8632...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3780...  Training loss: 1.8870...  0.1321 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15...  Training Step: 3781...  Training loss: 1.8631...  0.1332 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3782...  Training loss: 1.8980...  0.1332 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3783...  Training loss: 1.8898...  0.1360 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3784...  Training loss: 1.9069...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3785...  Training loss: 1.8952...  0.1350 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3786...  Training loss: 1.8735...  0.1367 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3787...  Training loss: 1.8995...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3788...  Training loss: 1.8955...  0.1357 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3789...  Training loss: 1.8892...  0.1315 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3790...  Training loss: 1.8978...  0.1350 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3791...  Training loss: 1.8843...  0.1355 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3792...  Training loss: 1.8956...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3793...  Training loss: 1.8840...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3794...  Training loss: 1.8905...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3795...  Training loss: 1.9047...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3796...  Training loss: 1.8931...  0.1360 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3797...  Training loss: 1.8627...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3798...  Training loss: 1.8771...  0.1358 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3799...  Training loss: 1.8747...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3800...  Training loss: 1.8831...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3801...  Training loss: 1.8672...  0.1349 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3802...  Training loss: 1.9152...  0.1356 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3803...  Training loss: 1.8717...  0.1326 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3804...  Training loss: 1.8943...  0.1366 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3805...  Training loss: 1.8829...  0.1317 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3806...  Training loss: 1.8919...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3807...  Training loss: 1.8771...  0.1363 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3808...  Training loss: 1.8818...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3809...  Training loss: 1.8829...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3810...  Training loss: 1.8831...  0.1359 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3811...  Training loss: 1.8710...  0.1315 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3812...  Training loss: 1.8600...  0.1324 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3813...  Training loss: 1.8839...  0.1370 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3814...  Training loss: 1.8790...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3815...  Training loss: 1.8701...  0.1313 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3816...  Training loss: 1.8770...  0.1310 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3817...  Training loss: 1.8778...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3818...  Training loss: 1.9014...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3819...  Training loss: 1.8819...  0.1357 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3820...  Training loss: 1.8701...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3821...  Training loss: 1.8808...  0.1364 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3822...  Training loss: 1.8813...  0.1362 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3823...  Training loss: 1.8714...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3824...  Training loss: 1.8812...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3825...  Training loss: 1.8746...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3826...  Training loss: 1.8788...  0.1314 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3827...  Training loss: 1.8620...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3828...  Training loss: 1.8700...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3829...  Training loss: 1.8699...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3830...  Training loss: 1.8881...  0.1354 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3831...  Training loss: 1.8603...  0.1357 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3832...  Training loss: 1.8574...  0.1303 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3833...  Training loss: 1.8935...  0.1382 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3834...  Training loss: 1.8860...  0.1314 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3835...  Training loss: 1.8801...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3836...  Training loss: 1.8911...  0.1361 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3837...  Training loss: 1.8740...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3838...  Training loss: 1.8896...  0.1307 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3839...  Training loss: 1.9003...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3840...  Training loss: 1.9191...  0.1318 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3841...  Training loss: 1.9244...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3842...  Training loss: 1.9002...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3843...  Training loss: 1.9101...  0.1339 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3844...  Training loss: 1.9061...  0.1309 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3845...  Training loss: 1.9108...  0.1356 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3846...  Training loss: 1.8982...  0.1320 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3847...  Training loss: 1.8866...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3848...  Training loss: 1.8932...  0.1319 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3849...  Training loss: 1.9044...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3850...  Training loss: 1.8843...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3851...  Training loss: 1.8902...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3852...  Training loss: 1.9071...  0.1325 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3853...  Training loss: 1.8876...  0.1303 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3854...  Training loss: 1.8955...  0.1346 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3855...  Training loss: 1.8976...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3856...  Training loss: 1.8942...  0.1356 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3857...  Training loss: 1.8924...  0.1355 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3858...  Training loss: 1.8875...  0.1324 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3859...  Training loss: 1.8896...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3860...  Training loss: 1.8822...  0.1318 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3861...  Training loss: 1.8942...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3862...  Training loss: 1.8963...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3863...  Training loss: 1.9004...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3864...  Training loss: 1.8832...  0.1318 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3865...  Training loss: 1.9070...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3866...  Training loss: 1.9175...  0.1322 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3867...  Training loss: 1.8797...  0.1317 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3868...  Training loss: 1.8969...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3869...  Training loss: 1.8876...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3870...  Training loss: 1.8990...  0.1289 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3871...  Training loss: 1.8848...  0.1329 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3872...  Training loss: 1.8803...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3873...  Training loss: 1.8890...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3874...  Training loss: 1.8994...  0.1354 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3875...  Training loss: 1.8893...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3876...  Training loss: 1.8937...  0.1321 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3877...  Training loss: 1.9066...  0.1333 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3878...  Training loss: 1.8931...  0.1344 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/15...  Training Step: 3879...  Training loss: 1.8687...  0.1299 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3880...  Training loss: 1.8972...  0.1362 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3881...  Training loss: 1.8726...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3882...  Training loss: 1.8593...  0.1308 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3883...  Training loss: 1.9028...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3884...  Training loss: 1.8573...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3885...  Training loss: 1.8878...  0.1324 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3886...  Training loss: 1.8931...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3887...  Training loss: 1.8648...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3888...  Training loss: 1.8813...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3889...  Training loss: 1.8760...  0.1337 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3890...  Training loss: 1.8539...  0.1314 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3891...  Training loss: 1.8646...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3892...  Training loss: 1.8611...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3893...  Training loss: 1.8910...  0.1356 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3894...  Training loss: 1.9053...  0.1323 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3895...  Training loss: 1.8701...  0.1317 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3896...  Training loss: 1.8907...  0.1348 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3897...  Training loss: 1.8982...  0.1353 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3898...  Training loss: 1.8431...  0.1363 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3899...  Training loss: 1.8769...  0.1328 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3900...  Training loss: 1.8627...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3901...  Training loss: 1.8561...  0.1336 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3902...  Training loss: 1.8719...  0.1306 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3903...  Training loss: 1.8648...  0.1342 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3904...  Training loss: 1.8735...  0.1303 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3905...  Training loss: 1.8802...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3906...  Training loss: 1.8986...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3907...  Training loss: 1.8518...  0.1343 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3908...  Training loss: 1.8519...  0.1330 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3909...  Training loss: 1.8849...  0.1359 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3910...  Training loss: 1.8247...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3911...  Training loss: 1.8726...  0.1320 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3912...  Training loss: 1.8797...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3913...  Training loss: 1.8420...  0.1309 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3914...  Training loss: 1.8453...  0.1320 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3915...  Training loss: 1.8614...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3916...  Training loss: 1.8646...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3917...  Training loss: 1.8658...  0.1367 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3918...  Training loss: 1.8722...  0.1322 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3919...  Training loss: 1.8720...  0.1335 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3920...  Training loss: 1.8688...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3921...  Training loss: 1.8400...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3922...  Training loss: 1.8732...  0.1340 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3923...  Training loss: 1.8961...  0.1316 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3924...  Training loss: 1.8634...  0.1345 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3925...  Training loss: 1.8746...  0.1334 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3926...  Training loss: 1.8979...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3927...  Training loss: 1.9041...  0.1338 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3928...  Training loss: 1.9196...  0.1323 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3929...  Training loss: 1.9313...  0.1307 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3930...  Training loss: 1.9287...  0.1363 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3931...  Training loss: 1.8806...  0.1347 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3932...  Training loss: 1.9270...  0.1362 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3933...  Training loss: 1.8839...  0.1351 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3934...  Training loss: 1.8841...  0.1341 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3935...  Training loss: 1.8899...  0.1327 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3936...  Training loss: 1.8958...  0.1344 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3937...  Training loss: 1.8810...  0.1307 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3938...  Training loss: 1.8791...  0.1324 sec/batch\n",
      "Epoch: 13/15...  Training Step: 3939...  Training loss: 1.8806...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3940...  Training loss: 1.9628...  0.1318 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3941...  Training loss: 1.9063...  0.1305 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3942...  Training loss: 1.8795...  0.1338 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3943...  Training loss: 1.8833...  0.1368 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3944...  Training loss: 1.8925...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3945...  Training loss: 1.9175...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3946...  Training loss: 1.8937...  0.1363 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3947...  Training loss: 1.9001...  0.1349 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3948...  Training loss: 1.8522...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3949...  Training loss: 1.8637...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3950...  Training loss: 1.8548...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3951...  Training loss: 1.8695...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3952...  Training loss: 1.8918...  0.1330 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3953...  Training loss: 1.8884...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3954...  Training loss: 1.8977...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3955...  Training loss: 1.9075...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3956...  Training loss: 1.9173...  0.1354 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3957...  Training loss: 1.9031...  0.1325 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3958...  Training loss: 1.9189...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3959...  Training loss: 1.9245...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3960...  Training loss: 1.8975...  0.1305 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3961...  Training loss: 1.8883...  0.1325 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3962...  Training loss: 1.8979...  0.1358 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3963...  Training loss: 1.8869...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3964...  Training loss: 1.9225...  0.1332 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3965...  Training loss: 1.9035...  0.1360 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3966...  Training loss: 1.8751...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3967...  Training loss: 1.8947...  0.1311 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3968...  Training loss: 1.9184...  0.1312 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3969...  Training loss: 1.8953...  0.1345 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3970...  Training loss: 1.8928...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3971...  Training loss: 1.8968...  0.1326 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3972...  Training loss: 1.8814...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3973...  Training loss: 1.8958...  0.1327 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3974...  Training loss: 1.8736...  0.1330 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3975...  Training loss: 1.8761...  0.1348 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3976...  Training loss: 1.8970...  0.1346 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15...  Training Step: 3977...  Training loss: 1.8675...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3978...  Training loss: 1.8818...  0.1338 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3979...  Training loss: 1.8924...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3980...  Training loss: 1.8852...  0.1338 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3981...  Training loss: 1.8773...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3982...  Training loss: 1.8937...  0.1314 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3983...  Training loss: 1.8610...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3984...  Training loss: 1.8828...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3985...  Training loss: 1.8697...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3986...  Training loss: 1.8646...  0.1320 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3987...  Training loss: 1.8736...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3988...  Training loss: 1.8575...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3989...  Training loss: 1.8477...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3990...  Training loss: 1.8580...  0.1330 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3991...  Training loss: 1.8679...  0.1358 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3992...  Training loss: 1.8724...  0.1306 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3993...  Training loss: 1.8633...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3994...  Training loss: 1.8721...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3995...  Training loss: 1.8653...  0.1320 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3996...  Training loss: 1.8563...  0.1359 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3997...  Training loss: 1.8761...  0.1308 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3998...  Training loss: 1.8575...  0.1324 sec/batch\n",
      "Epoch: 14/15...  Training Step: 3999...  Training loss: 1.8593...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4000...  Training loss: 1.8564...  0.1309 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4001...  Training loss: 1.8889...  0.1344 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4002...  Training loss: 1.8597...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4003...  Training loss: 1.8838...  0.1306 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4004...  Training loss: 1.8944...  0.1368 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4005...  Training loss: 1.9123...  0.1371 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4006...  Training loss: 1.8903...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4007...  Training loss: 1.8513...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4008...  Training loss: 1.8754...  0.1352 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4009...  Training loss: 1.8632...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4010...  Training loss: 1.8936...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4011...  Training loss: 1.8748...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4012...  Training loss: 1.8802...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4013...  Training loss: 1.8761...  0.1354 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4014...  Training loss: 1.8562...  0.1317 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4015...  Training loss: 1.8792...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4016...  Training loss: 1.8914...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4017...  Training loss: 1.9094...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4018...  Training loss: 1.8932...  0.1324 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4019...  Training loss: 1.8842...  0.1322 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4020...  Training loss: 1.9100...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4021...  Training loss: 1.9016...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4022...  Training loss: 1.8846...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4023...  Training loss: 1.8803...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4024...  Training loss: 1.8837...  0.1359 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4025...  Training loss: 1.8558...  0.1317 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4026...  Training loss: 1.8745...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4027...  Training loss: 1.8795...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4028...  Training loss: 1.8732...  0.1363 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4029...  Training loss: 1.8885...  0.1330 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4030...  Training loss: 1.8582...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4031...  Training loss: 1.8871...  0.1351 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4032...  Training loss: 1.8778...  0.1311 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4033...  Training loss: 1.8535...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4034...  Training loss: 1.8573...  0.1349 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4035...  Training loss: 1.8707...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4036...  Training loss: 1.8827...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4037...  Training loss: 1.8802...  0.1363 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4038...  Training loss: 1.8493...  0.1361 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4039...  Training loss: 1.8659...  0.1430 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4040...  Training loss: 1.8723...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4041...  Training loss: 1.8714...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4042...  Training loss: 1.8770...  0.1310 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4043...  Training loss: 1.8558...  0.1322 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4044...  Training loss: 1.8766...  0.1327 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4045...  Training loss: 1.8794...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4046...  Training loss: 1.8800...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4047...  Training loss: 1.8760...  0.1320 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4048...  Training loss: 1.8985...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4049...  Training loss: 1.8977...  0.1315 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4050...  Training loss: 1.8972...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4051...  Training loss: 1.8663...  0.1318 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4052...  Training loss: 1.8549...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4053...  Training loss: 1.8888...  0.1359 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4054...  Training loss: 1.8983...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4055...  Training loss: 1.8792...  0.1326 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4056...  Training loss: 1.8771...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4057...  Training loss: 1.8787...  0.1326 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4058...  Training loss: 1.9024...  0.1314 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4059...  Training loss: 1.8595...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4060...  Training loss: 1.8599...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4061...  Training loss: 1.8823...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4062...  Training loss: 1.8776...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4063...  Training loss: 1.8827...  0.1351 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4064...  Training loss: 1.8936...  0.1347 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4065...  Training loss: 1.8934...  0.1319 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4066...  Training loss: 1.8948...  0.1348 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4067...  Training loss: 1.9195...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4068...  Training loss: 1.9000...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4069...  Training loss: 1.9133...  0.1349 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4070...  Training loss: 1.9035...  0.1304 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4071...  Training loss: 1.9043...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4072...  Training loss: 1.9037...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4073...  Training loss: 1.9120...  0.1349 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4074...  Training loss: 1.8992...  0.1362 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15...  Training Step: 4075...  Training loss: 1.8726...  0.1364 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4076...  Training loss: 1.9153...  0.1329 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4077...  Training loss: 1.8882...  0.1354 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4078...  Training loss: 1.8915...  0.1365 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4079...  Training loss: 1.8797...  0.1375 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4080...  Training loss: 1.8593...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4081...  Training loss: 1.8489...  0.1345 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4082...  Training loss: 1.8591...  0.1372 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4083...  Training loss: 1.8755...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4084...  Training loss: 1.8533...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4085...  Training loss: 1.8949...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4086...  Training loss: 1.8865...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4087...  Training loss: 1.8983...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4088...  Training loss: 1.8872...  0.1355 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4089...  Training loss: 1.8602...  0.1351 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4090...  Training loss: 1.8855...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4091...  Training loss: 1.8851...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4092...  Training loss: 1.8827...  0.1357 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4093...  Training loss: 1.9007...  0.1344 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4094...  Training loss: 1.8683...  0.1369 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4095...  Training loss: 1.8958...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4096...  Training loss: 1.8809...  0.1338 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4097...  Training loss: 1.8797...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4098...  Training loss: 1.8926...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4099...  Training loss: 1.8818...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4100...  Training loss: 1.8598...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4101...  Training loss: 1.8633...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4102...  Training loss: 1.8673...  0.1317 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4103...  Training loss: 1.8780...  0.1312 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4104...  Training loss: 1.8599...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4105...  Training loss: 1.8979...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4106...  Training loss: 1.8642...  0.1357 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4107...  Training loss: 1.8773...  0.1348 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4108...  Training loss: 1.8859...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4109...  Training loss: 1.8771...  0.1319 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4110...  Training loss: 1.8723...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4111...  Training loss: 1.8736...  0.1362 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4112...  Training loss: 1.8753...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4113...  Training loss: 1.8675...  0.1360 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4114...  Training loss: 1.8704...  0.1360 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4115...  Training loss: 1.8554...  0.1345 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4116...  Training loss: 1.8776...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4117...  Training loss: 1.8707...  0.1327 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4118...  Training loss: 1.8490...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4119...  Training loss: 1.8615...  0.1322 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4120...  Training loss: 1.8800...  0.1364 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4121...  Training loss: 1.8850...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4122...  Training loss: 1.8720...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4123...  Training loss: 1.8615...  0.1338 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4124...  Training loss: 1.8818...  0.1347 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4125...  Training loss: 1.8805...  0.1323 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4126...  Training loss: 1.8590...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4127...  Training loss: 1.8626...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4128...  Training loss: 1.8739...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4129...  Training loss: 1.8750...  0.1359 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4130...  Training loss: 1.8553...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4131...  Training loss: 1.8610...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4132...  Training loss: 1.8650...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4133...  Training loss: 1.8757...  0.1312 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4134...  Training loss: 1.8534...  0.1345 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4135...  Training loss: 1.8523...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4136...  Training loss: 1.8897...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4137...  Training loss: 1.8750...  0.1332 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4138...  Training loss: 1.8716...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4139...  Training loss: 1.8896...  0.1309 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4140...  Training loss: 1.8743...  0.1303 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4141...  Training loss: 1.8789...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4142...  Training loss: 1.8964...  0.1318 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4143...  Training loss: 1.9201...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4144...  Training loss: 1.9126...  0.1317 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4145...  Training loss: 1.8928...  0.1365 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4146...  Training loss: 1.8989...  0.1312 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4147...  Training loss: 1.8798...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4148...  Training loss: 1.9007...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4149...  Training loss: 1.8843...  0.1356 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4150...  Training loss: 1.8712...  0.1320 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4151...  Training loss: 1.8854...  0.1327 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4152...  Training loss: 1.8936...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4153...  Training loss: 1.8837...  0.1358 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4154...  Training loss: 1.8826...  0.1352 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4155...  Training loss: 1.8922...  0.1348 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4156...  Training loss: 1.8696...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4157...  Training loss: 1.8833...  0.1313 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4158...  Training loss: 1.8853...  0.1318 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4159...  Training loss: 1.8836...  0.1311 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4160...  Training loss: 1.8736...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4161...  Training loss: 1.8882...  0.1332 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4162...  Training loss: 1.8658...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4163...  Training loss: 1.8777...  0.1305 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4164...  Training loss: 1.8853...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4165...  Training loss: 1.8822...  0.1332 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4166...  Training loss: 1.8923...  0.1327 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4167...  Training loss: 1.8748...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4168...  Training loss: 1.9046...  0.1317 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4169...  Training loss: 1.9113...  0.1306 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4170...  Training loss: 1.8736...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4171...  Training loss: 1.8833...  0.1316 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4172...  Training loss: 1.8771...  0.1356 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/15...  Training Step: 4173...  Training loss: 1.8951...  0.1319 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4174...  Training loss: 1.8731...  0.1344 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4175...  Training loss: 1.8718...  0.1347 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4176...  Training loss: 1.8927...  0.1344 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4177...  Training loss: 1.8897...  0.1356 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4178...  Training loss: 1.8802...  0.1351 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4179...  Training loss: 1.8926...  0.1320 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4180...  Training loss: 1.8935...  0.1324 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4181...  Training loss: 1.8750...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4182...  Training loss: 1.8564...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4183...  Training loss: 1.8848...  0.1335 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4184...  Training loss: 1.8691...  0.1307 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4185...  Training loss: 1.8555...  0.1312 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4186...  Training loss: 1.8943...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4187...  Training loss: 1.8488...  0.1322 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4188...  Training loss: 1.8828...  0.1360 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4189...  Training loss: 1.8903...  0.1355 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4190...  Training loss: 1.8575...  0.1352 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4191...  Training loss: 1.8711...  0.1343 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4192...  Training loss: 1.8637...  0.1328 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4193...  Training loss: 1.8613...  0.1348 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4194...  Training loss: 1.8534...  0.1359 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4195...  Training loss: 1.8705...  0.1364 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4196...  Training loss: 1.8903...  0.1329 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4197...  Training loss: 1.8954...  0.1329 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4198...  Training loss: 1.8538...  0.1344 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4199...  Training loss: 1.8795...  0.1299 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4200...  Training loss: 1.8900...  0.1314 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4201...  Training loss: 1.8403...  0.1341 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4202...  Training loss: 1.8658...  0.1326 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4203...  Training loss: 1.8443...  0.1337 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4204...  Training loss: 1.8546...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4205...  Training loss: 1.8657...  0.1306 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4206...  Training loss: 1.8541...  0.1308 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4207...  Training loss: 1.8633...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4208...  Training loss: 1.8623...  0.1356 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4209...  Training loss: 1.8835...  0.1329 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4210...  Training loss: 1.8447...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4211...  Training loss: 1.8575...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4212...  Training loss: 1.8694...  0.1340 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4213...  Training loss: 1.8136...  0.1362 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4214...  Training loss: 1.8561...  0.1358 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4215...  Training loss: 1.8662...  0.1353 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4216...  Training loss: 1.8386...  0.1315 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4217...  Training loss: 1.8351...  0.1356 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4218...  Training loss: 1.8503...  0.1339 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4219...  Training loss: 1.8558...  0.1347 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4220...  Training loss: 1.8553...  0.1323 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4221...  Training loss: 1.8558...  0.1333 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4222...  Training loss: 1.8674...  0.1334 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4223...  Training loss: 1.8501...  0.1294 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4224...  Training loss: 1.8398...  0.1329 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4225...  Training loss: 1.8687...  0.1324 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4226...  Training loss: 1.8866...  0.1322 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4227...  Training loss: 1.8582...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4228...  Training loss: 1.8599...  0.1324 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4229...  Training loss: 1.8867...  0.1384 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4230...  Training loss: 1.8943...  0.1303 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4231...  Training loss: 1.9089...  0.1347 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4232...  Training loss: 1.9201...  0.1342 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4233...  Training loss: 1.9187...  0.1321 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4234...  Training loss: 1.8793...  0.1331 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4235...  Training loss: 1.9070...  0.1349 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4236...  Training loss: 1.8759...  0.1362 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4237...  Training loss: 1.8706...  0.1350 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4238...  Training loss: 1.8781...  0.1301 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4239...  Training loss: 1.8795...  0.1346 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4240...  Training loss: 1.8786...  0.1336 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4241...  Training loss: 1.8655...  0.1361 sec/batch\n",
      "Epoch: 14/15...  Training Step: 4242...  Training loss: 1.8737...  0.1360 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4243...  Training loss: 1.9557...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4244...  Training loss: 1.9032...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4245...  Training loss: 1.8635...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4246...  Training loss: 1.8797...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4247...  Training loss: 1.8936...  0.1317 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4248...  Training loss: 1.9088...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4249...  Training loss: 1.8936...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4250...  Training loss: 1.8899...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4251...  Training loss: 1.8427...  0.1355 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4252...  Training loss: 1.8553...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4253...  Training loss: 1.8427...  0.1299 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4254...  Training loss: 1.8648...  0.1354 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4255...  Training loss: 1.8751...  0.1328 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4256...  Training loss: 1.8790...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4257...  Training loss: 1.8828...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4258...  Training loss: 1.8890...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4259...  Training loss: 1.9100...  0.1310 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4260...  Training loss: 1.8893...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4261...  Training loss: 1.9064...  0.1365 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4262...  Training loss: 1.9104...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4263...  Training loss: 1.8868...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4264...  Training loss: 1.8767...  0.1317 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4265...  Training loss: 1.8877...  0.1367 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4266...  Training loss: 1.8740...  0.1304 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4267...  Training loss: 1.9105...  0.1312 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4268...  Training loss: 1.9066...  0.1335 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4269...  Training loss: 1.8704...  0.1359 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4270...  Training loss: 1.8897...  0.1326 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15...  Training Step: 4271...  Training loss: 1.9118...  0.1326 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4272...  Training loss: 1.8797...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4273...  Training loss: 1.8890...  0.1323 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4274...  Training loss: 1.8863...  0.1322 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4275...  Training loss: 1.8797...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4276...  Training loss: 1.8830...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4277...  Training loss: 1.8719...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4278...  Training loss: 1.8611...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4279...  Training loss: 1.8888...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4280...  Training loss: 1.8564...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4281...  Training loss: 1.8779...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4282...  Training loss: 1.8784...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4283...  Training loss: 1.8643...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4284...  Training loss: 1.8709...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4285...  Training loss: 1.8838...  0.1314 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4286...  Training loss: 1.8540...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4287...  Training loss: 1.8685...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4288...  Training loss: 1.8568...  0.1352 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4289...  Training loss: 1.8489...  0.1318 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4290...  Training loss: 1.8506...  0.1334 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4291...  Training loss: 1.8559...  0.1328 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4292...  Training loss: 1.8429...  0.1326 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4293...  Training loss: 1.8537...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4294...  Training loss: 1.8642...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4295...  Training loss: 1.8613...  0.1352 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4296...  Training loss: 1.8578...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4297...  Training loss: 1.8627...  0.1361 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4298...  Training loss: 1.8570...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4299...  Training loss: 1.8482...  0.1352 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4300...  Training loss: 1.8652...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4301...  Training loss: 1.8507...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4302...  Training loss: 1.8431...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4303...  Training loss: 1.8523...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4304...  Training loss: 1.8787...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4305...  Training loss: 1.8568...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4306...  Training loss: 1.8629...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4307...  Training loss: 1.8832...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4308...  Training loss: 1.9023...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4309...  Training loss: 1.8817...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4310...  Training loss: 1.8473...  0.1330 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4311...  Training loss: 1.8690...  0.1320 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4312...  Training loss: 1.8547...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4313...  Training loss: 1.8801...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4314...  Training loss: 1.8672...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4315...  Training loss: 1.8679...  0.1349 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4316...  Training loss: 1.8676...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4317...  Training loss: 1.8509...  0.1314 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4318...  Training loss: 1.8762...  0.1312 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4319...  Training loss: 1.8920...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4320...  Training loss: 1.9002...  0.1313 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4321...  Training loss: 1.8859...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4322...  Training loss: 1.8810...  0.1325 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4323...  Training loss: 1.8996...  0.1332 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4324...  Training loss: 1.9061...  0.1307 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4325...  Training loss: 1.8772...  0.1342 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4326...  Training loss: 1.8703...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4327...  Training loss: 1.8780...  0.1324 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4328...  Training loss: 1.8548...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4329...  Training loss: 1.8689...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4330...  Training loss: 1.8756...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4331...  Training loss: 1.8636...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4332...  Training loss: 1.8850...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4333...  Training loss: 1.8494...  0.1335 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4334...  Training loss: 1.8807...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4335...  Training loss: 1.8654...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4336...  Training loss: 1.8522...  0.1362 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4337...  Training loss: 1.8523...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4338...  Training loss: 1.8583...  0.1315 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4339...  Training loss: 1.8798...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4340...  Training loss: 1.8747...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4341...  Training loss: 1.8474...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4342...  Training loss: 1.8527...  0.1340 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4343...  Training loss: 1.8682...  0.1351 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4344...  Training loss: 1.8709...  0.1316 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4345...  Training loss: 1.8661...  0.1316 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4346...  Training loss: 1.8588...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4347...  Training loss: 1.8764...  0.1302 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4348...  Training loss: 1.8687...  0.1320 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4349...  Training loss: 1.8764...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4350...  Training loss: 1.8715...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4351...  Training loss: 1.8923...  0.1343 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4352...  Training loss: 1.8925...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4353...  Training loss: 1.8972...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4354...  Training loss: 1.8580...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4355...  Training loss: 1.8493...  0.1342 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4356...  Training loss: 1.8698...  0.1298 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4357...  Training loss: 1.8873...  0.1343 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4358...  Training loss: 1.8712...  0.1360 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4359...  Training loss: 1.8819...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4360...  Training loss: 1.8748...  0.1331 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4361...  Training loss: 1.8934...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4362...  Training loss: 1.8448...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4363...  Training loss: 1.8498...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4364...  Training loss: 1.8797...  0.1342 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4365...  Training loss: 1.8840...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4366...  Training loss: 1.8766...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4367...  Training loss: 1.8856...  0.1326 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4368...  Training loss: 1.8893...  0.1357 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15...  Training Step: 4369...  Training loss: 1.8830...  0.1303 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4370...  Training loss: 1.9132...  0.1330 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4371...  Training loss: 1.8883...  0.1335 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4372...  Training loss: 1.9092...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4373...  Training loss: 1.8948...  0.1367 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4374...  Training loss: 1.8918...  0.1323 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4375...  Training loss: 1.8896...  0.1359 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4376...  Training loss: 1.9138...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4377...  Training loss: 1.8921...  0.1322 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4378...  Training loss: 1.8591...  0.1320 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4379...  Training loss: 1.9070...  0.1312 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4380...  Training loss: 1.8769...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4381...  Training loss: 1.8791...  0.1362 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4382...  Training loss: 1.8739...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4383...  Training loss: 1.8603...  0.1315 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4384...  Training loss: 1.8466...  0.1359 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4385...  Training loss: 1.8483...  0.1330 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4386...  Training loss: 1.8666...  0.1323 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4387...  Training loss: 1.8369...  0.1362 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4388...  Training loss: 1.8786...  0.1315 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4389...  Training loss: 1.8701...  0.1355 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4390...  Training loss: 1.8873...  0.1369 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4391...  Training loss: 1.8873...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4392...  Training loss: 1.8510...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4393...  Training loss: 1.8835...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4394...  Training loss: 1.8791...  0.1303 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4395...  Training loss: 1.8748...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4396...  Training loss: 1.8876...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4397...  Training loss: 1.8600...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4398...  Training loss: 1.8846...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4399...  Training loss: 1.8685...  0.1342 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4400...  Training loss: 1.8620...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4401...  Training loss: 1.8819...  0.1316 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4402...  Training loss: 1.8710...  0.1318 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4403...  Training loss: 1.8462...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4404...  Training loss: 1.8560...  0.1292 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4405...  Training loss: 1.8513...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4406...  Training loss: 1.8760...  0.1335 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4407...  Training loss: 1.8553...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4408...  Training loss: 1.8949...  0.1297 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4409...  Training loss: 1.8552...  0.1351 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4410...  Training loss: 1.8731...  0.1332 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4411...  Training loss: 1.8739...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4412...  Training loss: 1.8778...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4413...  Training loss: 1.8639...  0.1355 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4414...  Training loss: 1.8592...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4415...  Training loss: 1.8686...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4416...  Training loss: 1.8601...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4417...  Training loss: 1.8565...  0.1354 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4418...  Training loss: 1.8478...  0.1363 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4419...  Training loss: 1.8685...  0.1351 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4420...  Training loss: 1.8591...  0.1313 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4421...  Training loss: 1.8520...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4422...  Training loss: 1.8584...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4423...  Training loss: 1.8618...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4424...  Training loss: 1.8715...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4425...  Training loss: 1.8634...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4426...  Training loss: 1.8554...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4427...  Training loss: 1.8647...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4428...  Training loss: 1.8671...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4429...  Training loss: 1.8576...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4430...  Training loss: 1.8618...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4431...  Training loss: 1.8602...  0.1364 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4432...  Training loss: 1.8607...  0.1349 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4433...  Training loss: 1.8471...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4434...  Training loss: 1.8546...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4435...  Training loss: 1.8602...  0.1309 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4436...  Training loss: 1.8777...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4437...  Training loss: 1.8472...  0.1358 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4438...  Training loss: 1.8428...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4439...  Training loss: 1.8840...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4440...  Training loss: 1.8650...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4441...  Training loss: 1.8646...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4442...  Training loss: 1.8777...  0.1302 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4443...  Training loss: 1.8705...  0.1354 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4444...  Training loss: 1.8629...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4445...  Training loss: 1.8959...  0.1334 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4446...  Training loss: 1.9080...  0.1299 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4447...  Training loss: 1.9066...  0.1322 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4448...  Training loss: 1.8824...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4449...  Training loss: 1.8934...  0.1340 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4450...  Training loss: 1.8829...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4451...  Training loss: 1.8920...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4452...  Training loss: 1.8824...  0.1354 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4453...  Training loss: 1.8707...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4454...  Training loss: 1.8752...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4455...  Training loss: 1.8942...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4456...  Training loss: 1.8726...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4457...  Training loss: 1.8849...  0.1362 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4458...  Training loss: 1.8888...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4459...  Training loss: 1.8622...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4460...  Training loss: 1.8804...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4461...  Training loss: 1.8871...  0.1336 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4462...  Training loss: 1.8782...  0.1366 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4463...  Training loss: 1.8628...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4464...  Training loss: 1.8731...  0.1320 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4465...  Training loss: 1.8633...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4466...  Training loss: 1.8715...  0.1306 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/15...  Training Step: 4467...  Training loss: 1.8740...  0.1302 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4468...  Training loss: 1.8872...  0.1349 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4469...  Training loss: 1.8764...  0.1302 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4470...  Training loss: 1.8720...  0.1344 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4471...  Training loss: 1.8923...  0.1359 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4472...  Training loss: 1.8982...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4473...  Training loss: 1.8585...  0.1331 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4474...  Training loss: 1.8733...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4475...  Training loss: 1.8654...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4476...  Training loss: 1.8868...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4477...  Training loss: 1.8674...  0.1319 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4478...  Training loss: 1.8645...  0.1353 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4479...  Training loss: 1.8815...  0.1334 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4480...  Training loss: 1.8821...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4481...  Training loss: 1.8715...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4482...  Training loss: 1.8851...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4483...  Training loss: 1.8903...  0.1334 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4484...  Training loss: 1.8731...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4485...  Training loss: 1.8519...  0.1320 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4486...  Training loss: 1.8821...  0.1329 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4487...  Training loss: 1.8596...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4488...  Training loss: 1.8482...  0.1350 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4489...  Training loss: 1.8924...  0.1326 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4490...  Training loss: 1.8447...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4491...  Training loss: 1.8714...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4492...  Training loss: 1.8743...  0.1309 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4493...  Training loss: 1.8463...  0.1351 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4494...  Training loss: 1.8656...  0.1356 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4495...  Training loss: 1.8503...  0.1352 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4496...  Training loss: 1.8494...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4497...  Training loss: 1.8448...  0.1360 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4498...  Training loss: 1.8622...  0.1323 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4499...  Training loss: 1.8735...  0.1325 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4500...  Training loss: 1.8962...  0.1324 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4501...  Training loss: 1.8560...  0.1349 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4502...  Training loss: 1.8705...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4503...  Training loss: 1.8754...  0.1361 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4504...  Training loss: 1.8270...  0.1310 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4505...  Training loss: 1.8623...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4506...  Training loss: 1.8420...  0.1349 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4507...  Training loss: 1.8480...  0.1337 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4508...  Training loss: 1.8580...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4509...  Training loss: 1.8439...  0.1345 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4510...  Training loss: 1.8519...  0.1355 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4511...  Training loss: 1.8585...  0.1325 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4512...  Training loss: 1.8740...  0.1355 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4513...  Training loss: 1.8391...  0.1305 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4514...  Training loss: 1.8454...  0.1335 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4515...  Training loss: 1.8635...  0.1311 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4516...  Training loss: 1.8132...  0.1327 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4517...  Training loss: 1.8521...  0.1348 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4518...  Training loss: 1.8629...  0.1340 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4519...  Training loss: 1.8305...  0.1333 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4520...  Training loss: 1.8261...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4521...  Training loss: 1.8414...  0.1347 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4522...  Training loss: 1.8553...  0.1317 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4523...  Training loss: 1.8452...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4524...  Training loss: 1.8503...  0.1328 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4525...  Training loss: 1.8571...  0.1340 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4526...  Training loss: 1.8390...  0.1341 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4527...  Training loss: 1.8304...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4528...  Training loss: 1.8601...  0.1361 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4529...  Training loss: 1.8746...  0.1363 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4530...  Training loss: 1.8424...  0.1331 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4531...  Training loss: 1.8622...  0.1321 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4532...  Training loss: 1.8785...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4533...  Training loss: 1.8843...  0.1331 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4534...  Training loss: 1.9051...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4535...  Training loss: 1.9067...  0.1346 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4536...  Training loss: 1.9200...  0.1359 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4537...  Training loss: 1.8685...  0.1357 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4538...  Training loss: 1.9002...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4539...  Training loss: 1.8658...  0.1314 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4540...  Training loss: 1.8749...  0.1338 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4541...  Training loss: 1.8741...  0.1324 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4542...  Training loss: 1.8795...  0.1305 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4543...  Training loss: 1.8774...  0.1319 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4544...  Training loss: 1.8612...  0.1339 sec/batch\n",
      "Epoch: 15/15...  Training Step: 4545...  Training loss: 1.8664...  0.1288 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkpoint file will save the trained model into a checkpoint file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i4545_l128.ckpt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faring in otherly. I advone to the the to become is and that's both. \n",
      "\n",
      "Work the sometimes our compentate it's a making, but I also subreddit is a mearer is chatalities.\"\n",
      "\"Thanking around that is so stand and would story the point and they seem instant talk incorses are than the percester is a poot and showed.\"\n",
      "\"It all the meaning as the most to a corlers in a roliconin a play a second or there and we are seans a contral way, as a crice as there to a busing is an and is stayel a price watch.\"\n",
      "\"Well was sumpout the peaser as with the tall only seem to said and in this on myself to aller all it the plantically antiman in the car a past and the sort is netter, and there was problems on the pretty and is stopped a steal and a most is a company.\n",
      "\n",
      "Was tern outens to have to can soled of around to that solutions, this. \n",
      "\"\n",
      "\"It were a count that also then and to said the too too took. I criced to consint that that are seen to the time and are still say it sublest someone, the partass of make out with the make over the constills, but this actually there self instant that were a per thing that and a performs to trate any times. I was a course and that and are trualed and the time in the consinand worther of the completion than it's being sere to them as solinitied to be perfect as there as sell so a marned with the sence was soming with anyone isn't a strate time of it to supes with their antolates, between was try and should comically arat wearly, their can would be the talter are asdederss.\n",
      "\n",
      "Your sourt a laint or tarkers with a pact to crise.  They she can be thing in an it and without always this the thing of how anything also a preferertally.  \n",
      "\n",
      "If you're always the thrar thing that. The crimes take that way a level. The prose is some time. I are a parts is the compoterry on it and this took that she did we want to actond on the proper in this and words are asked out to control the trive to an assanders its are service to back in the point the stars that are the people take any\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fary is a posted. The way is so serial are it and the crict things.\n",
      "\n",
      "I are when I'm sort there or a class to comparers and assect that the poor.  I'm seen the parks, and season is to stop with the topically that the comment times of a since, then the plates the makes is so the tonal tarleries any poon they aren't couldn't have an everyone if it are an answer at any thing.\n",
      "\n",
      "In a consing of the provine as a pollity in about the sort, but you had to completed a bot indeem and is assect and what you still be so should care to crost to chock a long of information titles and anyone in any something are are.   I as we start works out it the same can arancist with a pressing to be as wanting about any contring and we are a camplar at mise intermant issue and suck that and wat is already saying to the at of the consult that this was a person through what you'd be playing they was talking of the marking of an issue would be trurtange in a sen on that.\n",
      "\n",
      "In me was police to and who could support, the \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i4545_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fary.\n",
      "\n",
      "And the took, I have been strength in to make on a match work in a point a barden other and there wasn't the comment was.  I don't. Also the seant in the percension was thinked to all it, and that this warranning to a point and in the same compect and sterears isn't subreddit women such on those there to an to the sound though if you completely some are an one. That's no solating a castary working to start. It subreddit. The waining on the same to try in the time which is around this was pateded are assume of the sound of it in a cars on a clessions. \n",
      "\n",
      "I have no tanters of a chertational.\"\" \n",
      "\"\n",
      "\"It's tash is the prices in an insure of the second as to to compense into anything in the say a processive is asked. I can't could a cricer inclused araline their sering, because their through wouldn't charable out arans of this tand there of the sarical. \n",
      "\n",
      "That's all some there to think to the chense on all that this to straight that the prampting it.  \n",
      "\n",
      "Also if you crangion ats a poor with \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i4545_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fars taken and shit the thing all it's all that were to check the track at a things. It taken it as anything and we ack if the trardening as to some place the save and to be an this thing that there are perfect over other of the clas and all that work of as the prisaration want a level. This story it that's strikge and their anternalitin on other streallys it isn stop the marn indesical or is it to be ward of in modest tarting on my to trast and a prove to be allowing, but I'm thread to be someone someto a part and a care tha laine armer of the completely sayed.  If it's betually a sideline of aranching on the past that watch any still sure a been that is that's people. I'm sure it's a completely in the thing well on it are around the side. The side tark to the countinas in other time. If it time to and they can an incompletely a people in the something of it. I wanted to be a classing it and a controt times that someone.\n",
      "\"I have not it. I would be to the conclidener on is troat this inter\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i4545_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farered to the saying are storage. \n",
      "\n",
      "I and a latter oun and an encompot and is the this to be that the count astall of the strong and, it, are sound to could be that your server is not the peepleding or the past of manation was around a like to seem to complate in my one that should bad in a so the ting that in the possessional assuming of a more takes and the then one than a poor to a lest and are steating and anynger as the creates. \n",
      "\n",
      "Woller triss to be to a let or ser the too sounds and sound a can that the compates also were arain tool with oth are then what it are and trying indonding on my surreed and this seen in the plails, they actually term as the check of the trives.\"\n",
      "\"You don't think you want to sumparity to the same also, we have been something are contuct a starten to any assing the per though out a compromed to tratival into summer that try to any there in the sereasion. And aren't an accesse is not inferers about the sentinator. It and that if you are and to treatian allowe\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i4545_l128.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
